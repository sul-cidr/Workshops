{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGRR3FVUVyQb"
   },
   "source": [
    "<center>\n",
    "  <h1>Digital Tools and Methods for the Humanities and Social Sciences</h1>\n",
    "  <img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/cidr-logo.no-text.240x140.png\" alt=\"Center for Interdisciplinary Digital Research @ Stanford\"/>\n",
    "</center>\n",
    "\n",
    "<h1>Text Analysis with Python (and spaCy/textacy)</h1>\n",
    "\n",
    "### Instructors\n",
    "- Peter Broadwell (CIDR), <em>broadwell@stanford.edu</em>\n",
    "- Simon Wiles (CIDR), <em>simon.wiles@stanford.edu</em>\n",
    "\n",
    "### Signing in\n",
    "Please sign in for this workshop at https://signin.cidr.link/Text_Analysis_with_Python/ -- when you've submitted the sign-in form, please keep the evaluation form open in a browser tab as a reminder to complete it when the workshop is over.\n",
    "\n",
    "### About the workshop\n",
    "\n",
    "**Learning objective**: To develop practical knowledge of methods for analyzing single documents and multi-text corpora in Python using two popular libraries: spaCy and textacy.\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Document Tokenization\n",
    "- Part-of-Speech (POS) Tagging\n",
    "- Named-Entity Recognition (NER)\n",
    "- Corpus Vectorization\n",
    "- Topic Modeling\n",
    "- Document Similarity\n",
    "- Stylistic Analysis\n",
    "\n",
    "**Note:** The examples from this workshop use English texts, but all of the methods are applicable to other languages. The availability of specialized resources (parsing rules, dictionaries, trained models) can vary considerably by language, however.\n",
    "\n",
    "### A brief word about terms\n",
    "\n",
    "**Text analysis** involves extraction of information from significant amounts  of free-form text, e.g., literature (prose, poetry), historical records, long-form survey responses, legal documents. Some of the techniques used also are applicable to short-form text data, including documents that are already in tabular format.\n",
    "\n",
    "Text analysis methods are built upon techniques for **Natural Language Processing** (NLP), which began as rule-based approaches to parsing human language and eventually incorporated statistical machine learning methods as well as, most recently, neural network/deep learning-based approaches.\n",
    "\n",
    "**Text mining** typically refers to the extraction of information from very large corpora of unstructured texts.\n",
    "\n",
    "### Jupyter notebooks and Google Colaboratory\n",
    "\n",
    "Jupyter notebooks are a way to write and run Python code interactively. They're now a standard tool for putting together data, code, and written explanations or visualizations into a single shareable document. There are a lot of ways to run Jupyter notebooks, including just locally on your computer, but we've decided to use Google's Colaboratory notebook platform for this workshop. Colaboratory is “a Google research project created to help disseminate machine learning education and research.”  If you would like to know more about Colaboratory in general, you can visit the [Welcome Notebook](https://colab.research.google.com/notebooks/welcome.ipynb).\n",
    "\n",
    "Using the Google Colaboratory platform allows us to focus on learning and writing Python in the workshop rather than on setting up Python, which can sometimes take a bit of extra work depending on platforms, operating systems, and other installed applications. If you'd like to install a Python distribution locally, though, we have some instructions (with gifs!) on installing Python through the Anaconda distribution, which will also help you handle virtual environments: https://github.com/sul-cidr/Workshops/wiki/Installing-and-Configuring-Anaconda-and-Jupyter-Notebooks\n",
    "\n",
    "If you run into problems, or would like to look into other ways of installing Python or handling virtual environments, feel free to send us an email (contact-cidr@stanford.edu) for an online consultation.\n",
    "\n",
    "### Environment\n",
    "If you would prefer to use Anaconda or your own local installation of Python or Jupyter Notebooks, you will need an environment with the following packages installed and available to complete this workshop:\n",
    "- `spacy`\n",
    "- `textacy`\n",
    "\n",
    "Please note that we will not have time during the workshop to support you with problems related to a local environment, so we do recommend using the Colaboratory notebooks during the workshop.\n",
    "\n",
    "### Evaluation survey\n",
    "At the end of the workshop, we would be very grateful if you would please spend a minute answering a few questions that will help us continue to develop our workshop series.\n",
    "- https://evaluations.cidr.link/Text_Analysis_with_Python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62xfY80v4QJ4"
   },
   "source": [
    "## Why spaCy and textacy?\n",
    "\n",
    "The language processing features of spaCy and the corpus analysis methods of textacy together offer a wide range of functionality for text analysis in a well-maintained and well-documented software package that incorporates cutting-edge techniques as well as standard approaches.\n",
    "\n",
    "The \"C\" in spaCy (and textacy) stands for Cython, which is Python that is compiled to C code and thus offers some performance advantages over interpreted Python, especially when working with large machine-learning models. The use of machine-learning models, including neural networks, is a key feature of spaCy and textacy. The writers of these libraries also have developed [Prodigy](https://prodi.gy/), a similarly leading-edge but approachable tool for training custom machine-learning models for text analysis, among other uses.\n",
    "\n",
    "### Other Python-based text analysis tools\n",
    "\n",
    "The powerful and easy-to-use string manipulation features built into Python and its standard library, along with its flexible data structures and straightforward web and file I/O, make the language a popular choice for text processing. Numerous other libraries incorporating sophisticated features for text analysis and natural language processing have been built upon these capabilities.\n",
    "\n",
    "[nltk](https://www.nltk.org/) -- Natural Language Toolkit; old-school symbolic and statistical natural language processing; English only\n",
    "\n",
    "[Stanza](https://github.com/stanfordnlp/stanza/) -- a wrapper for accessing the Java-based Stanford CoreNLP package; also now has a pipeline for using neural networks for NLP tasks\n",
    "\n",
    "[TextBlob](https://textblob.readthedocs.io/en/dev/) -- similar to spaCy in ease of use, though not as expansive in functionality and also limited to English\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) -- the central machine learning library collection for Python; includes many functions for text analysis and model building, including vectorization and topic models (which textacy uses behind the scenes)\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/) -- a popular library for higher-level analyses like semantic word embedding; also does topic modeling\n",
    "\n",
    "[flairNLP](https://github.com/flairNLP/flair) -- a somewhat more bleeding-edge NLP library for multiple languages that incorporates deep-learning frameworks\n",
    "\n",
    "It's also possible to build text analysis models directly upon Python-friendly neural network/deep learing platforms like TensorFlow and PyTorch, although some of the tools above offer similar features with much less hassle.\n",
    "\n",
    "Finally, the big cloud computing platforms all offer various text processing capabilities, often with Python APIs -- though it's recommended to get familiar with locally run libraries like those above so that you can judge whether using cloud services is warranted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEFaMNm3VyQf"
   },
   "source": [
    "# Document-level analysis with `spaCy`\n",
    "\n",
    "Let's start by learning how spaCy works and using it to begin analyzing a single text document. We'll work with larger corpora later in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1u3OoHQVyQh"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRVkmEOvVyQq"
   },
   "source": [
    "spaCy uses pre-trained statistical and deep-learning [models](https://spacy.io/models/en) to process text. The models are differentiated by language (17 languages are supported at present), capabilities, training text, and size. Smaller models are more efficient; larger models are more accurate. Here we'll download and use a medium-sized English multi-task model, which supports part of speech tagging, entity recognition, and includes a word vector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3lrUP1cVyQs"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31NWduWIVyQz"
   },
   "outputs": [],
   "source": [
    "# Once we've installed the model, we can import it like any other Python library\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VT2rin_fVyQ6"
   },
   "outputs": [],
   "source": [
    "# This instantiates a spaCy text processor based on the installed model\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQYz476fVyRA"
   },
   "outputs": [],
   "source": [
    "# From H.G. Wells's A Short History of the World, Project Gutenberg \n",
    "text = \"\"\"Even under the Assyrian monarchs and especially under\n",
    "Sardanapalus, Babylon had been a scene of great intellectual\n",
    "activity.  {111} Sardanapalus, though an Assyrian, had been quite\n",
    "Babylon-ized.  He made a library, a library not of paper but of\n",
    "the clay tablets that were used for writing in Mesopotamia since\n",
    "early Sumerian days.  His collection has been unearthed and is\n",
    "perhaps the most precious store of historical material in the\n",
    "world.  The last of the Chaldean line of Babylonian monarchs,\n",
    "Nabonidus, had even keener literary tastes.  He patronized\n",
    "antiquarian researches, and when a date was worked out by his\n",
    "investigators for the accession of Sargon I he commemorated the\n",
    "fact by inscriptions.  But there were many signs of disunion in\n",
    "his empire, and he sought to centralize it by bringing a number of\n",
    "the various local gods to Babylon and setting up temples to them\n",
    "there.  This device was to be practised quite successfully by the\n",
    "Romans in later times, but in Babylon it roused the jealousy of\n",
    "the powerful priesthood of Bel Marduk, the dominant god of the\n",
    "Babylonians.  They cast about for a possible alternative to\n",
    "Nabonidus and found it in Cyrus the Persian, the ruler of the\n",
    "adjacent Median Empire.  Cyrus had already distinguished himself\n",
    "by conquering Croesus, the rich king of Lydia in Eastern Asia\n",
    "Minor.  {112} He came up against Babylon, there was a battle\n",
    "outside the walls, and the gates of the city were opened to him\n",
    "(538 B.C.).  His soldiers entered the city without fighting.  The\n",
    "crown prince Belshazzar, the son of Nabonidus, was feasting, the\n",
    "Bible relates, when a hand appeared and wrote in letters of fire\n",
    "upon the wall these mystical words: _\"Mene, Mene, Tekel,\n",
    "Upharsin,\"_ which was interpreted by the prophet Daniel, whom he\n",
    "summoned to read the riddle, as \"God has numbered thy kingdom and\n",
    "finished it; thou art weighed in the balance and found wanting and\n",
    "thy kingdom is given to the Medes and Persians.\"  Possibly the\n",
    "priests of Bel Marduk knew something about that writing on the\n",
    "wall.  Belshazzar was killed that night, says the Bible.\n",
    "Nabonidus was taken prisoner, and the occupation of the city was\n",
    "so peaceful that the services of Bel Marduk continued without\n",
    "intermission.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9pYgwcqVyRL"
   },
   "source": [
    "By default, spaCy applies its entire NLP \"pipeline\" to the text as soon as it is provided to the model and outputs a processed \"doc.\"\n",
    "\n",
    "<img src=\"https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LR5v_iE3VyRG"
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnkTWvuwVyRN"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "The doc created by spaCy immediately provides access to the word-level tokens of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg6EB7WeVyRR"
   },
   "outputs": [],
   "source": [
    "for token in doc[:15]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh2z0VkgVyRW"
   },
   "source": [
    "Each of these tokens has a number of properties, and we'll look a bit more closely at them in a minute.\n",
    "\n",
    "spaCy also automatically provides sentence-level segmenting (senticization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6Rr6LvXVyRY"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "for sent in itertools.islice(doc.sents, 10):\n",
    "    print(sent.text + \"\\n--\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3PhTxwz7cmD"
   },
   "source": [
    "You'll notice that the line breaks in the sample text are making the extracted sentences and also the word-level tokens a bit messy. The simplest way to avoid this is just to replace all single line breaks from the text with spaces before running it throug the spaCy pipeline, i.e., as a **preprocessing** step.\n",
    "\n",
    "There are other ways to handle this within the spaCy pipeline; an important feature of spaCy is that every phase of the built-in pipeline can be replaced by a custom module. One could imagine, for example, writing a replacement sentencizer that takes advantage of the presence of two spaces between all sentences in the sample text. But we will leave that as an exercise for the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoP90Ys12tqB"
   },
   "outputs": [],
   "source": [
    "text_as_line = text.replace(\"\\n\", \" \")\n",
    "\n",
    "doc = nlp(text_as_line)\n",
    "\n",
    "for sent in itertools.islice(doc.sents, 10):\n",
    "    print(sent.text + \"\\n--\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcwG9tH9VyRd"
   },
   "source": [
    "We can collect both words and sentences into standard Python data structures (lists, in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwnDSjL7VyRe"
   },
   "outputs": [],
   "source": [
    "sentences = [sent.text for sent in doc.sents]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVJF1L5PVyRi"
   },
   "outputs": [],
   "source": [
    "words = [token.text for token in doc]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xndApEFuVyRn"
   },
   "source": [
    "### Filtering tokens\n",
    "\n",
    "After extracting the tokens, we can use some attributes and methods provided by spaCy, along with some vanilla Python methods, to filter the tokens to just the types we're interested in analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSHaSQWqVyRo"
   },
   "outputs": [],
   "source": [
    "# If we're only interested in analyzing word tokens, we can remove punctuation:\n",
    "for token in doc[:20]:\n",
    "    print(f'TOKEN: {token.text:15} IS_PUNCTUATION: {token.is_punct:}')\n",
    "no_punct = [token for token in doc if token.is_punct == False]\n",
    "\n",
    "no_punct[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K63rP_PJVyRs"
   },
   "outputs": [],
   "source": [
    "# There are still some space tokens; here's how to remove spaces and newlines:\n",
    "no_punct_or_space = [token for token in doc if token.is_punct == False and token.is_space == False]\n",
    "for token in no_punct_or_space[:30]:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHjzgbbgVyRw"
   },
   "outputs": [],
   "source": [
    "# Let's say we also want to remove numbers and lowercase everything that remains\n",
    "lower_alpha = [token.lower_ for token in no_punct_or_space if token.is_alpha == True]\n",
    "lower_alpha[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbpLhAqnVyR1"
   },
   "source": [
    "One additional common filtering step is to remove stopwords. In theory, stopwords can be any words we're not interested in analyzing, but in practice, they are often the most common words in a language that do not carry much semantic information (e.g., articles, conjunctions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STyEpj96VyR2"
   },
   "outputs": [],
   "source": [
    "clean = [token.lower_ for token in no_punct_or_space if token.is_alpha == True and token.is_stop == False]\n",
    "clean[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWe736X7mwKF"
   },
   "source": [
    "We've used spaCy's built-in stopword list; membership in this list determines the property `is_stop` for each token. It's good practice to be wary of any built-in stopword list, however -- there's a good chance you will want to remove some words that aren't on the list and to include some that are, especially if you're working with specialized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tP8b8upcmx5q"
   },
   "outputs": [],
   "source": [
    "# We'll just pick a couple of words we know are in the example\n",
    "custom_stopwords = [\"assyrian\", \"babylon\"]\n",
    "\n",
    "custom_clean = [token for token in clean if token not in custom_stopwords]\n",
    "custom_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ENXxjBHVyR8"
   },
   "source": [
    "At this point, we have a list of lower-cased tokens that doesn't contain punctuation, white-space, numbers, or stopwords. Depending on your analytical goals, you may or may not want to do this much cleaning, but hopefully you have a greater appreciation for the kinds of cleaning that can be done with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSGJfxiaVyR-"
   },
   "source": [
    "### Counting tokens\n",
    "\n",
    "Now that we've used spaCy to tokenize and clean our text, we can begin one of the most fundamental text analysis tasks: counting words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEFjnPPLVySA"
   },
   "outputs": [],
   "source": [
    "print(\"Number of tokens in document: \", len(doc))\n",
    "print(\"Number of tokens in cleaned document: \", len(clean))\n",
    "print(\"Number of unique tokens in cleaned document: \", len(set(clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZwZ3En1VySY"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "full_counter = Counter([token.lower_ for token in doc])\n",
    "full_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIrMQFp6VySg"
   },
   "outputs": [],
   "source": [
    "cleaned_counter = Counter(clean)\n",
    "cleaned_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRNYHP7wVySv"
   },
   "source": [
    "## Part-of-speech tagging\n",
    "\n",
    "Let's consider some other aspects of the text that spaCy exposes for us. One of the most noteworthy features is part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLVUUOT9VySw"
   },
   "outputs": [],
   "source": [
    "# spaCy provides two levels of POS tagging. Here's the more general level.\n",
    "for token in doc[:30]:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbB2eeMTVyS2"
   },
   "outputs": [],
   "source": [
    "# spaCy also provides the more specific Penn Treenbank tags.\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "for token in doc[:30]:\n",
    "    print(token.text, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6X3cbcmVyS4"
   },
   "source": [
    "We can count the occurrences of each part of speech in the text, which may be useful for document classification (fiction may have different proportions of parts of speech relative to nonfiction, for example) or stylistic analysis (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVey8EXsVyS5"
   },
   "outputs": [],
   "source": [
    "nouns = [token for token in doc if token.pos_ == \"NOUN\"]\n",
    "verbs = [token for token in doc if token.pos_ == \"VERB\"]\n",
    "proper_nouns = [token for token in doc if token.pos_ == \"PROPN\"]\n",
    "adjectives = [token for token in doc if token.pos_ == \"ADJ\"]\n",
    "adverbs = [token for token in doc if token.pos_ == \"ADV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qp6pH7VjVyTC"
   },
   "outputs": [],
   "source": [
    "pos_counts = {\n",
    "    \"nouns\": len(nouns),\n",
    "    \"verbs\": len(verbs),\n",
    "    \"proper_nouns\": len(proper_nouns),\n",
    "    \"adjectives\": len(adjectives),\n",
    "    \"adverbs\": len(adverbs) \n",
    "}\n",
    "\n",
    "pos_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3y4af1Z7bR4C"
   },
   "source": [
    "spaCy performs morphosyntactic analysis of individual tokens, including lemmatizing inflected or conjugated forms to their base (dictionary) forms. Reducing words to their lemmatized forms can help to make a large corpus more manageable and is generally more effective than just stemming words (trimming the inflected/conjugated endings of words until just the base portion remains), but should only be done if the inflections are not relevant to your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpYHR5pgb0Tn"
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ in [\"NOUN\", \"VERB\"] and token.orth_ != token.lemma_:\n",
    "        print(f\"{token.text:15} {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_1y3C5LVyTP"
   },
   "source": [
    "### Parsing\n",
    "\n",
    "spaCy's trained models also provide full dependency parsing, tagging word tokens with their syntactic relations to other tokens. This functionality drives spaCy's built-in senticization as well.\n",
    "\n",
    "We won't spend much time exploring this feature, but it's useful to see how it enables the extraction of multi-word \"noun chunks\" from the text. Note also that textacy (discussed below) has a built-in function to extract subject-verb-object triples from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qM28u6ZwE6v5"
   },
   "outputs": [],
   "source": [
    "for chunk in itertools.islice(doc.noun_chunks, 20):\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29Mqf_S0VyTR"
   },
   "source": [
    "## Named-entity recognition\n",
    "\n",
    "spaCy's models do a pretty good job of identifying and classifying named entities (people, places, organizations).\n",
    "\n",
    "It is also fairly easy to customize and fine-tune these models by providing additional training data (e.g., texts with entities labeled according to the desired scheme), but that's out of the scope of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KodfOLmHVyTS"
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:20} {ent.label_:15} {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGCSdUMAVyTa"
   },
   "source": [
    "What if we only care about geo-political entities or locations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhIk-M0DVyTc"
   },
   "outputs": [],
   "source": [
    "ent_filtered = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
    "ent_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buJBVUPQVyTe"
   },
   "source": [
    "### Visualizing Parses\n",
    "\n",
    "The built-in displaCy visualizer can render the results of the named-entity recognition, as well as the dependency parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3Ra-HtPVyTf"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIO_FEoLVyTi"
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU5dIAnMiODg"
   },
   "source": [
    "### Activity\n",
    "\n",
    "Pick either a particular part of speech or a named entity type, and write code to determine the most common words of that type in the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mro3MhI-ieQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F5P7cbMVyTl"
   },
   "source": [
    "# Corpus-level analysis with `textacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCwkgf9pVyTl"
   },
   "source": [
    "Let's shift to thinking about a whole corpus rather than a single document. We could analyze multiple documents with spaCy and then knit the results together with some extra Python. Instead, though, we're going to take advantage of textacy, a library built on spaCy that adds corpus analysis features.\n",
    "\n",
    "For reference, here's the [online documentation for textacy](https://textacy.readthedocs.io/en/stable/api_reference/root.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jp3AcJezVyTn"
   },
   "outputs": [],
   "source": [
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMqTW64-VyTp"
   },
   "source": [
    "## Generating corpora\n",
    "\n",
    "We'll use some of the data that is included in textacy as our corpus. It is certainly possible to build your own corpus by importing data from files in plain text, XML, JSON, CSV or other formats, but working with one of textacy's \"pre-cooked\" datasets simplifies things a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8HmDN36VyTq"
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYKe7-BCVyTs"
   },
   "outputs": [],
   "source": [
    "# We'll work with a dataset of ~8,400 (\"almost all\") U.S. Supreme Court\n",
    "# decisions from November 1946 through June 2016\n",
    "# https://github.com/bdewilde/textacy-data/releases/tag/supreme_court_py3_v1.0\n",
    "data = textacy.datasets.SupremeCourt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xmidNbxVyTu"
   },
   "outputs": [],
   "source": [
    "data.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzjG0hgUPP1M"
   },
   "source": [
    "The documentation indicates the metadata that is available with each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHMbaig9VyTx"
   },
   "outputs": [],
   "source": [
    "help(textacy.datasets.supreme_court)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TwurMgHVyT0"
   },
   "source": [
    "textacy is based on the concept of a corpus, whereas spaCy focuses on single documents. A textacy corpus is instantiated with a spaCy language model (we're using the one from the first half of this workshop) that is used to apply its analytical pipeline to each text in the corpus, and also given a set of records consisting of texts with metadata (if metadata is available).\n",
    "\n",
    "Let's go ahead and define a set of records (texts with metadata) that we'll then add to our corpus. To keep the processing time of the data set a bit more manageable, we'll just look at a set of court decisions from a short span of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTamJjiex_HE"
   },
   "outputs": [],
   "source": [
    "corpus = textacy.Corpus(nlp)\n",
    "\n",
    "# There are 79 docs in this range -- they'll take a minute or two to process\n",
    "recent_decisions = data.records(date_range=('2010-01-01', '2010-12-31'))\n",
    "\n",
    "for record in recent_decisions:\n",
    "    print(\"Adding\",record[1]['case_name'])\n",
    "    corpus.add_record(record)\n",
    "\n",
    "# If the three lines above are taking too long to process all 79 docs,\n",
    "# comment them out and uncomment the two lines below to download and import\n",
    "# a preprocessed version of the corpus\n",
    "\n",
    "#!wget https://github.com/sul-cidr/Workshops/raw/master/Text_Analysis_with_Python/data/scotus_2010.bin.gz\n",
    "#corpus = textacy.Corpus.load(nlp, \"scotus_2010.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBnvE5ZJVyT7"
   },
   "outputs": [],
   "source": [
    "print(len(corpus))\n",
    "[doc._.preview for doc in corpus[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuBjFKQ4VyT8"
   },
   "source": [
    "We can see that the type of each item in the corpus is a `Doc` - this is a processed spaCy output document, with all of the extracted features. textacy provides some capacity to work with those features via its API, and also exposes new document-level features, such as ngrams and algorithms to determine a document's readability level, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DFWqESvVyT8"
   },
   "source": [
    "We can filter this corpus based on metadata attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW_s4Eodx_HF"
   },
   "outputs": [],
   "source": [
    "corpus[0]._.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-CqNgQaVyUC"
   },
   "outputs": [],
   "source": [
    "# Here we'll find all the cases where the number of justices voting in the majority was greater than 6. \n",
    "supermajorities = [doc for doc in corpus.get(lambda doc: doc._.meta[\"n_maj_votes\"] > 6)]\n",
    "len(supermajorities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4o5hy0pCVyUG"
   },
   "outputs": [],
   "source": [
    "supermajorities[0]._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8YO2bgIVyUM"
   },
   "source": [
    "## Finding important words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wOMpBE0VyUM"
   },
   "outputs": [],
   "source": [
    "print(\"number of documents: \", corpus.n_docs)\n",
    "print(\"number of sentences: \", corpus.n_sents)\n",
    "print(\"number of tokens: \", corpus.n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiQQrx5SVyUO"
   },
   "outputs": [],
   "source": [
    "# Set as_strings to True so that the results will display strings rather than unique ids.\n",
    "counts = corpus.word_counts(by = \"orth\", filter_nums=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELd2rxUZVyUU"
   },
   "outputs": [],
   "source": [
    "def show_doc_counts(input_corpus, weighting, limit=20):\n",
    "    doc_counts = input_corpus.word_doc_counts(weighting=weighting, filter_stops=True, by = \"orth\")\n",
    "    print(\"\\n\".join([f'{a:15} {str(b)}' for a,b in sorted(doc_counts.items(), key=lambda x:x[1], reverse=True)[:limit]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5M6Nyy0079J"
   },
   "source": [
    "`word_doc_counts` provides a few ways of quantifying the prevalence of individual words across the corpus: whether a word appears many times in most documents, just a few times in a few documents, many times in a few documents, or just a few times in most documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIIkImS2VyUW"
   },
   "outputs": [],
   "source": [
    "print(\"# DOCS APPEARING IN / TOTAL # DOCS\")\n",
    "show_doc_counts(corpus, \"freq\")\n",
    "print(\"\\nLOG(TOTAL # DOCS / # DOCS APPEARING IN)\")\n",
    "show_doc_counts(corpus, \"idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx69GIqUdqtx"
   },
   "source": [
    "textacy provides implementations of algorithms for identifying words and phrases that are representative of a document (aka **keyterm extraction**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQCRD7WTeI1j"
   },
   "outputs": [],
   "source": [
    "from textacy.extract import keyterms as ke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEfkQ1IWx_HH"
   },
   "outputs": [],
   "source": [
    "corpus[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSwQ4Rw6dWaT"
   },
   "outputs": [],
   "source": [
    "# Run the TextRank algorithm (Mihalcea, R., & Tarau, P., 2004) for a given document\n",
    "\n",
    "key_terms_textrank = ke.textrank(corpus[0])\n",
    "key_terms_textrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRyHfZekewCT"
   },
   "source": [
    "For comparison, we'll take a look at another algorithm, Yake (Campos et al., 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HWvJ8ube4Vc"
   },
   "outputs": [],
   "source": [
    "key_terms_yake = ke.yake(corpus[0])\n",
    "key_terms_yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P296E9nxhnQZ"
   },
   "source": [
    "### Activity:\n",
    "Let's combine a few different pieces. Try filtering the corpus on some metadata to construct a sub-corpus. Then use one of the textacy keyword algorithms to determine the most common keywords across your subcorpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9otxxqxx_HI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuk7s1yZlvVi"
   },
   "source": [
    "## Keyword in context\n",
    "\n",
    "Sometimes researchers find it helpful just to see a particular keyword in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxgjXFnRla1-"
   },
   "outputs": [],
   "source": [
    "for doc in corpus[:5]:\n",
    "    print(doc._.meta.get('case_name'))\n",
    "    for match in textacy.extract.kwic.keyword_in_context(doc.text, \"judgment\"):\n",
    "        print(\" \".join(match).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuSHX_2OVyUb"
   },
   "source": [
    "## Vectorization\n",
    "\n",
    "Let's continue with corpus-level analysis by taking advantage of textacy's vectorizer class, which wraps functionality from `scikit-learn` to count the prevalence of certain tokens in each document of the corpus and to apply weights to these counts if desired. We could just work directly in `scikit-learn`, but it can be nice for mental overhead to learn one library and be able to do a great deal with it.\n",
    "\n",
    "We'll create a vectorizer, sticking with the normal term frequency defaults but discarding words that appear in fewer than 3 documents or more than 95% of documents. We'll also limit our features to the top 500 words according to document frequency. This means our feature set, or columns, will have a higher degree of representation across the corpus. We could further scale these counts according to document frequency (or inverse document frequency) weights, or normalize the weights so that they add up to 1 for each document row (L1 norm), and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LfE48GbVyUb"
   },
   "outputs": [],
   "source": [
    "import textacy.representations\n",
    "\n",
    "vectorizer = textacy.representations.Vectorizer(min_df=3, max_df=.95, max_n_terms=500)\n",
    "\n",
    "tokenized_corpus = [[token.orth_ for token in list(textacy.extract.words(doc, filter_nums=True, filter_stops=True, filter_punct=True))] for doc in corpus]\n",
    "\n",
    "dtm = vectorizer.fit_transform(tokenized_corpus)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5lQ10twVyUf"
   },
   "source": [
    "We have now have a matrix representation of our corpus, where rows are documents, and columns (or features) are words from the corpus. The value at any given point is the number of times that the word appears in that document. Once we have a document-term matrix, we could do several things with it just within textacy, though we also can pass it into different algorithms within `scikit-learn` or other libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37KLE01TVyUg"
   },
   "outputs": [],
   "source": [
    "# Let's look at some of the terms\n",
    "vectorizer.terms_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_I6y0JIVyUh"
   },
   "source": [
    "We can see that we are still getting a number of terms which might be filtered out, such as symbols and abbreviations. The most straightforward solutions are to filter the terms against a dictionary during vectorization, which carries the risk of inadvertently filtering words that you'd prefer to keep in the dataset, or curating a custom stopword list, which can be inflexible and time consuming. Otherwise, it is often the case that the corpus analysis tools used with the vectorized texts (e.g., topic modeling or stylistic analysis -- see below) have ways of recognizing and sequestering unwanted terms so that they can be excluded from the results if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7c7uSmdnLOz"
   },
   "source": [
    "## Topic modeling\n",
    "\n",
    "Let's look quickly at one example of what we can do with a vectorized corpus. Topic modeling is very popular for semantic exploration of texts, and there are numerous implementations of it. Textacy uses implementations from scikit-learn. \n",
    "\n",
    "Our corpus is rather small for topic modeling, but just to see how it's done here, we'll go ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12O2kmUOQx43"
   },
   "source": [
    "First, though, topic modeling works best when the texts are divided into approximately equal-sized \"chunks.\" A quick word-count of the corpus will show that the decisions are of quite variable lengths, which will skew the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwoHLaL8RgNC"
   },
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    print(doc._.meta.get('case_name'), doc._.meta.get('decision_date'), doc.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L41Er3jBRhVG"
   },
   "source": [
    "We'll re-chunk the texts into documents of not more than 500 words and then recompute the document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6XnxyCBR0wY"
   },
   "outputs": [],
   "source": [
    "chunked_corpus_unflattened = [ [text[x:x+500] for x in range(0,len(text),500)] for text in tokenized_corpus]\n",
    "chunked_corpus = list(itertools.chain.from_iterable(chunked_corpus_unflattened))\n",
    "chunked_dtm = vectorizer.fit_transform(chunked_corpus)\n",
    "chunked_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFusfd9sm1dv"
   },
   "outputs": [],
   "source": [
    "import textacy.tm\n",
    "\n",
    "model = textacy.tm.TopicModel(\"lda\", n_topics=15)\n",
    "model.fit(chunked_dtm)\n",
    "doc_topic_matrix = model.transform(chunked_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zP_oC22nJLq"
   },
   "outputs": [],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=10):\n",
    "  print(\"topic\", topic_idx, f\"{model.topic_weights(doc_topic_matrix)[topic_idx]:.0%}\", \":\", \"   \".join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiSsVaW3x_HL"
   },
   "source": [
    "## Document similarity with word2vec and clustering\n",
    "\n",
    "textacy provides several built-in methods for measuring the degree of similarity between two documents, including a `word2vec`-based approach that computes the semantic similarity between documents based on the word vector model included with the spaCy language model. This technique is capable of inferring, for example, that two documents are topically related even if they don't share any words but use synonyms for a shared concept.\n",
    "\n",
    "To evaluate this similarity comparison, we'll compute the similarity of each pair of docs in the corpus, and then branch out into `scikit-learn` a bit to look for clusters based on these similarity measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY1Hwcpxx_HL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dim = corpus.n_docs\n",
    "\n",
    "distance_matrix = np.zeros((dim,dim))\n",
    "    \n",
    "for i, doc_i in enumerate(corpus):\n",
    "    for j, doc_j in enumerate(corpus):\n",
    "        if i == j:\n",
    "            continue # defaults to 0\n",
    "        if i > j:\n",
    "            distance_matrix[i,j] = distance_matrix[j,i]\n",
    "        else:\n",
    "            distance_matrix[i,j] = 1 - doc_i.similarity(doc_j)\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WMJNsAJx_HZ"
   },
   "source": [
    "The [OPTICS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) hierarchical density-based clustering algorithm only finds one cluster with its default settings, but an examination of the legal issue types coded to each decision indicates that the `word2vec`-based clustering has indeed produced a group of semantically related documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ac0aESX3x_Ha"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "clustering = OPTICS(metric='precomputed').fit(distance_matrix)\n",
    "print(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4NiQ0BPx_Ha"
   },
   "outputs": [],
   "source": [
    "for i, cluster_id in enumerate(clustering.labels_):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    print(cluster_id, corpus[i]._.meta['us_cite_id'], data.issue_area_codes[corpus[i]._.meta['issue_area']], ':', data.issue_codes[corpus[i]._.meta['issue']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IxCE-RBx_Hb"
   },
   "source": [
    "## Case study: Stylistic analysis of U.S. Supreme Court opinions\n",
    "\n",
    "One of the more impressive affordances of text analysis is the ability to infer the authorship of a text with some degree of confidence based only upon the stylistic attributes observed through statistical analysis of the writer's use of common \"function\" words (conjunctions, articles). Note however that if a writer desires to be anonymous, she can attempt to confuse such algorithms by intentionally adopting a different writing style.\n",
    "\n",
    "The U.S. Supreme Court decisions corpus does usually identify the author of each opinion, but sometimes these are inconsistently described in the text, especially for decisions issued with no majority author listed (e.g., unanimous decisions). Applying stylistic analysis techniques to a substantial subset of the corpus (here we use just the majority author portion of the decisions from 1993 to 2016) also can expose similarities between the writing styles of particular justices -- or perhaps of their clerks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZk5b7zxVyTz"
   },
   "outputs": [],
   "source": [
    "# This code replaces the corpus above with a much larger corpus that may take a long\n",
    "# time to process and which may take up a considerable amount of RAM.\n",
    "\n",
    "# Note that the text of each decision in this corpus is trimmed to include only\n",
    "# the opinion of the majority author (and not any footnotes, supporting or\n",
    "# dissenting opinions, etc.) via regular expression matching of formulaic\n",
    "# phrases in the case file, specifically:\n",
    "# [JUSTICE NAME] delivered the opinion of the Court.\n",
    "# --- MAJORITY AUTHOR'S OPINION HERE ---\n",
    "# It is so ordered.\n",
    "\n",
    "\"\"\"\n",
    "corpus = textacy.Corpus(nlp)\n",
    "import re\n",
    "\n",
    "# 1986 - 2016 = Rehnquist and Roberts courts, but start at 1993 because there was a lot of turnover then\n",
    "for year in range(1993, 2017):\n",
    "    print(\"Finding record(s) from\",year)\n",
    "    year_generator = data.records(date_range=(str(year)+'-01-01', str(year)+'-12-31'))\n",
    "    for record in year_generator:\n",
    "        # There are only 2 opinions in this range by Byron White; exclude them\n",
    "        if record[1]['maj_opinion_author'] == 95:\n",
    "            continue\n",
    "        text = record[0]\n",
    "        # NOTE: These regexs works well on 1993-2016, but it is NOT guaranteed to work on the full corpus\n",
    "        if record[1]['maj_opinion_author'] != -1:\n",
    "            match = re.search(r\"delivered the opinion of the Court\\.(.*)It is so ordered\", text, flags=re.I | re.M | re.DOTALL)\n",
    "        else:\n",
    "            match = re.search(r\"Per Curiam\\.(.*)It is so ordered\", text, flags=re.I | re.M | re.DOTALL)\n",
    "        if match is None:\n",
    "            continue\n",
    "        opinion = match.group(1)\n",
    "        if len(opinion) < 500:\n",
    "            continue\n",
    "        mutable_record = list(record)\n",
    "        mutable_record[0] = opinion\n",
    "        corpus.add_record(mutable_record)\n",
    "\"\"\"\n",
    "\n",
    "# Rather than wait for the corpus to be processed, we'll just download a pre-processed\n",
    "# version from Github. It should take about two minutes to import.\n",
    "\n",
    "!wget https://github.com/sul-cidr/Workshops/raw/master/Text_Analysis_with_Python/data/recent_opinions.bin.gz\n",
    "opinions_corpus = textacy.Corpus.load(nlp, \"recent_opinions.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook locally, unhashtag the below lines if you get an error in the above cell: \n",
    "# !pip install wget\n",
    "# import wget\n",
    "# wget.download(\"https://github.com/sul-cidr/Workshops/raw/master/Text_Analysis_with_Python/data/recent_opinions.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSGBVDgZx_Hc"
   },
   "outputs": [],
   "source": [
    "# A helper function to extract the non-numeric tokens from a document, then\n",
    "# filter these by function (no nouns) and semantic (no stopwords) tokens,\n",
    "# and to return these lists as well as the sentences in the document.\n",
    "def get_doc_tokens_and_sents(doc):\n",
    "    sents = doc.sents\n",
    "    alpha_tokens = [token for token in doc if token.is_punct == False and token.is_space == False and token.is_alpha == True]\n",
    "    semantic_tokens = [token for token in alpha_tokens if token.is_stop == False]\n",
    "    function_tokens = [token for token in alpha_tokens if token.is_stop == True]\n",
    "    \n",
    "    return [sents, alpha_tokens, semantic_tokens, function_tokens]\n",
    "\n",
    "# This code just builds a list of author names from the corpus and assigns\n",
    "# them to colors and shapes for drawing plots later\n",
    "doc_author_names = []\n",
    "\n",
    "author_opinions = {}\n",
    "\n",
    "author_names = []\n",
    "author_surnames = []\n",
    "\n",
    "for i, doc in enumerate(opinions_corpus):\n",
    "    maj_author_id = doc._.meta[\"maj_opinion_author\"]\n",
    "    maj_author_name = data.opinion_author_codes[maj_author_id]\n",
    "    if maj_author_name is None:\n",
    "        maj_author_name = \"None\"\n",
    "    doc_author_names.append(maj_author_name)\n",
    "    if maj_author_name not in author_opinions:\n",
    "        author_opinions[maj_author_name] = [i]\n",
    "        author_names.append(maj_author_name)\n",
    "        author_surnames.append(maj_author_name.split(',')[0].strip())\n",
    "    else:\n",
    "        author_opinions[maj_author_name].append(i)\n",
    "    \n",
    "from matplotlib import cm\n",
    "cmap = cm.get_cmap('gist_rainbow', len(author_names))\n",
    "\n",
    "available_markers = ['o', 'v', '^', '<', '>', 's', 'P', '*', '+', 'X', 'D', '1', '2', '3', '4', 'p', 'x', '|', '_']\n",
    "\n",
    "author_name_to_color = {}\n",
    "author_name_to_marker = {}\n",
    "for i, name in enumerate(author_names):\n",
    "    author_name_to_color[name] = cmap(i)\n",
    "    author_name_to_marker[name] = available_markers[i]\n",
    "\n",
    "doc_author_colors = [author_name_to_color[name] for name in doc_author_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXGFo1OTx_Hc"
   },
   "source": [
    "We'll create a vectorized version of the majority author decisions 1993-2016 subcorpus in which all nouns have been removed, leaving mostly \"functional\" rather than \"semantic\" words behind. Although it's not entirely foolproof, this filtering step is meant to ensure that stylistic, rather than content-based attributes of the documents remain as material for comparison and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYI_18ORx_Hc"
   },
   "outputs": [],
   "source": [
    "from textacy.representations.vectorizers import Vectorizer\n",
    "\n",
    "function_vectorizer = Vectorizer(min_df=1, max_df=1.0) # This will count all words in the corpus\n",
    "# Remove the nouns\n",
    "function_corpus = [[token.lower_ for token in list(textacy.extract.words(doc, filter_nums=True, filter_stops=False, filter_punct=True, exclude_pos=['NOUN', 'PROPN']))] for doc in opinions_corpus]\n",
    "\n",
    "function_dtm = function_vectorizer.fit_transform(function_corpus)\n",
    "# For comparison, the full corpus contains 65,840 terms\n",
    "print(function_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dEHrXm_x_Hc"
   },
   "outputs": [],
   "source": [
    "# This is a helper function to compute pairwise cosine (dis)similarities of docs in the corpus.\n",
    "# cosine = similarity based on proportions of shared word frequencies\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def get_distance_matrix(method, corpus_doc_vectors):\n",
    "    dim = len(corpus_doc_vectors)\n",
    "    distance_matrix = np.zeros((dim,dim))\n",
    "    \n",
    "    for i, vec1 in enumerate(corpus_doc_vectors):\n",
    "        print(\"row\",i)\n",
    "        for j, vec2 in enumerate(corpus_doc_vectors):\n",
    "            if i == j:\n",
    "                continue # defaults to 0\n",
    "            if i > j:\n",
    "                distance_matrix[i,j] = distance_matrix[j,i]\n",
    "            else:\n",
    "                #print(vec1, vec2)\n",
    "                distance_matrix[i,j] = method(vec1, vec2)\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F35Wzwiix_Hc"
   },
   "outputs": [],
   "source": [
    "# This reorders the corpus documents by opinion author, rather than case ID/date,\n",
    "# then computes a distance matrix based on the cosine similarity of the\n",
    "# \"non-semantic\" (function) word set for each pair of documents in the corpus\n",
    "\n",
    "dim = len(opinions_corpus)\n",
    "\n",
    "reordered_corpus = [None] * dim\n",
    "reordered_author_docs = []\n",
    "\n",
    "author_start_indices = []\n",
    "author_ranges = {}\n",
    "\n",
    "new_index=0\n",
    "for i, name in enumerate(author_names):\n",
    "    author_ranges[name] = [new_index, -1]\n",
    "    author_start_indices.append(new_index)\n",
    "    for doc_index in author_opinions[name]:\n",
    "        reordered_corpus[new_index] = opinions_corpus[doc_index]\n",
    "        reordered_author_docs.append(name)\n",
    "        author_ranges[name][1] = new_index\n",
    "        new_index += 1\n",
    "\n",
    "reordered_functions = [[token.orth_ for token in list(textacy.extract.words(doc, filter_nums=True, filter_stops=False, filter_punct=True, exclude_pos=['NOUN', 'PROPN']))] for doc in reordered_corpus]\n",
    "reordered_dtm = function_vectorizer.fit_transform(reordered_functions)\n",
    "reordered_function_matrix = get_distance_matrix(distance.cosine, reordered_dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyCTO9Xyx_Hc"
   },
   "outputs": [],
   "source": [
    "# Plot a correlation heatmap matrix view of the corpus, ordered by opinion author.\n",
    "\n",
    "maxval = np.amax(reordered_function_matrix)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12, 12), dpi=80)\n",
    "plt.imshow(reordered_function_matrix, cmap=\"viridis\", vmax=maxval/2, interpolation='bicubic')\n",
    "plt.colorbar()\n",
    "ax = fig.gca()\n",
    "ax.set_xticks(author_start_indices)\n",
    "ax.set_xticklabels(author_surnames)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "# Including the left-hand labels causes the plot to be shifted incorrectly, for some reason.\n",
    "#ax.set_yticks(author_start_indices)\n",
    "#ax.set_yticklabels(author_surnames)\n",
    "\n",
    "plt.show()\n",
    "for author in author_names:\n",
    "    print(author,str(author_ranges[author][0]) + '-' + str(author_ranges[author][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUNhABB7x_Hd"
   },
   "source": [
    "The document-level stylistic similarity relationships from the matrix above\n",
    "also can be viewed as a two-dimensional scatterplot via techniques like PCA\n",
    "(Principal Component Analysis) and MDS (Multi-Dimensional Scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzTG5KNvx_Hd"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "mds = MDS(dissimilarity='precomputed')\n",
    "pos = mds.fit_transform(reordered_function_matrix)\n",
    "#pca = PCA()\n",
    "#pos = pca.fit_transform(reordered_function_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOiqWWIlEwGC"
   },
   "source": [
    "The resulting plot does not immediately reveal obvious document clusterings, but this doesn't necessarily indicate that the stylistic features are wholly uninformative..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nCKbm0xx_Hd"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12,8), dpi=72)\n",
    "\n",
    "for name in author_names:\n",
    "    color = author_name_to_color[name]\n",
    "    marker = author_name_to_marker[name]\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i, doc_name in enumerate(doc_author_names):\n",
    "        if doc_name != name:\n",
    "            continue\n",
    "        xs.append(pos[i,0])\n",
    "        ys.append(pos[i,1])\n",
    "\n",
    "    sc = plt.scatter(xs, ys, c=[color], label=name, alpha=0.7, marker=marker, edgecolors='none')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5y0lO9ex_Hd"
   },
   "source": [
    "Another approach is to train a document classifier on the observed stylistic\n",
    "features (i.e., function word frequencies) and then test the classifier to see\n",
    "how well it can differentiate between authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_r6dReJjx_Hd"
   },
   "outputs": [],
   "source": [
    "# We'll use sklearn's own vectorizer for this part\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "doc_texts = []\n",
    "\n",
    "for doc in opinions_corpus:\n",
    "    sents, alpha_tokens, semantic_tokens, function_tokens = get_doc_tokens_and_sents(doc)\n",
    "    doc_texts.append(\" \".join(token.lower_ for token in function_tokens))\n",
    "\n",
    "vectorizer.fit(doc_texts)\n",
    "\n",
    "X_texts = np.array(doc_texts)\n",
    "y_labels = np.array(doc_author_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLgJNPnhx_Hd"
   },
   "source": [
    "If we train a Naive Bayes document classifier (which is perhaps most famous for its effectiveness as a spam filter) so that it learns various word frequency associations with the author labels, we can then run the classifier against a \"held-out\" portion of the corpus to see how well it performs.\n",
    "\n",
    "Note that the resulting classification accuracy is around 50%, which is much higher than the 7% you'd exect if the classifier was just choosing one of the 15 author names at random. Not too bad considering the features it is working with -- mostly counts of words like \"a\", \"and\", and \"the\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2E8y0_y6x_He"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "def train_test_nb(random_seed):\n",
    "    (X_texts_train, X_texts_test,\n",
    "     y_labels_train, y_labels_test) = train_test_split(X_texts, y_labels, test_size=0.33, random_state=random_seed)\n",
    "\n",
    "    X_features_train = vectorizer.fit_transform(X_texts_train)\n",
    "    X_features_test = vectorizer.transform(X_texts_test)\n",
    "\n",
    "    classifier.fit(X_features_train, y_labels_train)\n",
    "\n",
    "    accuracy = classifier.score(X_features_test, y_labels_test)\n",
    "    print(\"NB accuracy:\",accuracy)\n",
    "\n",
    "# Run a few times with different train/test sets, to make sure it's not a fluke\n",
    "train_test_nb(42)\n",
    "train_test_nb(16)\n",
    "train_test_nb(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDmG99fcx_He"
   },
   "source": [
    "A related approach is to train a classifier on the full corpus and then to run this classifier on the full corpus. Normally, this kind of \"overfitting\" produces artificially high accuracy, but it also can be used to expose cases in which the classifier gets certain classifications consistently wrong. This type of classifier \"confusion\" is a sign that the writing styles of the authors being confused are in fact similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ca_tr-exx_He"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_features = vectorizer.fit_transform(X_texts)\n",
    "\n",
    "classifier.fit(X_features, y_labels)\n",
    "\n",
    "y_labels_pred = classifier.predict(X_features)\n",
    "cm = confusion_matrix(y_labels, y_labels_pred, labels=author_names)\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "# Read the confusion matrix as\n",
    "# left-hand labels = the \"true\" author\n",
    "# bottom-row labels = the author predicted by the classifier\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, cbar=False, xticklabels=author_surnames, yticklabels=author_surnames, cmap=\"Blues\", fmt='d')\n",
    "b, t = plt.ylim()\n",
    "b += 0.5\n",
    "t -= 0.5\n",
    "plt.ylim(b, t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or8waiF7x_He"
   },
   "outputs": [],
   "source": [
    "# This code displays a few of the features (words) that the classifier\n",
    "# found most helpful when making its classification decisions. These are\n",
    "# indeed very common \"function\" words (the, of, to, that, and, in, is, for)\n",
    "\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "def get_feature_counts(dtm, labels, categories, term, vocab):\n",
    "  category_counts = {}\n",
    "  for category in categories:\n",
    "    category_counts[category] = 0\n",
    "    for i, label in enumerate(labels):\n",
    "      if label == category:\n",
    "        vocab_position = np.where(vocab == term)[0][0]\n",
    "        category_counts[category] += dtm[i, vocab_position]\n",
    "  return category_counts\n",
    "\n",
    "def most_informative_features(classifier, vectorizer, categories, n=20):\n",
    "    class_labels = classifier.classes_\n",
    "    if vectorizer is None:\n",
    "        feature_names = classifier.steps[0].get_feature_names()\n",
    "    else:\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_log_prob_[0], feature_names))[-n:]\n",
    "    topn_class2 = sorted(zip(classifier.feature_log_prob_[1], feature_names))[-n:]\n",
    "    for prob, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], prob, feat)\n",
    "        print(str(get_feature_counts(X_features, y_labels, categories, feat, vocab)))\n",
    "    print()\n",
    "    for prob, feat in reversed(topn_class1):\n",
    "        print(class_labels[0], prob, feat)\n",
    "        print(str(get_feature_counts(X_features, y_labels, categories, feat, vocab)))\n",
    "\n",
    "most_informative_features(classifier, vectorizer, author_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiXDcIM9x_Hf"
   },
   "source": [
    "Here we'll compute and visualize a few more style-related aspects of the texts associated with each majority author.\n",
    "\n",
    "The \"entropy\" of the words used by an author in a given document, which roughly corresponds to the \"unpredictability\" or \"variety\" in an author's word choices, has proven to be effective in differentiating authors stylistically in the absence of any other identifying data. This seems to be the case here as well, at least for some of the justices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaqYhsITx_Hf"
   },
   "outputs": [],
   "source": [
    "author_function_entropies = []\n",
    "author_alpha_entropies = []\n",
    "author_opinion_lengths = []\n",
    "\n",
    "for i, author_name in enumerate(author_opinions):\n",
    "    author_function_entropies.append([])\n",
    "    author_alpha_entropies.append([])\n",
    "    author_opinion_lengths.append([])\n",
    "    for doc_id in author_opinions[author_name]:\n",
    "        doc = opinions_corpus[doc_id]\n",
    "        sents, alpha_tokens, semantic_tokens, function_tokens = get_doc_tokens_and_sents(doc)\n",
    "        function_entropy = textacy.text_stats.basics.entropy(function_tokens)\n",
    "        alpha_entropy = textacy.text_stats.basics.entropy(alpha_tokens)\n",
    "        opinion_length = len(alpha_tokens)\n",
    "\n",
    "        author_function_entropies[i].append(function_entropy)\n",
    "        author_alpha_entropies[i].append(alpha_entropy)\n",
    "        author_opinion_lengths[i].append(opinion_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1wOTm3Lx_Hf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "bplots = {\"Function Word Entropies\": author_function_entropies,\n",
    "          \"Semantic + Function Word Entropies\": author_alpha_entropies,\n",
    "          \"Opinion Lengths\": author_opinion_lengths}\n",
    "\n",
    "for bplot in bplots:\n",
    "    fig = plt.figure(figsize=(12,6), dpi=72)\n",
    "    fig.gca().set_title(bplot)\n",
    "    plt.boxplot(bplots[bplot], labels=author_surnames, notch=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS3JuVG6awew"
   },
   "source": [
    "## Additional Topics and Resources\n",
    "\n",
    "**Sentiment analysis**: This workshop has touched on most of the main features of spaCy and textacy. One popular type of text analysis not covered is sentiment analysis, which neither spaCy nor textacy provides as a built-in feature (yet), and which anyway is not directly applicable to the historical and legal corpora used in this workshop. Among the available Python packages, [TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html) provides an especially straightforward implementation of sentiment analysis, albeit only for English texts.\n",
    "\n",
    "**State of the field**: Text analysis is one of the foundational areas of inquiry for computational scholarship in the humanities and social sciences. Research continues to progress at a rapid pace, particularly involving deep-learning approaches to language translation, comprehension and generation. These are now quite likely to involve the productive exchange of methods with research into computational analysis of images, audio and video. Surveying the other software packages listed at the beginning of this workshop is a good way to stay up to date on what is currently available.\n",
    "\n",
    "**Ethical concerns**: Although this workshop covers fairly basic topics in text analysis, the more advanced methods discussed here already begin to engage with issues of bias, privacy, and automation related to computational models and AI. The research reports and essays at https://ainowinstitute.org/research.html provide illuminating perspectives on these topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QW2MkH8h-ZEV"
   },
   "source": [
    "Here's the link to the evaluation survey again:\n",
    "- https://evaluations.cidr.link/Text_Analysis_with_Python/\n",
    "\n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Text_Analysis_with_Python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "206px",
    "width": "555px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
