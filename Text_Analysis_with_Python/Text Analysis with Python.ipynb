{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGRR3FVUVyQb"
   },
   "source": [
    "<center>\n",
    "  <h1>Digital Tools and Methods for the Humanities and Social Sciences</h1>\n",
    "  <img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/cidr-logo.no-text.240x140.png\" alt=\"Center for Interdisciplinary Digital Research @ Stanford\"/>\n",
    "</center>\n",
    "\n",
    "<h1>Text Analysis with Python (and spaCy/textacy)</h1>\n",
    "\n",
    "### Instructors\n",
    "- Peter Broadwell (CIDR), <em>broadwell@stanford.edu</em>\n",
    "- Simon Wiles (CIDR), <em>simon.wiles@stanford.edu</em>\n",
    "\n",
    "### Signing in\n",
    "Please sign in for this workshop at https://signin.cidr.link/Text_Analysis_with_Python/ -- when you've submitted the sign-in form, please keep the evaluation form open in a browser tab as a reminder to complete it when the workshop is over.\n",
    "\n",
    "### About the workshop\n",
    "\n",
    "**Learning objective**: To develop practical knowledge of methods for analyzing single documents and multi-text corpora in Python using two popular libraries: spaCy and textacy.\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Document Tokenization\n",
    "- Part-of-Speech (POS) Tagging\n",
    "- Named-Entity Recognition (NER)\n",
    "- Corpus Vectorization\n",
    "- Topic Modeling\n",
    "- Document Similarity\n",
    "- Stylistic Analysis\n",
    "\n",
    "**Note:** The examples from this workshop use English texts, but all of the methods are applicable to other languages. The availability of specialized resources (parsing rules, dictionaries, trained models) can vary considerably by language, however.\n",
    "\n",
    "### A brief word about terms\n",
    "\n",
    "**Text analysis** involves extraction of information from significant amounts  of free-form text, e.g., literature (prose, poetry), historical records, long-form survey responses, legal documents. Some of the techniques used also are applicable to short-form text data, including documents that are already in tabular format.\n",
    "\n",
    "Text analysis methods are built upon techniques for **Natural Language Processing** (NLP), which began as rule-based approaches to parsing human language and eventually incorporated statistical machine learning methods as well as, most recently, neural network/deep learning-based approaches.\n",
    "\n",
    "**Text mining** typically refers to the extraction of information from very large corpora of unstructured texts.\n",
    "\n",
    "### Jupyter notebooks and Google Colaboratory\n",
    "\n",
    "Jupyter notebooks are a way to write and run Python code interactively. They're now a standard tool for putting together data, code, and written explanations or visualizations into a single shareable document. There are a lot of ways to run Jupyter notebooks, including just locally on your computer, but we've decided to use Google's Colaboratory notebook platform for this workshop. Colaboratory is “a Google research project created to help disseminate machine learning education and research.”  If you would like to know more about Colaboratory in general, you can visit the [Welcome Notebook](https://colab.research.google.com/notebooks/welcome.ipynb).\n",
    "\n",
    "Using the Google Colaboratory platform allows us to focus on learning and writing Python in the workshop rather than on setting up Python, which can sometimes take a bit of extra work depending on platforms, operating systems, and other installed applications. If you'd like to install a Python distribution locally, though, we have some instructions (with gifs!) on installing Python through the Anaconda distribution, which will also help you handle virtual environments: https://github.com/sul-cidr/Workshops/wiki/Installing-and-Configuring-Anaconda-and-Jupyter-Notebooks\n",
    "\n",
    "If you run into problems, or would like to look into other ways of installing Python or handling virtual environments, feel free to send us an email (contact-cidr@stanford.edu) for an online consultation.\n",
    "\n",
    "### Environment\n",
    "If you would prefer to use Anaconda or your own local installation of Python or Jupyter Notebooks, you will need an environment with the following packages installed and available to complete this workshop:\n",
    "- `spacy`\n",
    "- `textacy`\n",
    "\n",
    "Please note that we will not have time during the workshop to support you with problems related to a local environment, so we do recommend using the Colaboratory notebooks during the workshop.\n",
    "\n",
    "### Evaluation survey\n",
    "At the end of the workshop, we would be very grateful if you would please spend a minute answering a few questions that will help us continue to develop our workshop series.\n",
    "- https://evaluations.cidr.link/Text_Analysis_with_Python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62xfY80v4QJ4"
   },
   "source": [
    "## Why spaCy and textacy?\n",
    "\n",
    "The language processing features of spaCy and the corpus analysis methods of textacy together offer a wide range of functionality for text analysis in a well-maintained and well-documented software package that incorporates cutting-edge techniques as well as standard approaches.\n",
    "\n",
    "The \"C\" in spaCy (and textacy) stands for Cython, which is Python that is compiled to C code and thus offers some performance advantages over interpreted Python, especially when working with large machine-learning models. The use of machine-learning models, including neural networks, is a key feature of spaCy and textacy. The writers of these libraries also have developed [Prodigy](https://prodi.gy/), a similarly leading-edge but approachable tool for training custom machine-learning models for text analysis, among other uses.\n",
    "\n",
    "### Other Python-based text analysis tools\n",
    "\n",
    "The powerful and easy-to-use string manipulation features built into Python and its standard library, along with its flexible data structures and straightforward web and file I/O, make the language a popular choice for text processing. Numerous other libraries incorporating sophisticated features for text analysis and natural language processing have been built upon these capabilities.\n",
    "\n",
    "[nltk](https://www.nltk.org/) -- Natural Language Toolkit; old-school symbolic and statistical natural language processing; English only\n",
    "\n",
    "[Stanza](https://github.com/stanfordnlp/stanza/) -- a wrapper for accessing the Java-based Stanford CoreNLP package; also now has a pipeline for using neural networks for NLP tasks\n",
    "\n",
    "[TextBlob](https://textblob.readthedocs.io/en/dev/) -- similar to spaCy in ease of use, though not as expansive in functionality and also limited to English\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) -- the central machine learning library collection for Python; includes many functions for text analysis and model building, including vectorization and topic models (which textacy uses behind the scenes)\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/) -- a popular library for higher-level analyses like semantic word embedding; also does topic modeling\n",
    "\n",
    "[flairNLP](https://github.com/flairNLP/flair) -- a somewhat more bleeding-edge NLP library for multiple languages that incorporates deep-learning frameworks\n",
    "\n",
    "It's also possible to build text analysis models directly upon Python-friendly neural network/deep learing platforms like TensorFlow and PyTorch, although some of the tools above offer similar features with much less hassle.\n",
    "\n",
    "Finally, the big cloud computing platforms all offer various text processing capabilities, often with Python APIs -- though it's recommended to get familiar with locally run libraries like those above so that you can judge whether using cloud services is warranted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEFaMNm3VyQf"
   },
   "source": [
    "# Document-level analysis with `spaCy`\n",
    "\n",
    "Let's start by learning how spaCy works and using it to begin analyzing a single text document. We'll work with larger corpora later in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (3.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: jinja2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: setuptools in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m1u3OoHQVyQh"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRVkmEOvVyQq"
   },
   "source": [
    "spaCy uses pre-trained statistical and deep-learning [models](https://spacy.io/models/en) to process text. The models are differentiated by language (17 languages are supported at present), capabilities, training text, and size. Smaller models are more efficient; larger models are more accurate. Here we'll download and use a medium-sized English multi-task model, which supports part of speech tagging, entity recognition, and includes a word vector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h3lrUP1cVyQs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 45.7 MB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from en-core-web-md==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (20.9)\n",
      "Requirement already satisfied: jinja2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.20.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "31NWduWIVyQz"
   },
   "outputs": [],
   "source": [
    "# Once we've installed the model, we can import it like any other Python library\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VT2rin_fVyQ6"
   },
   "outputs": [],
   "source": [
    "# This instantiates a spaCy text processor based on the installed model\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gQYz476fVyRA"
   },
   "outputs": [],
   "source": [
    "# From H.G. Wells's A Short History of the World, Project Gutenberg \n",
    "text = \"\"\"Even under the Assyrian monarchs and especially under\n",
    "Sardanapalus, Babylon had been a scene of great intellectual\n",
    "activity.  {111} Sardanapalus, though an Assyrian, had been quite\n",
    "Babylon-ized.  He made a library, a library not of paper but of\n",
    "the clay tablets that were used for writing in Mesopotamia since\n",
    "early Sumerian days.  His collection has been unearthed and is\n",
    "perhaps the most precious store of historical material in the\n",
    "world.  The last of the Chaldean line of Babylonian monarchs,\n",
    "Nabonidus, had even keener literary tastes.  He patronized\n",
    "antiquarian researches, and when a date was worked out by his\n",
    "investigators for the accession of Sargon I he commemorated the\n",
    "fact by inscriptions.  But there were many signs of disunion in\n",
    "his empire, and he sought to centralize it by bringing a number of\n",
    "the various local gods to Babylon and setting up temples to them\n",
    "there.  This device was to be practised quite successfully by the\n",
    "Romans in later times, but in Babylon it roused the jealousy of\n",
    "the powerful priesthood of Bel Marduk, the dominant god of the\n",
    "Babylonians.  They cast about for a possible alternative to\n",
    "Nabonidus and found it in Cyrus the Persian, the ruler of the\n",
    "adjacent Median Empire.  Cyrus had already distinguished himself\n",
    "by conquering Croesus, the rich king of Lydia in Eastern Asia\n",
    "Minor.  {112} He came up against Babylon, there was a battle\n",
    "outside the walls, and the gates of the city were opened to him\n",
    "(538 B.C.).  His soldiers entered the city without fighting.  The\n",
    "crown prince Belshazzar, the son of Nabonidus, was feasting, the\n",
    "Bible relates, when a hand appeared and wrote in letters of fire\n",
    "upon the wall these mystical words: _\"Mene, Mene, Tekel,\n",
    "Upharsin,\"_ which was interpreted by the prophet Daniel, whom he\n",
    "summoned to read the riddle, as \"God has numbered thy kingdom and\n",
    "finished it; thou art weighed in the balance and found wanting and\n",
    "thy kingdom is given to the Medes and Persians.\"  Possibly the\n",
    "priests of Bel Marduk knew something about that writing on the\n",
    "wall.  Belshazzar was killed that night, says the Bible.\n",
    "Nabonidus was taken prisoner, and the occupation of the city was\n",
    "so peaceful that the services of Bel Marduk continued without\n",
    "intermission.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9pYgwcqVyRL"
   },
   "source": [
    "By default, spaCy applies its entire NLP \"pipeline\" to the text as soon as it is provided to the model and outputs a processed \"doc.\"\n",
    "\n",
    "<img src=\"https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LR5v_iE3VyRG"
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnkTWvuwVyRN"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "The doc created by spaCy immediately provides access to the word-level tokens of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Zg6EB7WeVyRR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even\n",
      "under\n",
      "the\n",
      "Assyrian\n",
      "monarchs\n",
      "and\n",
      "especially\n",
      "under\n",
      "\n",
      "\n",
      "Sardanapalus\n",
      ",\n",
      "Babylon\n",
      "had\n",
      "been\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:15]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh2z0VkgVyRW"
   },
   "source": [
    "Each of these tokens has a number of properties, and we'll look a bit more closely at them in a minute.\n",
    "\n",
    "spaCy also automatically provides sentence-level segmenting (senticization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "c6Rr6LvXVyRY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even under the Assyrian monarchs and especially under\n",
      "Sardanapalus, Babylon had been a scene of great intellectual\n",
      "activity.\n",
      "--\n",
      "\n",
      " {111} Sardanapalus, though an Assyrian, had been quite\n",
      "Babylon-ized.\n",
      "--\n",
      "\n",
      " He made a library, a library not of paper but of\n",
      "the clay tablets that were used for writing in Mesopotamia since\n",
      "early Sumerian days.\n",
      "--\n",
      "\n",
      " His collection has been unearthed and is\n",
      "perhaps the most precious store of historical material in the\n",
      "world.\n",
      "--\n",
      "\n",
      " The last of the Chaldean line of Babylonian monarchs,\n",
      "Nabonidus, had even keener literary tastes.\n",
      "--\n",
      "\n",
      " He patronized\n",
      "antiquarian researches, and when a date was worked out by his\n",
      "investigators for the accession of Sargon I he commemorated the\n",
      "fact by inscriptions.\n",
      "--\n",
      "\n",
      " But there were many signs of disunion in\n",
      "his empire, and he sought to centralize it by bringing a number of\n",
      "the various local gods to Babylon and setting up temples to them\n",
      "there.\n",
      "--\n",
      "\n",
      " This device was to be practised quite successfully by the\n",
      "Romans in later times, but in Babylon it roused the jealousy of\n",
      "the powerful priesthood of Bel Marduk, the dominant god of the\n",
      "Babylonians.\n",
      "--\n",
      "\n",
      " They cast about for a possible alternative to\n",
      "Nabonidus and found it in Cyrus the Persian, the ruler of the\n",
      "adjacent Median Empire.\n",
      "--\n",
      "\n",
      " Cyrus had already distinguished himself\n",
      "by conquering Croesus, the rich king of Lydia in Eastern Asia\n",
      "Minor.\n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for sent in itertools.islice(doc.sents, 10):\n",
    "    print(sent.text + \"\\n--\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3PhTxwz7cmD"
   },
   "source": [
    "You'll notice that the line breaks in the sample text are making the extracted sentences and also the word-level tokens a bit messy. The simplest way to avoid this is just to replace all single line breaks from the text with spaces before running it throug the spaCy pipeline, i.e., as a **preprocessing** step.\n",
    "\n",
    "There are other ways to handle this within the spaCy pipeline; an important feature of spaCy is that every phase of the built-in pipeline can be replaced by a custom module. One could imagine, for example, writing a replacement sentencizer that takes advantage of the presence of two spaces between all sentences in the sample text. But we will leave that as an exercise for the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XoP90Ys12tqB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even under the Assyrian monarchs and especially under Sardanapalus, Babylon had been a scene of great intellectual activity.\n",
      "--\n",
      "\n",
      " {111} Sardanapalus, though an Assyrian, had been quite Babylon-ized.\n",
      "--\n",
      "\n",
      " He made a library, a library not of paper but of the clay tablets that were used for writing in Mesopotamia since early Sumerian days.\n",
      "--\n",
      "\n",
      " His collection has been unearthed and is perhaps the most precious store of historical material in the world.\n",
      "--\n",
      "\n",
      " The last of the Chaldean line of Babylonian monarchs, Nabonidus, had even keener literary tastes.\n",
      "--\n",
      "\n",
      " He patronized antiquarian researches, and when a date was worked out by his investigators for the accession of Sargon I he commemorated the fact by inscriptions.\n",
      "--\n",
      "\n",
      " But there were many signs of disunion in his empire, and he sought to centralize it by bringing a number of the various local gods to Babylon and setting up temples to them there.\n",
      "--\n",
      "\n",
      " This device was to be practised quite successfully by the Romans in later times, but in Babylon it roused the jealousy of the powerful priesthood of Bel Marduk, the dominant god of the Babylonians.\n",
      "--\n",
      "\n",
      " They cast about for a possible alternative to Nabonidus and found it in Cyrus the Persian, the ruler of the adjacent Median Empire.\n",
      "--\n",
      "\n",
      " Cyrus had already distinguished himself by conquering Croesus, the rich king of Lydia in Eastern Asia Minor.\n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_as_line = text.replace(\"\\n\", \" \")\n",
    "\n",
    "doc = nlp(text_as_line)\n",
    "\n",
    "for sent in itertools.islice(doc.sents, 10):\n",
    "    print(sent.text + \"\\n--\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcwG9tH9VyRd"
   },
   "source": [
    "We can collect both words and sentences into standard Python data structures (lists, in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZwnDSjL7VyRe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Even under the Assyrian monarchs and especially under Sardanapalus, Babylon had been a scene of great intellectual activity.',\n",
       " ' {111} Sardanapalus, though an Assyrian, had been quite Babylon-ized.',\n",
       " ' He made a library, a library not of paper but of the clay tablets that were used for writing in Mesopotamia since early Sumerian days.',\n",
       " ' His collection has been unearthed and is perhaps the most precious store of historical material in the world.',\n",
       " ' The last of the Chaldean line of Babylonian monarchs, Nabonidus, had even keener literary tastes.',\n",
       " ' He patronized antiquarian researches, and when a date was worked out by his investigators for the accession of Sargon I he commemorated the fact by inscriptions.',\n",
       " ' But there were many signs of disunion in his empire, and he sought to centralize it by bringing a number of the various local gods to Babylon and setting up temples to them there.',\n",
       " ' This device was to be practised quite successfully by the Romans in later times, but in Babylon it roused the jealousy of the powerful priesthood of Bel Marduk, the dominant god of the Babylonians.',\n",
       " ' They cast about for a possible alternative to Nabonidus and found it in Cyrus the Persian, the ruler of the adjacent Median Empire.',\n",
       " ' Cyrus had already distinguished himself by conquering Croesus, the rich king of Lydia in Eastern Asia Minor.',\n",
       " ' {112} He came up against Babylon, there was a battle outside the walls, and the gates of the city were opened to him (538 B.C.).',\n",
       " ' His soldiers entered the city without fighting.',\n",
       " ' The crown prince Belshazzar, the son of Nabonidus, was feasting, the Bible relates, when a hand appeared and wrote in letters of fire upon the wall these mystical words: _\"Mene, Mene, Tekel, Upharsin,\"_ which was interpreted by the prophet Daniel, whom he summoned to read the riddle, as \"God has numbered thy kingdom and finished it; thou art weighed in the balance and found wanting and thy kingdom is given to the Medes and Persians.\"',\n",
       " ' Possibly the priests of Bel Marduk knew something about that writing on the wall.',\n",
       " ' Belshazzar was killed that night, says the Bible.',\n",
       " 'Nabonidus was taken prisoner, and the occupation of the city was so peaceful that the services of Bel Marduk continued without intermission.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sent.text for sent in doc.sents]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NVJF1L5PVyRi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Even',\n",
       " 'under',\n",
       " 'the',\n",
       " 'Assyrian',\n",
       " 'monarchs',\n",
       " 'and',\n",
       " 'especially',\n",
       " 'under',\n",
       " 'Sardanapalus',\n",
       " ',',\n",
       " 'Babylon',\n",
       " 'had',\n",
       " 'been',\n",
       " 'a',\n",
       " 'scene',\n",
       " 'of',\n",
       " 'great',\n",
       " 'intellectual',\n",
       " 'activity',\n",
       " '.',\n",
       " ' ',\n",
       " '{',\n",
       " '111',\n",
       " '}',\n",
       " 'Sardanapalus',\n",
       " ',',\n",
       " 'though',\n",
       " 'an',\n",
       " 'Assyrian',\n",
       " ',',\n",
       " 'had',\n",
       " 'been',\n",
       " 'quite',\n",
       " 'Babylon',\n",
       " '-',\n",
       " 'ized',\n",
       " '.',\n",
       " ' ',\n",
       " 'He',\n",
       " 'made',\n",
       " 'a',\n",
       " 'library',\n",
       " ',',\n",
       " 'a',\n",
       " 'library',\n",
       " 'not',\n",
       " 'of',\n",
       " 'paper',\n",
       " 'but',\n",
       " 'of',\n",
       " 'the',\n",
       " 'clay',\n",
       " 'tablets',\n",
       " 'that',\n",
       " 'were',\n",
       " 'used',\n",
       " 'for',\n",
       " 'writing',\n",
       " 'in',\n",
       " 'Mesopotamia',\n",
       " 'since',\n",
       " 'early',\n",
       " 'Sumerian',\n",
       " 'days',\n",
       " '.',\n",
       " ' ',\n",
       " 'His',\n",
       " 'collection',\n",
       " 'has',\n",
       " 'been',\n",
       " 'unearthed',\n",
       " 'and',\n",
       " 'is',\n",
       " 'perhaps',\n",
       " 'the',\n",
       " 'most',\n",
       " 'precious',\n",
       " 'store',\n",
       " 'of',\n",
       " 'historical',\n",
       " 'material',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " ' ',\n",
       " 'The',\n",
       " 'last',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Chaldean',\n",
       " 'line',\n",
       " 'of',\n",
       " 'Babylonian',\n",
       " 'monarchs',\n",
       " ',',\n",
       " 'Nabonidus',\n",
       " ',',\n",
       " 'had',\n",
       " 'even',\n",
       " 'keener',\n",
       " 'literary',\n",
       " 'tastes',\n",
       " '.',\n",
       " ' ',\n",
       " 'He',\n",
       " 'patronized',\n",
       " 'antiquarian',\n",
       " 'researches',\n",
       " ',',\n",
       " 'and',\n",
       " 'when',\n",
       " 'a',\n",
       " 'date',\n",
       " 'was',\n",
       " 'worked',\n",
       " 'out',\n",
       " 'by',\n",
       " 'his',\n",
       " 'investigators',\n",
       " 'for',\n",
       " 'the',\n",
       " 'accession',\n",
       " 'of',\n",
       " 'Sargon',\n",
       " 'I',\n",
       " 'he',\n",
       " 'commemorated',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'by',\n",
       " 'inscriptions',\n",
       " '.',\n",
       " ' ',\n",
       " 'But',\n",
       " 'there',\n",
       " 'were',\n",
       " 'many',\n",
       " 'signs',\n",
       " 'of',\n",
       " 'disunion',\n",
       " 'in',\n",
       " 'his',\n",
       " 'empire',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'sought',\n",
       " 'to',\n",
       " 'centralize',\n",
       " 'it',\n",
       " 'by',\n",
       " 'bringing',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'the',\n",
       " 'various',\n",
       " 'local',\n",
       " 'gods',\n",
       " 'to',\n",
       " 'Babylon',\n",
       " 'and',\n",
       " 'setting',\n",
       " 'up',\n",
       " 'temples',\n",
       " 'to',\n",
       " 'them',\n",
       " 'there',\n",
       " '.',\n",
       " ' ',\n",
       " 'This',\n",
       " 'device',\n",
       " 'was',\n",
       " 'to',\n",
       " 'be',\n",
       " 'practised',\n",
       " 'quite',\n",
       " 'successfully',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Romans',\n",
       " 'in',\n",
       " 'later',\n",
       " 'times',\n",
       " ',',\n",
       " 'but',\n",
       " 'in',\n",
       " 'Babylon',\n",
       " 'it',\n",
       " 'roused',\n",
       " 'the',\n",
       " 'jealousy',\n",
       " 'of',\n",
       " 'the',\n",
       " 'powerful',\n",
       " 'priesthood',\n",
       " 'of',\n",
       " 'Bel',\n",
       " 'Marduk',\n",
       " ',',\n",
       " 'the',\n",
       " 'dominant',\n",
       " 'god',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Babylonians',\n",
       " '.',\n",
       " ' ',\n",
       " 'They',\n",
       " 'cast',\n",
       " 'about',\n",
       " 'for',\n",
       " 'a',\n",
       " 'possible',\n",
       " 'alternative',\n",
       " 'to',\n",
       " 'Nabonidus',\n",
       " 'and',\n",
       " 'found',\n",
       " 'it',\n",
       " 'in',\n",
       " 'Cyrus',\n",
       " 'the',\n",
       " 'Persian',\n",
       " ',',\n",
       " 'the',\n",
       " 'ruler',\n",
       " 'of',\n",
       " 'the',\n",
       " 'adjacent',\n",
       " 'Median',\n",
       " 'Empire',\n",
       " '.',\n",
       " ' ',\n",
       " 'Cyrus',\n",
       " 'had',\n",
       " 'already',\n",
       " 'distinguished',\n",
       " 'himself',\n",
       " 'by',\n",
       " 'conquering',\n",
       " 'Croesus',\n",
       " ',',\n",
       " 'the',\n",
       " 'rich',\n",
       " 'king',\n",
       " 'of',\n",
       " 'Lydia',\n",
       " 'in',\n",
       " 'Eastern',\n",
       " 'Asia',\n",
       " 'Minor',\n",
       " '.',\n",
       " ' ',\n",
       " '{',\n",
       " '112',\n",
       " '}',\n",
       " 'He',\n",
       " 'came',\n",
       " 'up',\n",
       " 'against',\n",
       " 'Babylon',\n",
       " ',',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'battle',\n",
       " 'outside',\n",
       " 'the',\n",
       " 'walls',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'gates',\n",
       " 'of',\n",
       " 'the',\n",
       " 'city',\n",
       " 'were',\n",
       " 'opened',\n",
       " 'to',\n",
       " 'him',\n",
       " '(',\n",
       " '538',\n",
       " 'B.C.',\n",
       " ')',\n",
       " '.',\n",
       " ' ',\n",
       " 'His',\n",
       " 'soldiers',\n",
       " 'entered',\n",
       " 'the',\n",
       " 'city',\n",
       " 'without',\n",
       " 'fighting',\n",
       " '.',\n",
       " ' ',\n",
       " 'The',\n",
       " 'crown',\n",
       " 'prince',\n",
       " 'Belshazzar',\n",
       " ',',\n",
       " 'the',\n",
       " 'son',\n",
       " 'of',\n",
       " 'Nabonidus',\n",
       " ',',\n",
       " 'was',\n",
       " 'feasting',\n",
       " ',',\n",
       " 'the',\n",
       " 'Bible',\n",
       " 'relates',\n",
       " ',',\n",
       " 'when',\n",
       " 'a',\n",
       " 'hand',\n",
       " 'appeared',\n",
       " 'and',\n",
       " 'wrote',\n",
       " 'in',\n",
       " 'letters',\n",
       " 'of',\n",
       " 'fire',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'wall',\n",
       " 'these',\n",
       " 'mystical',\n",
       " 'words',\n",
       " ':',\n",
       " '_',\n",
       " '\"',\n",
       " 'Mene',\n",
       " ',',\n",
       " 'Mene',\n",
       " ',',\n",
       " 'Tekel',\n",
       " ',',\n",
       " 'Upharsin',\n",
       " ',',\n",
       " '\"',\n",
       " '_',\n",
       " 'which',\n",
       " 'was',\n",
       " 'interpreted',\n",
       " 'by',\n",
       " 'the',\n",
       " 'prophet',\n",
       " 'Daniel',\n",
       " ',',\n",
       " 'whom',\n",
       " 'he',\n",
       " 'summoned',\n",
       " 'to',\n",
       " 'read',\n",
       " 'the',\n",
       " 'riddle',\n",
       " ',',\n",
       " 'as',\n",
       " '\"',\n",
       " 'God',\n",
       " 'has',\n",
       " 'numbered',\n",
       " 'thy',\n",
       " 'kingdom',\n",
       " 'and',\n",
       " 'finished',\n",
       " 'it',\n",
       " ';',\n",
       " 'thou',\n",
       " 'art',\n",
       " 'weighed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'balance',\n",
       " 'and',\n",
       " 'found',\n",
       " 'wanting',\n",
       " 'and',\n",
       " 'thy',\n",
       " 'kingdom',\n",
       " 'is',\n",
       " 'given',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Medes',\n",
       " 'and',\n",
       " 'Persians',\n",
       " '.',\n",
       " '\"',\n",
       " ' ',\n",
       " 'Possibly',\n",
       " 'the',\n",
       " 'priests',\n",
       " 'of',\n",
       " 'Bel',\n",
       " 'Marduk',\n",
       " 'knew',\n",
       " 'something',\n",
       " 'about',\n",
       " 'that',\n",
       " 'writing',\n",
       " 'on',\n",
       " 'the',\n",
       " 'wall',\n",
       " '.',\n",
       " ' ',\n",
       " 'Belshazzar',\n",
       " 'was',\n",
       " 'killed',\n",
       " 'that',\n",
       " 'night',\n",
       " ',',\n",
       " 'says',\n",
       " 'the',\n",
       " 'Bible',\n",
       " '.',\n",
       " 'Nabonidus',\n",
       " 'was',\n",
       " 'taken',\n",
       " 'prisoner',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'occupation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'city',\n",
       " 'was',\n",
       " 'so',\n",
       " 'peaceful',\n",
       " 'that',\n",
       " 'the',\n",
       " 'services',\n",
       " 'of',\n",
       " 'Bel',\n",
       " 'Marduk',\n",
       " 'continued',\n",
       " 'without',\n",
       " 'intermission',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [token.text for token in doc]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xndApEFuVyRn"
   },
   "source": [
    "### Filtering tokens\n",
    "\n",
    "After extracting the tokens, we can use some attributes and methods provided by spaCy, along with some vanilla Python methods, to filter the tokens to just the types we're interested in analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZSHaSQWqVyRo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN: Even            IS_PUNCTUATION: False\n",
      "TOKEN: under           IS_PUNCTUATION: False\n",
      "TOKEN: the             IS_PUNCTUATION: False\n",
      "TOKEN: Assyrian        IS_PUNCTUATION: False\n",
      "TOKEN: monarchs        IS_PUNCTUATION: False\n",
      "TOKEN: and             IS_PUNCTUATION: False\n",
      "TOKEN: especially      IS_PUNCTUATION: False\n",
      "TOKEN: under           IS_PUNCTUATION: False\n",
      "TOKEN: Sardanapalus    IS_PUNCTUATION: False\n",
      "TOKEN: ,               IS_PUNCTUATION: True\n",
      "TOKEN: Babylon         IS_PUNCTUATION: False\n",
      "TOKEN: had             IS_PUNCTUATION: False\n",
      "TOKEN: been            IS_PUNCTUATION: False\n",
      "TOKEN: a               IS_PUNCTUATION: False\n",
      "TOKEN: scene           IS_PUNCTUATION: False\n",
      "TOKEN: of              IS_PUNCTUATION: False\n",
      "TOKEN: great           IS_PUNCTUATION: False\n",
      "TOKEN: intellectual    IS_PUNCTUATION: False\n",
      "TOKEN: activity        IS_PUNCTUATION: False\n",
      "TOKEN: .               IS_PUNCTUATION: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Even,\n",
       " under,\n",
       " the,\n",
       " Assyrian,\n",
       " monarchs,\n",
       " and,\n",
       " especially,\n",
       " under,\n",
       " Sardanapalus,\n",
       " Babylon,\n",
       " had,\n",
       " been,\n",
       " a,\n",
       " scene,\n",
       " of,\n",
       " great,\n",
       " intellectual,\n",
       " activity,\n",
       "  ,\n",
       " 111]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we're only interested in analyzing word tokens, we can remove punctuation:\n",
    "for token in doc[:20]:\n",
    "    print(f'TOKEN: {token.text:15} IS_PUNCTUATION: {token.is_punct:}')\n",
    "no_punct = [token for token in doc if token.is_punct == False]\n",
    "\n",
    "no_punct[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "K63rP_PJVyRs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even\n",
      "under\n",
      "the\n",
      "Assyrian\n",
      "monarchs\n",
      "and\n",
      "especially\n",
      "under\n",
      "Sardanapalus\n",
      "Babylon\n",
      "had\n",
      "been\n",
      "a\n",
      "scene\n",
      "of\n",
      "great\n",
      "intellectual\n",
      "activity\n",
      "111\n",
      "Sardanapalus\n",
      "though\n",
      "an\n",
      "Assyrian\n",
      "had\n",
      "been\n",
      "quite\n",
      "Babylon\n",
      "ized\n",
      "He\n",
      "made\n"
     ]
    }
   ],
   "source": [
    "# There are still some space tokens; here's how to remove spaces and newlines:\n",
    "no_punct_or_space = [token for token in doc if token.is_punct == False and token.is_space == False]\n",
    "for token in no_punct_or_space[:30]:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YHjzgbbgVyRw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['even',\n",
       " 'under',\n",
       " 'the',\n",
       " 'assyrian',\n",
       " 'monarchs',\n",
       " 'and',\n",
       " 'especially',\n",
       " 'under',\n",
       " 'sardanapalus',\n",
       " 'babylon',\n",
       " 'had',\n",
       " 'been',\n",
       " 'a',\n",
       " 'scene',\n",
       " 'of',\n",
       " 'great',\n",
       " 'intellectual',\n",
       " 'activity',\n",
       " 'sardanapalus',\n",
       " 'though',\n",
       " 'an',\n",
       " 'assyrian',\n",
       " 'had',\n",
       " 'been',\n",
       " 'quite',\n",
       " 'babylon',\n",
       " 'ized',\n",
       " 'he',\n",
       " 'made',\n",
       " 'a']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's say we also want to remove numbers and lowercase everything that remains\n",
    "lower_alpha = [token.lower_ for token in no_punct_or_space if token.is_alpha == True]\n",
    "lower_alpha[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbpLhAqnVyR1"
   },
   "source": [
    "One additional common filtering step is to remove stopwords. In theory, stopwords can be any words we're not interested in analyzing, but in practice, they are often the most common words in a language that do not carry much semantic information (e.g., articles, conjunctions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "STyEpj96VyR2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assyrian',\n",
       " 'monarchs',\n",
       " 'especially',\n",
       " 'sardanapalus',\n",
       " 'babylon',\n",
       " 'scene',\n",
       " 'great',\n",
       " 'intellectual',\n",
       " 'activity',\n",
       " 'sardanapalus',\n",
       " 'assyrian',\n",
       " 'babylon',\n",
       " 'ized',\n",
       " 'library',\n",
       " 'library',\n",
       " 'paper',\n",
       " 'clay',\n",
       " 'tablets',\n",
       " 'writing',\n",
       " 'mesopotamia',\n",
       " 'early',\n",
       " 'sumerian',\n",
       " 'days',\n",
       " 'collection',\n",
       " 'unearthed',\n",
       " 'precious',\n",
       " 'store',\n",
       " 'historical',\n",
       " 'material',\n",
       " 'world']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = [token.lower_ for token in no_punct_or_space if token.is_alpha == True and token.is_stop == False]\n",
    "clean[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWe736X7mwKF"
   },
   "source": [
    "We've used spaCy's built-in stopword list; membership in this list determines the property `is_stop` for each token. It's good practice to be wary of any built-in stopword list, however -- there's a good chance you will want to remove some words that aren't on the list and to include some that are, especially if you're working with specialized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tP8b8upcmx5q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['monarchs',\n",
       " 'especially',\n",
       " 'sardanapalus',\n",
       " 'scene',\n",
       " 'great',\n",
       " 'intellectual',\n",
       " 'activity',\n",
       " 'sardanapalus',\n",
       " 'ized',\n",
       " 'library',\n",
       " 'library',\n",
       " 'paper',\n",
       " 'clay',\n",
       " 'tablets',\n",
       " 'writing',\n",
       " 'mesopotamia',\n",
       " 'early',\n",
       " 'sumerian',\n",
       " 'days',\n",
       " 'collection',\n",
       " 'unearthed',\n",
       " 'precious',\n",
       " 'store',\n",
       " 'historical',\n",
       " 'material',\n",
       " 'world',\n",
       " 'chaldean',\n",
       " 'line',\n",
       " 'babylonian',\n",
       " 'monarchs',\n",
       " 'nabonidus',\n",
       " 'keener',\n",
       " 'literary',\n",
       " 'tastes',\n",
       " 'patronized',\n",
       " 'antiquarian',\n",
       " 'researches',\n",
       " 'date',\n",
       " 'worked',\n",
       " 'investigators',\n",
       " 'accession',\n",
       " 'sargon',\n",
       " 'commemorated',\n",
       " 'fact',\n",
       " 'inscriptions',\n",
       " 'signs',\n",
       " 'disunion',\n",
       " 'empire',\n",
       " 'sought',\n",
       " 'centralize',\n",
       " 'bringing',\n",
       " 'number',\n",
       " 'local',\n",
       " 'gods',\n",
       " 'setting',\n",
       " 'temples',\n",
       " 'device',\n",
       " 'practised',\n",
       " 'successfully',\n",
       " 'romans',\n",
       " 'later',\n",
       " 'times',\n",
       " 'roused',\n",
       " 'jealousy',\n",
       " 'powerful',\n",
       " 'priesthood',\n",
       " 'bel',\n",
       " 'marduk',\n",
       " 'dominant',\n",
       " 'god',\n",
       " 'babylonians',\n",
       " 'cast',\n",
       " 'possible',\n",
       " 'alternative',\n",
       " 'nabonidus',\n",
       " 'found',\n",
       " 'cyrus',\n",
       " 'persian',\n",
       " 'ruler',\n",
       " 'adjacent',\n",
       " 'median',\n",
       " 'empire',\n",
       " 'cyrus',\n",
       " 'distinguished',\n",
       " 'conquering',\n",
       " 'croesus',\n",
       " 'rich',\n",
       " 'king',\n",
       " 'lydia',\n",
       " 'eastern',\n",
       " 'asia',\n",
       " 'minor',\n",
       " 'came',\n",
       " 'battle',\n",
       " 'outside',\n",
       " 'walls',\n",
       " 'gates',\n",
       " 'city',\n",
       " 'opened',\n",
       " 'soldiers',\n",
       " 'entered',\n",
       " 'city',\n",
       " 'fighting',\n",
       " 'crown',\n",
       " 'prince',\n",
       " 'belshazzar',\n",
       " 'son',\n",
       " 'nabonidus',\n",
       " 'feasting',\n",
       " 'bible',\n",
       " 'relates',\n",
       " 'hand',\n",
       " 'appeared',\n",
       " 'wrote',\n",
       " 'letters',\n",
       " 'fire',\n",
       " 'wall',\n",
       " 'mystical',\n",
       " 'words',\n",
       " 'mene',\n",
       " 'mene',\n",
       " 'tekel',\n",
       " 'upharsin',\n",
       " 'interpreted',\n",
       " 'prophet',\n",
       " 'daniel',\n",
       " 'summoned',\n",
       " 'read',\n",
       " 'riddle',\n",
       " 'god',\n",
       " 'numbered',\n",
       " 'thy',\n",
       " 'kingdom',\n",
       " 'finished',\n",
       " 'thou',\n",
       " 'art',\n",
       " 'weighed',\n",
       " 'balance',\n",
       " 'found',\n",
       " 'wanting',\n",
       " 'thy',\n",
       " 'kingdom',\n",
       " 'given',\n",
       " 'medes',\n",
       " 'persians',\n",
       " 'possibly',\n",
       " 'priests',\n",
       " 'bel',\n",
       " 'marduk',\n",
       " 'knew',\n",
       " 'writing',\n",
       " 'wall',\n",
       " 'belshazzar',\n",
       " 'killed',\n",
       " 'night',\n",
       " 'says',\n",
       " 'bible',\n",
       " 'nabonidus',\n",
       " 'taken',\n",
       " 'prisoner',\n",
       " 'occupation',\n",
       " 'city',\n",
       " 'peaceful',\n",
       " 'services',\n",
       " 'bel',\n",
       " 'marduk',\n",
       " 'continued',\n",
       " 'intermission']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll just pick a couple of words we know are in the example\n",
    "custom_stopwords = [\"assyrian\", \"babylon\"]\n",
    "\n",
    "custom_clean = [token for token in clean if token not in custom_stopwords]\n",
    "custom_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ENXxjBHVyR8"
   },
   "source": [
    "At this point, we have a list of lower-cased tokens that doesn't contain punctuation, white-space, numbers, or stopwords. Depending on your analytical goals, you may or may not want to do this much cleaning, but hopefully you have a greater appreciation for the kinds of cleaning that can be done with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSGJfxiaVyR-"
   },
   "source": [
    "### Counting tokens\n",
    "\n",
    "Now that we've used spaCy to tokenize and clean our text, we can begin one of the most fundamental text analysis tasks: counting words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NEFjnPPLVySA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in document:  442\n",
      "Number of tokens in cleaned document:  175\n",
      "Number of unique tokens in cleaned document:  147\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens in document: \", len(doc))\n",
    "print(\"Number of tokens in cleaned document: \", len(clean))\n",
    "print(\"Number of unique tokens in cleaned document: \", len(set(clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MZwZ3En1VySY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 36),\n",
       " (',', 26),\n",
       " ('of', 20),\n",
       " ('.', 16),\n",
       " (' ', 14),\n",
       " ('and', 13),\n",
       " ('in', 9),\n",
       " ('a', 8),\n",
       " ('was', 8),\n",
       " ('to', 8),\n",
       " ('he', 6),\n",
       " ('by', 6),\n",
       " ('babylon', 5),\n",
       " ('had', 4),\n",
       " ('that', 4),\n",
       " ('his', 4),\n",
       " ('nabonidus', 4),\n",
       " ('it', 4),\n",
       " ('\"', 4),\n",
       " ('been', 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "full_counter = Counter([token.lower_ for token in doc])\n",
    "full_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cIrMQFp6VySg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('babylon', 5),\n",
       " ('nabonidus', 4),\n",
       " ('bel', 3),\n",
       " ('marduk', 3),\n",
       " ('city', 3),\n",
       " ('assyrian', 2),\n",
       " ('monarchs', 2),\n",
       " ('sardanapalus', 2),\n",
       " ('library', 2),\n",
       " ('writing', 2),\n",
       " ('empire', 2),\n",
       " ('god', 2),\n",
       " ('found', 2),\n",
       " ('cyrus', 2),\n",
       " ('belshazzar', 2),\n",
       " ('bible', 2),\n",
       " ('wall', 2),\n",
       " ('mene', 2),\n",
       " ('thy', 2),\n",
       " ('kingdom', 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_counter = Counter(clean)\n",
    "cleaned_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRNYHP7wVySv"
   },
   "source": [
    "## Part-of-speech tagging\n",
    "\n",
    "Let's consider some other aspects of the text that spaCy exposes for us. One of the most noteworthy features is part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RLVUUOT9VySw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even ADV\n",
      "under ADP\n",
      "the DET\n",
      "Assyrian ADJ\n",
      "monarchs NOUN\n",
      "and CCONJ\n",
      "especially ADV\n",
      "under ADP\n",
      "Sardanapalus PROPN\n",
      ", PUNCT\n",
      "Babylon PROPN\n",
      "had AUX\n",
      "been AUX\n",
      "a DET\n",
      "scene NOUN\n",
      "of ADP\n",
      "great ADJ\n",
      "intellectual ADJ\n",
      "activity NOUN\n",
      ". PUNCT\n",
      "  SPACE\n",
      "{ PUNCT\n",
      "111 NUM\n",
      "} PUNCT\n",
      "Sardanapalus NOUN\n",
      ", PUNCT\n",
      "though SCONJ\n",
      "an DET\n",
      "Assyrian PROPN\n",
      ", PUNCT\n"
     ]
    }
   ],
   "source": [
    "# spaCy provides two levels of POS tagging. Here's the more general level.\n",
    "for token in doc[:30]:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FbB2eeMTVyS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even RB\n",
      "under IN\n",
      "the DT\n",
      "Assyrian JJ\n",
      "monarchs NNS\n",
      "and CC\n",
      "especially RB\n",
      "under IN\n",
      "Sardanapalus NNP\n",
      ", ,\n",
      "Babylon NNP\n",
      "had VBD\n",
      "been VBN\n",
      "a DT\n",
      "scene NN\n",
      "of IN\n",
      "great JJ\n",
      "intellectual JJ\n",
      "activity NN\n",
      ". .\n",
      "  _SP\n",
      "{ -LRB-\n",
      "111 CD\n",
      "} -RRB-\n",
      "Sardanapalus NN\n",
      ", ,\n",
      "though IN\n",
      "an DT\n",
      "Assyrian NNP\n",
      ", ,\n"
     ]
    }
   ],
   "source": [
    "# spaCy also provides the more specific Penn Treenbank tags.\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "for token in doc[:30]:\n",
    "    print(token.text, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6X3cbcmVyS4"
   },
   "source": [
    "We can count the occurrences of each part of speech in the text, which may be useful for document classification (fiction may have different proportions of parts of speech relative to nonfiction, for example) or stylistic analysis (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wVey8EXsVyS5"
   },
   "outputs": [],
   "source": [
    "nouns = [token for token in doc if token.pos_ == \"NOUN\"]\n",
    "verbs = [token for token in doc if token.pos_ == \"VERB\"]\n",
    "proper_nouns = [token for token in doc if token.pos_ == \"PROPN\"]\n",
    "adjectives = [token for token in doc if token.pos_ == \"ADJ\"]\n",
    "adverbs = [token for token in doc if token.pos_ == \"ADV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Qp6pH7VjVyTC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nouns': 66, 'verbs': 43, 'proper_nouns': 46, 'adjectives': 23, 'adverbs': 12}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_counts = {\n",
    "    \"nouns\": len(nouns),\n",
    "    \"verbs\": len(verbs),\n",
    "    \"proper_nouns\": len(proper_nouns),\n",
    "    \"adjectives\": len(adjectives),\n",
    "    \"adverbs\": len(adverbs) \n",
    "}\n",
    "\n",
    "pos_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3y4af1Z7bR4C"
   },
   "source": [
    "spaCy performs morphosyntactic analysis of individual tokens, including lemmatizing inflected or conjugated forms to their base (dictionary) forms. Reducing words to their lemmatized forms can help to make a large corpus more manageable and is generally more effective than just stemming words (trimming the inflected/conjugated endings of words until just the base portion remains), but should only be done if the inflections are not relevant to your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "LpYHR5pgb0Tn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monarchs        monarch\n",
      "Sardanapalus    sardanapalus\n",
      "ized            ize\n",
      "made            make\n",
      "tablets         tablet\n",
      "used            use\n",
      "writing         write\n",
      "days            day\n",
      "unearthed       unearth\n",
      "monarchs        monarch\n",
      "had             have\n",
      "tastes          taste\n",
      "patronized      patronize\n",
      "researches      research\n",
      "worked          work\n",
      "investigators   investigator\n",
      "commemorated    commemorate\n",
      "inscriptions    inscription\n",
      "were            be\n",
      "signs           sign\n",
      "sought          seek\n",
      "bringing        bring\n",
      "gods            god\n",
      "setting         set\n",
      "temples         temple\n",
      "practised       practise\n",
      "times           time\n",
      "roused          rouse\n",
      "found           find\n",
      "distinguished   distinguish\n",
      "conquering      conquer\n",
      "came            come\n",
      "was             be\n",
      "walls           wall\n",
      "gates           gate\n",
      "opened          open\n",
      "soldiers        soldier\n",
      "entered         enter\n",
      "fighting        fight\n",
      "feasting        feast\n",
      "relates         relate\n",
      "appeared        appear\n",
      "wrote           write\n",
      "letters         letter\n",
      "words           word\n",
      "interpreted     interpret\n",
      "summoned        summon\n",
      "numbered        number\n",
      "finished        finish\n",
      "weighed         weigh\n",
      "found           find\n",
      "wanting         want\n",
      "given           give\n",
      "priests         priest\n",
      "knew            know\n",
      "killed          kill\n",
      "says            say\n",
      "taken           take\n",
      "services        service\n",
      "continued       continue\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ in [\"NOUN\", \"VERB\"] and token.orth_ != token.lemma_:\n",
    "        print(f\"{token.text:15} {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_1y3C5LVyTP"
   },
   "source": [
    "### Parsing\n",
    "\n",
    "spaCy's trained models also provide full dependency parsing, tagging word tokens with their syntactic relations to other tokens. This functionality drives spaCy's built-in senticization as well.\n",
    "\n",
    "We won't spend much time exploring this feature, but it's useful to see how it enables the extraction of multi-word \"noun chunks\" from the text. Note also that textacy (discussed below) has a built-in function to extract subject-verb-object triples from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qM28u6ZwE6v5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Assyrian monarchs\n",
      "Sardanapalus\n",
      "Babylon\n",
      "a scene\n",
      "great intellectual activity\n",
      " {111} Sardanapalus\n",
      "an Assyrian\n",
      "He\n",
      "a library\n",
      "a library\n",
      "paper\n",
      "the clay tablets\n",
      "that\n",
      "Mesopotamia\n",
      "early Sumerian days\n",
      "His collection\n",
      "the most precious store\n",
      "historical material\n",
      "the world\n",
      "The last\n"
     ]
    }
   ],
   "source": [
    "for chunk in itertools.islice(doc.noun_chunks, 20):\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29Mqf_S0VyTR"
   },
   "source": [
    "## Named-entity recognition\n",
    "\n",
    "spaCy's models do a pretty good job of identifying and classifying named entities (people, places, organizations).\n",
    "\n",
    "It is also fairly easy to customize and fine-tune these models by providing additional training data (e.g., texts with entities labeled according to the desired scheme), but that's out of the scope of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "KodfOLmHVyTS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assyrian             NORP            Nationalities or religious or political groups\n",
      "Sardanapalus         WORK_OF_ART     Titles of books, songs, etc.\n",
      "111                  CARDINAL        Numerals that do not fall under another type\n",
      "Assyrian             NORP            Nationalities or religious or political groups\n",
      "Mesopotamia          LOC             Non-GPE locations, mountain ranges, bodies of water\n",
      "early Sumerian days  DATE            Absolute or relative dates or periods\n",
      "Chaldean             NORP            Nationalities or religious or political groups\n",
      "Babylonian           NORP            Nationalities or religious or political groups\n",
      "Nabonidus            ORG             Companies, agencies, institutions, etc.\n",
      "Sargon               ORG             Companies, agencies, institutions, etc.\n",
      "Romans               NORP            Nationalities or religious or political groups\n",
      "Bel Marduk           PERSON          People, including fictional\n",
      "Babylonians          NORP            Nationalities or religious or political groups\n",
      "Nabonidus            ORG             Companies, agencies, institutions, etc.\n",
      "Cyrus the Persian    GPE             Countries, cities, states\n",
      "Median Empire        GPE             Countries, cities, states\n",
      "Lydia                PERSON          People, including fictional\n",
      "Eastern Asia Minor   LOC             Non-GPE locations, mountain ranges, bodies of water\n",
      "112                  CARDINAL        Numerals that do not fall under another type\n",
      "538                  CARDINAL        Numerals that do not fall under another type\n",
      "B.C.                 GPE             Countries, cities, states\n",
      "Nabonidus            ORG             Companies, agencies, institutions, etc.\n",
      "Bible                WORK_OF_ART     Titles of books, songs, etc.\n",
      "Tekel                GPE             Countries, cities, states\n",
      "Upharsin             GPE             Countries, cities, states\n",
      "Medes                ORG             Companies, agencies, institutions, etc.\n",
      "Persians             NORP            Nationalities or religious or political groups\n",
      "Bel Marduk           PERSON          People, including fictional\n",
      "Bible                WORK_OF_ART     Titles of books, songs, etc.\n",
      "Nabonidus            ORG             Companies, agencies, institutions, etc.\n",
      "Bel Marduk           PERSON          People, including fictional\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:20} {ent.label_:15} {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGCSdUMAVyTa"
   },
   "source": [
    "What if we only care about geo-political entities or locations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AhIk-M0DVyTc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mesopotamia', 'LOC'),\n",
       " ('Cyrus the Persian', 'GPE'),\n",
       " ('Median Empire', 'GPE'),\n",
       " ('Eastern Asia Minor', 'LOC'),\n",
       " ('B.C.', 'GPE'),\n",
       " ('Tekel', 'GPE'),\n",
       " ('Upharsin', 'GPE')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_filtered = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
    "ent_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buJBVUPQVyTe"
   },
   "source": [
    "### Visualizing Parses\n",
    "\n",
    "The built-in displaCy visualizer can render the results of the named-entity recognition, as well as the dependency parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "f3Ra-HtPVyTf"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "NIO_FEoLVyTi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Even under the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Assyrian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " monarchs and especially under \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sardanapalus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ", Babylon had been a scene of great intellectual activity.  {\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    111\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "} Sardanapalus, though an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Assyrian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", had been quite Babylon-ized.  He made a library, a library not of paper but of the clay tablets that were used for writing in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mesopotamia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    early Sumerian days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".  His collection has been unearthed and is perhaps the most precious store of historical material in the world.  The last of the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chaldean\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " line of \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Babylonian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " monarchs, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nabonidus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", had even keener literary tastes.  He patronized antiquarian researches, and when a date was worked out by his investigators for the accession of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sargon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " I he commemorated the fact by inscriptions.  But there were many signs of disunion in his empire, and he sought to centralize it by bringing a number of the various local gods to Babylon and setting up temples to them there.  This device was to be practised quite successfully by the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Romans\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " in later times, but in Babylon it roused the jealousy of the powerful priesthood of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bel Marduk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", the dominant god of the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Babylonians\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ".  They cast about for a possible alternative to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nabonidus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and found it in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cyrus the Persian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", the ruler of the adjacent \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Median Empire\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".  Cyrus had already distinguished himself by conquering Croesus, the rich king of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lydia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Eastern Asia Minor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".  {\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    112\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "} He came up against Babylon, there was a battle outside the walls, and the gates of the city were opened to him (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    538\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    B.C.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ").  His soldiers entered the city without fighting.  The crown prince Belshazzar, the son of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nabonidus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", was feasting, the \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bible\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " relates, when a hand appeared and wrote in letters of fire upon the wall these mystical words: _&quot;Mene, Mene, \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tekel\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Upharsin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ",&quot;_ which was interpreted by the prophet Daniel, whom he summoned to read the riddle, as &quot;God has numbered thy kingdom and finished it; thou art weighed in the balance and found wanting and thy kingdom is given to the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Medes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Persians\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ".&quot;  Possibly the priests of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bel Marduk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " knew something about that writing on the wall.  Belshazzar was killed that night, says the \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bible\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nabonidus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " was taken prisoner, and the occupation of the city was so peaceful that the services of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bel Marduk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " continued without intermission.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU5dIAnMiODg"
   },
   "source": [
    "### Activity\n",
    "\n",
    "Pick either a particular part of speech or a named entity type, and write code to determine the most common words of that type in the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mro3MhI-ieQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F5P7cbMVyTl"
   },
   "source": [
    "# Corpus-level analysis with `textacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCwkgf9pVyTl"
   },
   "source": [
    "Let's shift to thinking about a whole corpus rather than a single document. We could analyze multiple documents with spaCy and then knit the results together with some extra Python. Instead, though, we're going to take advantage of textacy, a library built on spaCy that adds corpus analysis features.\n",
    "\n",
    "For reference, here's the [online documentation for textacy](https://textacy.readthedocs.io/en/stable/api_reference/root.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Jp3AcJezVyTn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textacy in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (4.59.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (1.20.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (0.24.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (2.5)\n",
      "Requirement already satisfied: cachetools>=4.0.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (4.2.4)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (1.6.2)\n",
      "Requirement already satisfied: pyphen>=0.10.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (0.11.0)\n",
      "Requirement already satisfied: spacy>=3.0.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (3.2.0)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (0.11.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (2.25.1)\n",
      "Requirement already satisfied: jellyfish>=0.8.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (0.8.9)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from textacy) (1.0.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from cytoolz>=0.10.1->textacy) (0.11.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from networkx>=2.0->textacy) (5.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->textacy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->textacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->textacy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->textacy) (4.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.19.0->textacy) (2.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (0.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (8.0.13)\n",
      "Requirement already satisfied: jinja2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (1.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (0.7.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (2.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (20.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (3.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (0.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (1.0.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (3.0.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=3.0.0->textacy) (3.7.4.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy>=3.0.0->textacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMqTW64-VyTp"
   },
   "source": [
    "## Generating corpora\n",
    "\n",
    "We'll use some of the data that is included in textacy as our corpus. It is certainly possible to build your own corpus by importing data from files in plain text, XML, JSON, CSV or other formats, but working with one of textacy's \"pre-cooked\" datasets simplifies things a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "t8HmDN36VyTq"
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VYKe7-BCVyTs"
   },
   "outputs": [],
   "source": [
    "# We'll work with a dataset of ~8,400 (\"almost all\") U.S. Supreme Court\n",
    "# decisions from November 1946 through June 2016\n",
    "# https://github.com/bdewilde/textacy-data/releases/tag/supreme_court_py3_v1.0\n",
    "data = textacy.datasets.SupremeCourt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "9xmidNbxVyTu"
   },
   "outputs": [],
   "source": [
    "data.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzjG0hgUPP1M"
   },
   "source": [
    "The documentation indicates the metadata that is available with each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "WHMbaig9VyTx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module textacy.datasets.supreme_court in textacy.datasets:\n",
      "\n",
      "NAME\n",
      "    textacy.datasets.supreme_court\n",
      "\n",
      "DESCRIPTION\n",
      "    Supreme Court decisions\n",
      "    -----------------------\n",
      "    \n",
      "    A collection of ~8.4k (almost all) decisions issued by the U.S. Supreme Court\n",
      "    from November 1946 through June 2016 -- the \"modern\" era.\n",
      "    \n",
      "    Records include the following data:\n",
      "    \n",
      "        - ``text``: Full text of the Court's decision.\n",
      "        - ``case_name``: Name of the court case, in all caps.\n",
      "        - ``argument_date``: Date on which the case was argued before the Court, as\n",
      "          an ISO-formatted string (\"YYYY-MM-DD\").\n",
      "        - ``decision_date``: Date on which the Court's decision was announced, as\n",
      "          an ISO-formatted string (\"YYYY-MM-DD\").\n",
      "        - ``decision_direction``: Ideological direction of the majority's decision:\n",
      "          one of \"conservative\", \"liberal\", or \"unspecifiable\".\n",
      "        - ``maj_opinion_author``: Name of the majority opinion's author, if available\n",
      "          and identifiable, as an integer code whose mapping is given in\n",
      "          :attr:`SupremeCourt.opinion_author_codes`.\n",
      "        - ``n_maj_votes``: Number of justices voting in the majority.\n",
      "        - ``n_min_votes``: Number of justices voting in the minority.\n",
      "        - ``issue``: Subject matter of the case's core disagreement (e.g. \"affirmative\n",
      "          action\") rather than its legal basis (e.g. \"the equal protection clause\"),\n",
      "          as a string code whose mapping is given in :attr:`SupremeCourt.issue_codes`.\n",
      "        - ``issue_area``: Higher-level categorization of the issue (e.g. \"Civil Rights\"),\n",
      "          as an integer code whose mapping is given in :attr:`SupremeCourt.issue_area_codes`.\n",
      "        - ``us_cite_id``: Citation identifier for each case according to the official\n",
      "          United States Reports. Note: There are ~300 cases with duplicate ids,\n",
      "          and it's not clear if that's \"correct\" or a data quality problem.\n",
      "    \n",
      "    The text in this dataset was derived from FindLaw's searchable database of court\n",
      "    cases: http://caselaw.findlaw.com/court/us-supreme-court.\n",
      "    \n",
      "    The metadata was extracted without modification from the Supreme Court Database:\n",
      "    Harold J. Spaeth, Lee Epstein, et al. 2016 Supreme Court Database, Version 2016\n",
      "    Release 1. http://supremecourtdatabase.org. Its license is CC BY-NC 3.0 US:\n",
      "    https://creativecommons.org/licenses/by-nc/3.0/us/.\n",
      "    \n",
      "    This dataset's creation was inspired by a blog post by Emily Barry:\n",
      "    http://www.emilyinamillion.me/blog/2016/7/13/visualizing-supreme-court-topics-over-time.\n",
      "    \n",
      "    The two datasets were merged through much munging and a carefully\n",
      "    trained model using the ``dedupe`` package. The model's duplicate threshold\n",
      "    was set so as to maximize the F-score where precision had twice as much\n",
      "    weight as recall. Still, given occasionally baffling inconsistencies in case\n",
      "    naming, citation ids, and decision dates, a very small percentage of texts\n",
      "    may be incorrectly matched to metadata. (Sorry.)\n",
      "\n",
      "CLASSES\n",
      "    textacy.datasets.base.Dataset(builtins.object)\n",
      "        SupremeCourt\n",
      "    \n",
      "    class SupremeCourt(textacy.datasets.base.Dataset)\n",
      "     |  SupremeCourt(data_dir: Union[str, pathlib.Path] = PosixPath('/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages/textacy/data/supreme_court'))\n",
      "     |  \n",
      "     |  Stream a collection of US Supreme Court decisions from a compressed json file on disk,\n",
      "     |  either as texts or text + metadata pairs.\n",
      "     |  \n",
      "     |  Download the data (one time only!) from the textacy-data repo\n",
      "     |  (https://github.com/bdewilde/textacy-data), and save its contents to disk::\n",
      "     |  \n",
      "     |      >>> import textacy.datasets\n",
      "     |      >>> ds = textacy.datasets.SupremeCourt()\n",
      "     |      >>> ds.download()\n",
      "     |      >>> ds.info\n",
      "     |      {'name': 'supreme_court',\n",
      "     |       'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court',\n",
      "     |       'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "     |  \n",
      "     |  Iterate over decisions as texts or records with both text and metadata::\n",
      "     |  \n",
      "     |      >>> for text in ds.texts(limit=3):\n",
      "     |      ...     print(text[:500], end=\"\\n\\n\")\n",
      "     |      >>> for text, meta in ds.records(limit=3):\n",
      "     |      ...     print(\"\\n{} ({})\\n{}\".format(meta[\"case_name\"], meta[\"decision_date\"], text[:500]))\n",
      "     |  \n",
      "     |  Filter decisions by a variety of metadata fields and text length::\n",
      "     |  \n",
      "     |      >>> for text, meta in ds.records(opinion_author=109, limit=3):  # Notorious RBG!\n",
      "     |      ...     print(meta[\"case_name\"], meta[\"decision_direction\"], meta[\"n_maj_votes\"])\n",
      "     |      >>> for text, meta in ds.records(decision_direction=\"liberal\",\n",
      "     |      ...                              issue_area={1, 9, 10}, limit=3):\n",
      "     |      ...     print(meta[\"case_name\"], meta[\"maj_opinion_author\"], meta[\"n_maj_votes\"])\n",
      "     |      >>> for text, meta in ds.records(opinion_author=102, date_range=('1985-02-11', '1986-02-11')):\n",
      "     |      ...     print(\"\\n{} ({})\".format(meta[\"case_name\"], meta[\"decision_date\"]))\n",
      "     |      ...     print(ds.issue_codes[meta[\"issue\"]], \"=>\", meta[\"decision_direction\"])\n",
      "     |      >>> for text in ds.texts(min_len=250000):\n",
      "     |      ...     print(len(text))\n",
      "     |  \n",
      "     |  Stream decisions into a :class:`textacy.Corpus <textacy.corpus.Corpus>`::\n",
      "     |  \n",
      "     |      >>> textacy.Corpus(\"en\", data=ds.records(limit=25))\n",
      "     |      Corpus(25 docs; 136696 tokens)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      data_dir (str or :class:`pathlib.Path`): Path to directory on disk\n",
      "     |          under which the data is stored, i.e. ``/path/to/data_dir/supreme_court``.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      full_date_range: First and last dates for which decisions are available,\n",
      "     |          each as an ISO-formatted string (YYYY-MM-DD).\n",
      "     |      decision_directions: All distinct decision directions, e.g. \"liberal\".\n",
      "     |      opinion_author_codes: Mapping of majority opinion authors,\n",
      "     |          from id code to full name.\n",
      "     |      issue_area_codes: Mapping of high-level issue area of the case's core disagreement,\n",
      "     |          from id code to description.\n",
      "     |      issue_codes: Mapping of the specific issue of the case's core disagreement,\n",
      "     |          from id code to description.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SupremeCourt\n",
      "     |      textacy.datasets.base.Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, data_dir: Union[str, pathlib.Path] = PosixPath('/Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages/textacy/data/supreme_court'))\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  download(self, *, force: bool = False) -> None\n",
      "     |      Download the data as a Python version-specific compressed json file and\n",
      "     |      save it to disk under the ``data_dir`` directory.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          force: If True, download the dataset, even if it already exists\n",
      "     |              on disk under ``data_dir``.\n",
      "     |  \n",
      "     |  records(self, *, opinion_author: Union[int, Set[int], NoneType] = None, decision_direction: Union[str, Set[str], NoneType] = None, issue_area: Union[int, Set[int], NoneType] = None, date_range: Union[Tuple[Union[str, NoneType], Union[str, NoneType]], NoneType] = None, min_len: Union[int, NoneType] = None, limit: Union[int, NoneType] = None) -> Iterable[textacy.types.Record]\n",
      "     |      Iterate over decisions in this dataset, optionally filtering by a variety\n",
      "     |      of metadata and/or text length, and yield text + metadata pairs,\n",
      "     |      in chronological order by decision date.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          opinion_author: Filter decisions by the name(s) of the majority opinion's author,\n",
      "     |              coded as an integer whose mapping is given in\n",
      "     |              :attr:`SupremeCourt.opinion_author_codes`.\n",
      "     |          decision_direction: Filter decisions by the ideological direction\n",
      "     |              of the majority's decision; see\n",
      "     |              :attr:`SupremeCourt.decision_directions`.\n",
      "     |          issue_area: Filter decisions by the issue area of the case's subject matter,\n",
      "     |              coded as an integer whose mapping is given in\n",
      "     |              :attr:`SupremeCourt.issue_area_codes`.\n",
      "     |          date_range: Filter decisions by the date on which they were decided;\n",
      "     |              both start and end date must be specified, but a null value for either\n",
      "     |              will be replaced by the min/max date available for the dataset.\n",
      "     |          min_len: Filter decisions by the length (# characters) of their text content.\n",
      "     |          limit: Yield no more than ``limit`` decisions that match all specified filters.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Text of the next decision in dataset passing all filters,\n",
      "     |          and its corresponding metadata.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: If any filtering options are invalid.\n",
      "     |  \n",
      "     |  texts(self, *, opinion_author: Union[int, Set[int], NoneType] = None, decision_direction: Union[str, Set[str], NoneType] = None, issue_area: Union[int, Set[int], NoneType] = None, date_range: Union[Tuple[Union[str, NoneType], Union[str, NoneType]], NoneType] = None, min_len: Union[int, NoneType] = None, limit: Union[int, NoneType] = None) -> Iterable[str]\n",
      "     |      Iterate over decisions in this dataset, optionally filtering by a variety\n",
      "     |      of metadata and/or text length, and yield texts only,\n",
      "     |      in chronological order by decision date.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          opinion_author: Filter decisions by the name(s) of the majority opinion's author,\n",
      "     |              coded as an integer whose mapping is given in\n",
      "     |              :attr:`SupremeCourt.opinion_author_codes`.\n",
      "     |          decision_direction: Filter decisions by the ideological direction\n",
      "     |              of the majority's decision; see\n",
      "     |              :attr:`SupremeCourt.decision_directions`.\n",
      "     |          issue_area: Filter decisions by the issue area of the case's subject matter,\n",
      "     |              coded as an integer whose mapping is given in\n",
      "     |              :attr:`SupremeCourt.issue_area_codes`.\n",
      "     |          date_range: Filter decisions by the date on which they were decided;\n",
      "     |              both start and end date must be specified, but a null value for either\n",
      "     |              will be replaced by the min/max date available for the dataset.\n",
      "     |          min_len: Filter decisions by the length (# characters) of their text content.\n",
      "     |          limit: Yield no more than ``limit`` decisions that match all specified filters.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Text of the next decision in dataset passing all filters.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: If any filtering options are invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  filepath\n",
      "     |      Full path on disk for SupremeCourt data as compressed json file.\n",
      "     |      ``None`` if file is not found, e.g. has not yet been downloaded.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'decision_directions': typing.Set[str], 'full_date_...\n",
      "     |  \n",
      "     |  decision_directions = {'conservative', 'liberal', 'unspecifiable'}\n",
      "     |  \n",
      "     |  full_date_range = ('1946-11-18', '2016-06-27')\n",
      "     |  \n",
      "     |  issue_area_codes = {-1: None, 1: 'Criminal Procedure', 2: 'Civil Right...\n",
      "     |  \n",
      "     |  issue_codes = {'100010': 'federal-state ownership dispute (cf. Submerg...\n",
      "     |  \n",
      "     |  opinion_author_codes = {-1: None, 1: 'Jay, John', 2: 'Rutledge, John',...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from textacy.datasets.base.Dataset:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from textacy.datasets.base.Dataset:\n",
      "     |  \n",
      "     |  info\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from textacy.datasets.base.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    DOWNLOAD_ROOT = 'https://github.com/bdewilde/textacy-data/releases/dow...\n",
      "    Dict = typing.Dict\n",
      "    Iterable = typing.Iterable\n",
      "    LOGGER = <Logger textacy.datasets.supreme_court (WARNING)>\n",
      "    META = {'description': 'Collection of ~8.4k decisions issued by the U....\n",
      "    NAME = 'supreme_court'\n",
      "    Optional = typing.Optional\n",
      "    Set = typing.Set\n",
      "    Tuple = typing.Tuple\n",
      "    Union = typing.Union\n",
      "\n",
      "FILE\n",
      "    /Users/evanmuzzall/opt/anaconda3/lib/python3.8/site-packages/textacy/datasets/supreme_court.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(textacy.datasets.supreme_court)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TwurMgHVyT0"
   },
   "source": [
    "textacy is based on the concept of a corpus, whereas spaCy focuses on single documents. A textacy corpus is instantiated with a spaCy language model (we're using the one from the first half of this workshop) that is used to apply its analytical pipeline to each text in the corpus, and also given a set of records consisting of texts with metadata (if metadata is available).\n",
    "\n",
    "Let's go ahead and define a set of records (texts with metadata) that we'll then add to our corpus. To keep the processing time of the data set a bit more manageable, we'll just look at a set of court decisions from a short span of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "dTamJjiex_HE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding HEMI GROUP, LLC AND KAI GACHUPIN v. CITY OF NEW YORK, NEW YORK\n",
      "Adding CITIZENS UNITED v. FEDERAL ELECTION COMMISSION\n",
      "Adding HOLLY WOOD, PETITIONER v. RICHARD F. ALLEN, COMMISSIONER, ALABAMA DEPARTMENT OF CORRECTIONS, et al.\n",
      "Adding AGRON KUCANA v. ERIC H. HOLDER, JR., ATTORNEY GENERAL\n",
      "Adding MARCUS A. WELLONS v. HILTON HALL, WARDEN\n",
      "Adding ERIC PRESLEY v. GEORGIA\n",
      "Adding NRG POWER MARKETING, LLC, et al. v. MAINE PUBLIC UTILITIES COMMISSION et al.\n",
      "Adding E. K. MCDANIEL, WARDEN, et al. v. TROY BROWN\n",
      "Adding MARYLAND v. MICHAEL BLAINE SHATZER, SR.\n",
      "Adding THE HERTZ CORPORATION v. MELINDA FRIEND et al.\n",
      "Adding FLORIDA v. KEVIN DEWAYNE POWELL\n",
      "Adding RICK THALER, DIRECTOR, TEXAS DEPARTMENT OF CRIMINAL JUSTICE, CORRECTIONAL INSTITUTIONS DIVISION v. ANTHONY CARDELL HAYNES\n",
      "Adding MAC'S SHELL SERVICE, INC., et al. v. SHELL OIL PRODUCTS CO. LLC et al.\n",
      "Adding REED ELSEVIER, INC., et al., v. IRVIN MUCHNICK et al.\n",
      "Adding CURTIS DARNELL JOHNSON v. UNITED STATES\n",
      "Adding JAMAL KIYEMBA et al. v. BARACK H. OBAMA, PRESIDENT OF THE UNITED STATES et al.\n",
      "Adding MILAVETZ, GALLOP & MILAVETZ, P. A., et al. v. UNITED STATES\n",
      "Adding TAYLOR JAMES BLOATE v. UNITED STATES\n",
      "Adding SHADY GROVE ORTHOPEDIC ASSOCIATES, P. A. v. ALLSTATE INSURANCE COMPANY\n",
      "Adding JOSE PADILLA v. KENTUCKY\n",
      "Adding JERRY N. JONES, et al. v. HARRIS ASSOCIATES L. P.\n",
      "Adding MARY BERGHUIS, WARDEN v. DIAPOLIS SMITH\n",
      "Adding GRAHAM COUNTY SOIL AND WATER CONSERVATION DISTRICT, et al. v. UNITED STATES ex rel. KAREN T. WILSON\n",
      "Adding UNITED STUDENT AID FUNDS, INC. v. FRANCISCO J. ESPINOSA\n",
      "Adding ESTHER HUI, et al. v. YANIRA CASTANEDA, AS PERSONAL REPRESENTATIVE OF THE ESTATE OF FRANCISCO CASTANEDA, et al.\n",
      "Adding PAUL RENICO, WARDEN v. REGINALD LETT\n",
      "Adding KEN L. SALAZAR, SECRETARY OF THE INTERIOR, et al. v. FRANK BUONO\n",
      "Adding STOLT-NIELSEN S. A., et al. v. ANIMALFEEDS INTERNATIONAL CORP.\n",
      "Adding MERCK & CO., INC., et al. v. RICHARD REYNOLDS et al.\n",
      "Adding KAREN L. JERMAN v. CARLISLE, MCNELLIE, RINI, KRAMER & ULRICH LPA, et al.\n",
      "Adding SONNY PERDUE, GOVERNOR OF GEORGIA, et al. v. KENNY A., BY HIS NEXT FRIEND LINDA WINN, et al.\n",
      "Adding UNITED STATES v. ROBERT J. STEVENS\n",
      "Adding TIMOTHY MARK CAMERON ABBOTT v. JACQUELYN VAYE ABBOTT\n",
      "Adding TERRANCE JAMAR GRAHAM v. FLORIDA\n",
      "Adding UNITED STATES v. GRAYDON EARL COMSTOCK, JR., et al.\n",
      "Adding JOE HARRIS SULLIVAN v. FLORIDA\n",
      "Adding AMERICAN NEEDLE, INC. v. NATIONAL FOOTBALL LEAGUE et al.\n",
      "Adding ARTHUR L. LEWIS, JR., et al. v. CITY OF CHICAGO, ILLINOIS\n",
      "Adding UNITED STATES v. MARTIN O'BRIEN AND ARTHUR BURGESS\n",
      "Adding BRIDGET HARDT v. RELIANCE STANDARD LIFE INSURANCE COMPANY\n",
      "Adding UNITED STATES v. GLENN MARCUS\n",
      "Adding JOHN ROBERTSON v. UNITED STATES ex rel. WYKENNA WATSON\n",
      "Adding LAWRENCE JOSEPH JEFFERSON v. STEPHEN UPTON, WARDEN\n",
      "Adding MOHAMED ALI SAMANTAR v. BASHE ABDI YOUSUF et al.\n",
      "Adding MARY BERGHUIS, WARDEN v. VAN CHESTER THOMPKINS\n",
      "Adding RICHARD A. LEVIN, TAX COMMISSIONER OF OHIO v. COMMERCE ENERGY, INC., et al.\n",
      "Adding THOMAS CARR v. UNITED STATES\n",
      "Adding MICHAEL GARY BARBER, et al. v. J. E. THOMAS, WARDEN\n",
      "Adding JAN HAMILTON, CHAPTER 13 TRUSTEE v. STEPHANIE KAY LANNING\n",
      "Adding WANDA KRUPSKI v. COSTA CROCIERE S. P. A.\n",
      "Adding JOSE ANGEL CARACHURI-ROSENDO v. ERIC H. HOLDER, JR., ATTORNEY GENERAL\n",
      "Adding MICHAEL J. ASTRUE, COMMISSIONER OF SOCIAL SECURITY v. CATHERINE G. RATLIFF\n",
      "Adding BRIAN RUSSELL DOLAN v. UNITED STATES\n",
      "Adding ALBERT HOLLAND v. FLORIDA\n",
      "Adding NEW PROCESS STEEL, L. P. v. NATIONAL LABOR RELATIONS BOARD\n",
      "Adding STOP THE BEACH RENOURISHMENT, INC. v. FLORIDA DEPARTMENT OF ENVIRONMENTAL PROTECTION et al.\n",
      "Adding CITY OF ONTARIO, CALIFORNIA, et al. v. JEFF QUON et al.\n",
      "Adding WILLIAM G. SCHWAB v. NADEJDA REILLY\n",
      "Adding PERCY DILLON v. UNITED STATES\n",
      "Adding ERIC H. HOLDER, JR., ATTORNEY GENERAL, et al. v. HUMANITARIAN LAW PROJECT et al.\n",
      "Adding RENT-A-CENTER, WEST, INC. v. ANTONIO JACKSON\n",
      "Adding KAWASAKI KISEN KAISHA LTD. et al. v. REGAL-BELOIT CORP. et al.\n",
      "Adding MONSANTO COMPANY, et al. v. GEERTSON SEED FARMS et al.\n",
      "Adding JOHN DOE #1, et al. v. SAM REED, WASHINGTON SECRETARY OF STATE, et al.\n",
      "Adding ROBERT MORRISON, et al. v. NATIONAL AUSTRALIA BANK LTD. et al.\n",
      "Adding GRANITE ROCK COMPANY v. INTERNATIONAL BROTHERHOOD OF TEAMSTERS et al.\n",
      "Adding BILLY JOE MAGWOOD v. TONY PATTERSON, WARDEN, et al.\n",
      "Adding JEFFREY K. SKILLING v. UNITED STATES\n",
      "Adding CONRAD M. BLACK, JOHN A. BOULTBEE, AND MARK S. KIPNIS v. UNITED STATES\n",
      "Adding FREE ENTERPRISE FUND AND BECKSTEAD AND WATTS, LLP v. PUBLIC COMPANY ACCOUNTING OVERSIGHT BOARD et al.\n",
      "Adding BERNARD L. BILSKI AND RAND A. WARSAW v. DAVID J. KAPPOS, UNDER SECRETARY OF COMMERCE FOR INTELLECTUAL PROPERTY AND DIRECTOR, PATENT AND TRADEMARK OFFICE\n",
      "Adding CHRISTIAN LEGAL SOCIETY CHAPTER OF THE UNIVERSITY OF CALIFORNIA, HASTINGS COLLEGE OF THE LAW, AKA HASTINGS CHRISTIAN FELLOWSHIP v. LEO P. MARTINEZ et al.\n",
      "Adding OTIS MCDONALD, et al. v. CITY OF CHICAGO, ILLINOIS, et al.\n",
      "Adding DEMARCUS ALI SEARS v. STEPHEN UPTON, WARDEN\n",
      "Adding BILL K. WILSON, SUPERINTENDENT, INDIANA STATE PRISON, PETITIONER v. JOSEPH E. CORCORAN\n",
      "Adding KEVIN ABBOTT, PETITIONER v. UNITED STATES\n",
      "Adding LOS ANGELES COUNTY, CALIFORNIA, PETITIONER v. CRAIG ARTHUR HUMPHRIES et al.\n",
      "Adding COSTCO WHOLESALE CORPORATION, PETITIONER v. OMEGA, S.A.\n",
      "Adding KEITH SMITH, WARDEN v. FRANK G. SPISAK, JR.\n"
     ]
    }
   ],
   "source": [
    "corpus = textacy.Corpus(nlp)\n",
    "\n",
    "# There are 79 docs in this range -- they'll take a minute or two to process\n",
    "recent_decisions = data.records(date_range=('2010-01-01', '2010-12-31'))\n",
    "\n",
    "for record in recent_decisions:\n",
    "    print(\"Adding\",record[1]['case_name'])\n",
    "    corpus.add_record(record)\n",
    "\n",
    "# If the three lines above are taking too long to process all 79 docs,\n",
    "# comment them out and uncomment the two lines below to download and import\n",
    "# a preprocessed version of the corpus\n",
    "\n",
    "#!wget https://github.com/sul-cidr/Workshops/raw/master/Text_Analysis_with_Python/data/scotus_2010.bin.gz\n",
    "#corpus = textacy.Corpus.load(nlp, \"scotus_2010.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "tBnvE5ZJVyT7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Doc(13007 tokens: \"Respondent New York City taxes the possession o...\")',\n",
       " 'Doc(72325 tokens: \"As amended by §203 of the Bipartisan Campaign R...\")',\n",
       " 'Doc(8333 tokens: \"Under 28 U. S. C. §2254(d)(2), a federal court ...\")',\n",
       " 'Doc(9947 tokens: \"The Illegal Immigration Reform and Immigrant Re...\")',\n",
       " 'Doc(5508 tokens: \"Per Curiam.  From beginning to end, judicial pr...\")']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "[doc._.preview for doc in corpus[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuBjFKQ4VyT8"
   },
   "source": [
    "We can see that the type of each item in the corpus is a `Doc` - this is a processed spaCy output document, with all of the extracted features. textacy provides some capacity to work with those features via its API, and also exposes new document-level features, such as ngrams and algorithms to determine a document's readability level, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DFWqESvVyT8"
   },
   "source": [
    "We can filter this corpus based on metadata attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zW_s4Eodx_HF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'issue': '80170',\n",
       " 'issue_area': 8,\n",
       " 'n_min_votes': 3,\n",
       " 'case_name': 'HEMI GROUP, LLC AND KAI GACHUPIN v. CITY OF NEW YORK, NEW YORK',\n",
       " 'maj_opinion_author': 111,\n",
       " 'decision_date': '2010-01-25',\n",
       " 'decision_direction': 'conservative',\n",
       " 'n_maj_votes': 5,\n",
       " 'us_cite_id': '559 U.S. 1',\n",
       " 'argument_date': '2009-11-03'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]._.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "j-CqNgQaVyUC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we'll find all the cases where the number of justices voting in the majority was greater than 6. \n",
    "supermajorities = [doc for doc in corpus.get(lambda doc: doc._.meta[\"n_maj_votes\"] > 6)]\n",
    "len(supermajorities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "4o5hy0pCVyUG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(8333 tokens: \"Under 28 U. S. C. §2254(d)(2), a federal court ...\")'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supermajorities[0]._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8YO2bgIVyUM"
   },
   "source": [
    "## Finding important words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "_wOMpBE0VyUM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents:  79\n",
      "number of sentences:  42519\n",
      "number of tokens:  1189205\n"
     ]
    }
   ],
   "source": [
    "print(\"number of documents: \", corpus.n_docs)\n",
    "print(\"number of sentences: \", corpus.n_sents)\n",
    "print(\"number of tokens: \", corpus.n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "WiQQrx5SVyUO"
   },
   "outputs": [],
   "source": [
    "# Set as_strings to True so that the results will display strings rather than unique ids.\n",
    "counts = corpus.word_counts(by = \"orth\", filter_nums=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ELd2rxUZVyUU"
   },
   "outputs": [],
   "source": [
    "def show_doc_counts(input_corpus, weighting, limit=20):\n",
    "    doc_counts = input_corpus.word_doc_counts(weighting=weighting, filter_stops=True, by = \"orth\")\n",
    "    print(\"\\n\".join([f'{a:15} {str(b)}' for a,b in sorted(doc_counts.items(), key=lambda x:x[1], reverse=True)[:limit]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5M6Nyy0079J"
   },
   "source": [
    "`word_doc_counts` provides a few ways of quantifying the prevalence of individual words across the corpus: whether a word appears many times in most documents, just a few times in a few documents, many times in a few documents, or just a few times in most documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "wIIkImS2VyUW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# DOCS APPEARING IN / TOTAL # DOCS\n",
      "7909698700018473301 0.9873417721518988\n",
      "8110129090154140942 0.9873417721518988\n",
      "12190959957860242726 0.9873417721518988\n",
      "7640242612580250176 0.9873417721518988\n",
      "8252066398621046228 0.9746835443037974\n",
      "14855555303352453619 0.9746835443037974\n",
      "12383688149036497881 0.9746835443037974\n",
      "10392126440436578108 0.9746835443037974\n",
      "5364348271814505298 0.9746835443037974\n",
      "9597162758872268865 0.9746835443037974\n",
      "6788458244867680173 0.9620253164556962\n",
      "3193630459561461984 0.9620253164556962\n",
      "14536103007527724270 0.9620253164556962\n",
      "10232638930417780466 0.9620253164556962\n",
      "5533571732986600803 0.9620253164556962\n",
      "5180745488931310612 0.9493670886075949\n",
      "13425056503550658438 0.9493670886075949\n",
      "8884242500009200765 0.9493670886075949\n",
      "2360781233418247603 0.9493670886075949\n",
      "4747730822158419200 0.9493670886075949\n",
      "\n",
      "LOG(TOTAL # DOCS / # DOCS APPEARING IN)\n",
      "10095831487921043068 4.382026634673881\n",
      "10234213947693239637 4.382026634673881\n",
      "11626748531136242742 4.382026634673881\n",
      "16419197339972121050 4.382026634673881\n",
      "8184745995423391585 4.382026634673881\n",
      "14661530404226401824 4.382026634673881\n",
      "10951819471252361798 4.382026634673881\n",
      "9614391188995406261 4.382026634673881\n",
      "15038740264642698121 4.382026634673881\n",
      "12275951821107647850 4.382026634673881\n",
      "11148977804453370581 4.382026634673881\n",
      "3067333291213353427 4.382026634673881\n",
      "3204186248727857869 4.382026634673881\n",
      "6189969109766188883 4.382026634673881\n",
      "8374339975676057067 4.382026634673881\n",
      "13643711785827136576 4.382026634673881\n",
      "16745419210534705223 4.382026634673881\n",
      "8107874622840311865 4.382026634673881\n",
      "2952638369299143353 4.382026634673881\n",
      "326240660805297930 4.382026634673881\n"
     ]
    }
   ],
   "source": [
    "print(\"# DOCS APPEARING IN / TOTAL # DOCS\")\n",
    "show_doc_counts(corpus, \"freq\")\n",
    "print(\"\\nLOG(TOTAL # DOCS / # DOCS APPEARING IN)\")\n",
    "show_doc_counts(corpus, \"idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx69GIqUdqtx"
   },
   "source": [
    "textacy provides implementations of algorithms for identifying words and phrases that are representative of a document (aka **keyterm extraction**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "aQCRD7WTeI1j"
   },
   "outputs": [],
   "source": [
    "from textacy.extract import keyterms as ke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "FEfkQ1IWx_HH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Respondent New York City taxes the possession of cigarettes. Petitioner Hemi Group, based in New Mexico, sells cigarettes online to residents of the City. Neither state nor city law requires out-of-state sellers such as Hemi to charge, collect, or remit the City\\'s tax; instead, the City must recover its tax on out-of-state sales directly from the purchasers. But the Jenkins Act, 15 U. S. C. §§375-378, requires out-of-state sellers to submit customer information to the States into which they ship cigarettes, and New York State has agreed to forward that information to the City. That information helps the City track down cigarette purchasers who do not pay their taxes. Against that backdrop, the City filed this lawsuit under the Racketeer Influenced and Corrupt Organizations Act (RICO), alleging that Hemi\\'s failure to file the Jenkins Act reports with the State constituted mail and wire fraud, which are defined as \"racketeering activit[ies],\" 18 U. S. C. §1961(1), subject to enforcement under civil RICO, §1964(c). The District Court dismissed the claims, but the Second Circuit vacated the judgment and remanded. Among other things, the Court of Appeals held that the City\\'s asserted injury — lost tax revenue — came about \"by reason of\" the predicate mail and wire frauds. It accordingly determined that the City had stated a valid RICO claim. Held: The judgment is reversed, and the case is remanded.\\n541 F. 3d 425, reversed and remanded.\\n Chief Justice Roberts delivered the opinion of the Court in part, concluding that because the City cannot show that it lost tax revenue \"by reason of\" the alleged RICO violation, it cannot state a RICO claim. Pp. 5-15.\\n (a) To establish that an injury came about \"by reason of\" a RICO violation, a plaintiff must show that a predicate offense \"not only was a \\'but for\\' cause of his injury, but was the proximate cause as well.\" Holmes v. Securities Investor Protection Corporation, 503 U. S. 258, 268. Proximate cause for RICO purposes should be evaluated in light of its common-law foundations; it thus requires \"some direct relation between the injury asserted and the injurious conduct alleged.\" Ibid. A link that is \"too remote,\" \"purely contingent,\" or \"indirec[t]\" is insufficient. Id., at 271, 274. The City\\'s causal theory cannot satisfy RICO\\'s direct relationship requirement. Indeed, the causal link here is far more attenuated than the one the Court rejected as \"purely contingent\" and \"too remote\" in Holmes. Id., at 271. According to the City, Hemi committed fraud by selling cigarettes to city residents and failing to submit the required customer information to the State. Without the reports from Hemi, the State could not pass on the information to the City, even if it had been so inclined. Some of the customers legally obligated to pay the cigarette tax to the City failed to do so. Because the City did not receive the customer information, it could not determine which customers had failed to pay the tax. The City thus could not pursue those customers for payment. The City thereby was injured in the amount of the portion of back taxes that were never collected. As the Court reiterated in Holmes, \"[t]he general tendency of the law, in regard to damages at least, is not to go beyond the first step,\" id., at 271-272, and that \"general tendency\" applies with full force to proximate cause inquiries under RICO, e.g., ibid. Because the City\\'s causation theory requires the Court to move well beyond the first step, that theory cannot satisfy RICO\\'s direct relationship requirement. The City\\'s claim suffers from the same defect as the RICO claim rejected in Anza v. Ideal Steel Supply Corp., 547 U. S. 451, 458-461, where the conduct directly causing the harm was distinct from the conduct giving rise to the fraud, see id., at 458. Indeed, the disconnect between the asserted injury and the alleged fraud in this case is even sharper. In Anza, the same party had both engaged in the harmful conduct and committed the fraudulent act. Here, the City\\'s theory of liability rests not just on separate actions, but separate actions carried out by separate parties. The City\\'s theory thus requires that the Court extend RICO liability to situations where the defendant\\'s fraud on the third party (the State) has made it easier for a fourth party (the taxpayer) to cause harm to the plaintiff (the City). Indeed, the fourth-party taxpayers here only caused harm to the City in the first place if they decided not to pay taxes they were legally obligated to pay. Put simply, Hemi\\'s obligation was to file Jenkins Act reports with the State, not the City, and the City\\'s harm was directly caused by the customers, not Hemi. The Court has never before stretched the causal chain of a RICO violation so far, and declines to do so today. See, e.g., id., at 460-461. Pp. 5-9.\\n (b) The City attempts to avoid this conclusion by characterizing the violation not merely as Hemi\\'s failure to file Jenkins Act reports with the State, but as a more general systematic scheme to defraud the City of tax revenue. But if the City could escape the proximate-cause requirement merely by alleging that the fraudulent scheme embraced all those indirectly harmed by the alleged conduct, the Court\\'s RICO proximate cause precedent would become a mere pleading rule. That precedent makes clear that \"the compensable injury flowing from a [RICO] violation ... \\'necessarily is the harm caused by [the] predicate acts.\\' \" Anza, supra, at 457. Because the only fraudulent conduct alleged here is a violation of the Jenkins Act, the City must, but cannot, show that Hemi\\'s failure to file the Jenkins Act reports led directly to its injuries. The City also errs in relying on Bridge v. Phoenix Bond & Indemnity Co., 553 U. S. ___. There, the plaintiffs\\' causation theory was \"straightforward\": The causal link in Bridge involved a direct and easily identifiable connection between the fraud at issue and the plaintiffs\\' injury, id., at ___; the plaintiffs there \"were the only parties injured by petitioners\\' misrepresentations,\" id., at ___; and there were \"no independent factors that account[ed] for [the] injury,\" id., at ___. The City\\'s theory in this case is anything but straightforward: Multiple steps separate the alleged fraud from the asserted injury. And in contrast to Bridge, where there were \"no independent factors that account[ed] for [the plaintiffs\\'] injury,\" id., at ___, here there certainly were: The City\\'s theory of liability rests on the independent actions of third and even fourth parties. Pp. 10-14.\\n Roberts, C. J., delivered the opinion of the Court in part, in which Scalia, Thomas, and Alito, JJ., joined, and in which Ginsburg, J., joined in part. Ginsburg, J., filed an opinion concurring in part and concurring in the judgment. Breyer, J., filed a dissenting opinion, in which Stevens and Kennedy, JJ., joined. Sotomayor, J., took no part in the consideration or decision of the case. HEMI GROUP, LLC and KAI GACHUPIN,PETITIONERS v. CITY OF NEW YORK,NEW YORK\\non writ of certiorari to the united states court of appeals for the second circuit\\n[January 25, 2010]\\n Chief Justice Roberts delivered the opinion of the Court in part.\\n The City of New York taxes the possession of cigarettes. Hemi Group, based in New Mexico, sells cigarettes online to residents of the City. Neither state nor city law requires Hemi to charge, collect, or remit the tax, and the purchasers seldom pay it on their own. Federal law, however, requires out-of-state vendors such as Hemi to submit customer information to the States into which they ship the cigarettes.\\n Against that backdrop, the City filed this lawsuit under the Racketeer Influenced and Corrupt Organizations Act (RICO), alleging that Hemi failed to file the required customer information with the State. That failure, the City argues, constitutes mail and wire fraud, which caused it to lose tens of millions of dollars in unrecovered cigarette taxes. Because the City cannot show that it lost the tax revenue \"by reason of\" the alleged RICO violation, 18 U. S. C. §1964(c), we hold that the City cannot state a claim under RICO. We therefore reverse the Court of Appeals\\' decision to the contrary.\\nI\\nA\\n This case arises from a motion to dismiss, and so we accept as true the factual allegations in the City\\'s second amended complaint. See Leatherman v. Tarrant County Narcotics Intelligence and Coordination Unit, 507 U. S. 163, 164 (1993).\\n New York State authorizes the City of New York to impose its own taxes on cigarettes. N. Y. Unconsol. Law Ann. §9436(1) (West Supp. 2009). Under that authority, the City has levied a $1.50 per pack tax on each standard pack of cigarettes possessed within the City for sale or use. N. Y. C. Admin. Code §11-1302(a) (2008); see also Record A1016. When purchasers buy cigarettes from in-state vendors, the seller is responsible for charging, collecting, and remitting the tax. N. Y. Tax Law Ann. §471(2) (West Supp. 2009). Out-of-state vendors, however, are not. Ibid.; see City of New York v. Smokes-Spirits.com, Inc., 541 F. 3d 425, 432-433 (CA2 2008). Instead, the City is responsible for recovering, directly from the customers, use taxes on cigarettes sold outside New York. That can be difficult, as those customers are often reluctant to pay and tough to track down. One way the City can gather information that would assist it in collecting the back taxes is through the Jenkins Act, 63 Stat. 884, as amended by 69 Stat. 627. That Act requires out-of-state cigarette sellers to register and to file a report with state tobacco tax administrators listing the name, address, and quantity of cigarettes purchased by state residents. 15 U. S. C. §§375-378.\\n New York State and the City have executed an agreement under which both parties undertake to \"cooperate fully with each other and keep each other fully and promptly informed with reference to any person or transaction subject to both State and City cigarette taxes including [i]nformation obtained which may result in additional cigarette tax revenue to the State or City provided that the disclosure of that information is permissible under existing laws and agreements.\" Record A1003. The City asserts that under that agreement, the State forwards Jenkins Act information to the City. Id., at A998; Second Amended Compl. ¶54. That information helps the City track down purchasers who do not pay their taxes. Id., ¶¶58-59.\\n Hemi Group is a New Mexico company that sells cigarettes online. Hemi, however, does not file Jenkins Act information with the State. The City alleges that this failure has cost it \"tens if not hundreds of millions of dollars a year in cigarette excise tax revenue.\" Record A996. Based on Hemi\\'s failure to file the information with the State, the City filed this federal RICO claim.\\nB\\n RICO provides a private cause of action for \"[a]ny person injured in his business or property by reason of a violation of section 1962 of this chapter.\" 18 U. S. C. §1964(c). Section 1962, in turn, contains RICO\\'s criminal provisions. Specifically, §1962(c), which the City invokes here, makes it \"unlawful for any person employed by or associated with any enterprise engaged in, or the activities of which affect, interstate ... commerce, to conduct or participate, directly or indirectly, in the conduct of such enterprise\\'s affairs through a pattern of racketeering activity.\" \"[R]acketeering activity\" is defined to include a number of so-called predicate acts, including the two at issue in this case — mail and wire fraud. See §1961(1).\\n The City alleges that Hemi\\'s \"interstate sale of cigarettes and the failure to file Jenkins Act reports identifying those sales\" constitute the RICO predicate offenses of mail and wire fraud in violation of §1962(c), for which §1964(c) provides a private cause of action. Record A980. Invoking that private cause of action, the City asserts that it has suffered injury in the form of lost tax revenue — its \"business or property\" in RICO terms--\"by reason of\" Hemi\\'s fraud.\\n Hemi does not contest the City\\'s characterization of the Jenkins Act violations as predicate offenses actionable under §1964(c). (We therefore assume, without deciding, that failure to file Jenkins Act material can serve as a RICO predicate offense.) Instead, Hemi argues that the City\\'s asserted injury — lost tax revenue — is not \"business or property\" under RICO, and that the City cannot show that it suffered any injury \"by reason of\" the failure to file Jenkins Act reports.\\n The District Court dismissed the City\\'s RICO claims, determining that Hemi owner and officer Kai Gachupin did not have an individual duty to file Jenkins Act reports, and thus could not have committed the alleged predicate acts. City of New York v. Nexicon, Inc., No. 03 CV 383 (DAB), 2006 WL 647716, *7-*8 (SDNY, Mar. 15, 2006). The District Court therefore held that the City could not establish that Hemi and Gachupin formed an \"enterprise\" as required to establish RICO liability. Id., at *7-*10. Because it dismissed on that ground, the District Court did not address whether the City\\'s loss of tax revenue constitutes an injury to its \"business or property\" under §1964, or whether that injury was caused \"by reason of\" Hemi\\'s failure to file the Jenkins Act reports.\\n The Second Circuit vacated the District Court\\'s judgment and remanded for further proceedings. The Court of Appeals held that the City had established that Gachupin and Hemi operated as an \"enterprise\" and that the enterprise committed the predicate RICO acts of mail and wire fraud, based on the failure to file the Jenkins Act material with the State. 541 F. 3d, at 447-448. The court also determined that the City\\'s asserted injury, lost tax revenue, was \"business or property\" under RICO. Id., at 444-445. And that injury, the court concluded, came about \"by reason of\" the predicate mail and wire frauds. Id., at 440-444. The City thus had stated a viable RICO claim. Judge Winter dissented on the ground that the alleged RICO violation was not the proximate cause of the City\\'s injury. Id., at 458-461.\\n Hemi filed a petition for certiorari, asking this Court to determine whether the City had been \"directly injured in its \\'business or property\\' \" by reason of the alleged mail and wire frauds. Pet. for Cert. i. We granted that petition. 556 U. S. __ (2009).\\nII\\n Though framed as a single question, Hemi\\'s petition for certiorari raises two distinct issues: First, whether a loss in tax revenue is \"business or property\" under 18 U. S. C. §1964(c); and second, whether the City\\'s asserted injury came about \"by reason of\" the allegedly fraudulent conduct, as required by §1964(c). We determine that the City cannot satisfy the causation requirement — that any injury the City suffered must be \"by reason of\" the alleged frauds — and therefore do not decide whether the City\\'s allegations of lost tax revenue constitute an injury to its \"business or property.\"\\nA\\n In Holmes v. Securities Investor Protection Corporation, 503 U. S. 258 (1992), we set forth the standard of causation that applies to civil RICO claims. In that case, we addressed a RICO claim brought by Securities Investor Protection Corporation (SIPC) against defendants whom SIPC alleged had manipulated stock prices. Id., at 262-263. SIPC had a duty to reimburse customers of certain registered broker-dealers in the event the broker-dealers were unable to meet their financial obligations. Id., at 261. When the conspiracy by the stock manipulators was detected, stock prices collapsed, and two broker-dealers were unable to meet their obligations to their customers. SIPC, as insurer against that loss, ultimately was on the hook for nearly $13 million to cover the customers\\' claims. The Court held that SIPC could not recover against the conspirators because it could not establish that it was injured \"by reason of\" the alleged fraud, as that phrase is used in RICO.\\n We explained that, to state a claim under civil RICO, the plaintiff is required to show that a RICO predicate offense \"not only was a \\'but for\\' cause of his injury, but was the proximate cause as well.\" Id., at 268. Proximate cause for RICO purposes, we made clear, should be evaluated in light of its common-law foundations; proximate cause thus requires \"some direct relation between the injury asserted and the injurious conduct alleged.\" Ibid. A link that is \"too remote,\" \"purely contingent,\" or \"indirec[t]\" is insufficient. Id., at 271, 274.\\n Applying that standard, we rejected SIPC\\'s RICO claim. The alleged conspiracy, we held, directly harmed only the broker-dealers; SIPC\\'s injury, on the other hand, was \"purely contingent\" on that harm. Id., at 271. The connection between the alleged conspiracy and SIPC\\'s injury was therefore \"too remote\" to satisfy RICO\\'s direct relationship requirement. Ibid.\\n The City\\'s causal theory is far more attenuated than the one we rejected in Holmes. According to the City, Hemi committed fraud by selling cigarettes to city residents and failing to submit the required customer information to the State. Without the reports from Hemi, the State could not pass on the information to the City, even if it had been so inclined. Some of the customers legally obligated to pay the cigarette tax to the City failed to do so. Because the City did not receive the customer information, the City could not determine which customers had failed to pay the tax. The City thus could not pursue those customers for payment. The City thereby was injured in the amount of the portion of back taxes that were never collected. See Record A996.\\n But as we reiterated in Holmes, \"[t]he general tendency of the law, in regard to damages at least, is not to go beyond the first step.\" 503 U. S., at 271-272 (quoting Associated Gen. Contractors of Cal., Inc. v. Carpenters, 459 U. S. 519, 534 (1983), in turn quoting Southern Pacific Co. v. Darnell-Taenzer Lumber Co., 245 U. S. 531, 533 (1918), internal quotation marks omitted). Our cases confirm that the \"general tendency\" applies with full force to proximate cause inquiries under RICO. Holmes, supra, at 271-272; see also Bridge v. Phoenix Bond & Indemnity Co., 553 U. S. __, __ (2008) (slip op., at 18-19); Anza v. Ideal Steel Supply Corp., 547 U. S. 451, 460-461 (2006). Because the City\\'s theory of causation requires us to move well beyond the first step, that theory cannot meet RICO\\'s direct relationship requirement.\\n Our decision in Anza, supra, confirms that the City\\'s theory of causation is far too indirect. There we considered a RICO claim brought by Ideal Steel Supply against its competitor, National Steel Supply. Ideal alleged that National had defrauded New York State by failing to charge and remit sales taxes, and that National was thus able to undercut Ideal\\'s prices. The lower prices offered by National, Ideal contended, allowed National to attract customers at Ideal\\'s expense. Id., at 458.\\n Finding the link between the fraud alleged and injury suffered to be \"attenuated,\" we rejected Ideal\\'s claim. Id., at 459. \"The direct victim of this conduct,\" we held, was \"the State of New York, not Ideal.\" Id., at 458. \"It was the State that was being defrauded and the State that lost tax revenue as a result.\" Ibid. We recognized that Ideal had asserted \"its own harms when [National] failed to charge customers for the applicable sales tax.\" Ibid. But the cause of Ideal\\'s harm was \"a set of actions (offering lower prices) entirely distinct from the alleged RICO violation (defrauding the State).\" Ibid. The alleged violation therefore had not \"led directly to the plaintiff\\'s injuries,\" and Ideal accordingly had failed to meet RICO\\'s \"requirement of a direct causal connection\" between the predicate offense and the alleged harm. Id., at 460-461.\\n The City\\'s claim suffers from the same defect as the claim in Anza. Here, the conduct directly responsible for the City\\'s harm was the customers\\' failure to pay their taxes. And the conduct constituting the alleged fraud was Hemi\\'s failure to file Jenkins Act reports. Thus, as in Anza, the conduct directly causing the harm was distinct from the conduct giving rise to the fraud. See id., at 458.\\n Indeed, the disconnect between the asserted injury and the alleged fraud in this case is even sharper than in Anza. There, we viewed the point as important because the same party — National Steel — had both engaged in the harmful conduct and committed the fraudulent act. We nevertheless found the distinction between the relevant acts sufficient to defeat Ideal\\'s RICO claim. Here, the City\\'s theory of liability rests not just on separate actions, but separate actions carried out by separate parties.\\n The City\\'s theory thus requires that we extend RICO liability to situations where the defendant\\'s fraud on the third party (the State) has made it easier for a fourth party (the taxpayer) to cause harm to the plaintiff (the City). Indeed, the fourth-party taxpayers here only caused harm to the City in the first place if they decided not to pay taxes they were legally obligated to pay. Put simply, Hemi\\'s obligation was to file the Jenkins Act reports with the State, not the City, and the City\\'s harm was directly caused by the customers, not Hemi. We have never before stretched the causal chain of a RICO violation so far, and we decline to do so today. See id., at 460-461; cf. Associated Gen. Contractors, supra, at 541, n. 46 (finding no proximate cause in the antitrust context where the plaintiff\\'s \"harm stems most directly from the conduct of persons who are not victims of the conspiracy\").\\n One consideration we have highlighted as relevant to the RICO \"direct relationship\" requirement is whether better situated plaintiffs would have an incentive to sue. See Holmes, supra, at 269-270. The State certainly is better situated than the City to seek recovery from Hemi. And the State has an incentive to sue — the State imposes its own $2.75 per pack tax on cigarettes possessed within the State, nearly double what the City charges. N. Y. Tax Law Ann. §471(1) (West Supp. 2009). We do not opine on whether the State could bring a RICO action for any lost tax revenue. Suffice it to say that the State would have concrete incentives to try. See Anza, supra, at 460 (\"Ideal accuses the Anzas of defrauding the State of New York out of a substantial amount of money. If the allegations are true, the State can be expected to pursue appropriate remedies\"). The dissent would have RICO\\'s proximate cause requirement turn on foreseeability, rather than on the existence of a sufficiently \"direct relationship\" between the fraud and the harm. It would find that the City has satisfied that requirement because \"the harm is foreseeable; it is a consequence that Hemi intended, indeed desired; and it falls well within the set of risks that Congress sought to prevent.\" Post, at 6 (opinion of Breyer, J.). If this line of reasoning sounds familiar, it should. It is precisely the argument lodged against the majority opinion in Anza. There, the dissent criticized the majority\\'s view for \"permit[ting] a defendant to evade liability for harms that are not only foreseeable, but the intended consequences of the defendant\\'s unlawful behavior.\" 547 U. S., at 470 (Thomas, J., concurring in part and dissenting in part). But the dissent there did not carry the day, and no one has asked us to revisit Anza.\\n The concepts of direct relationship and foreseeability are of course two of the \"many shapes [proximate cause] took at common law,\" Holmes, supra, at 268. Our precedents make clear that in the RICO context, the focus is on the directness of the relationship between the conduct and the harm. Indeed, Anza and Holmes never even mention the concept of foreseeability. B\\n The City offers a number of responses. It first challenges our characterization of the violation at issue. In the City\\'s view, the violation is not merely Hemi\\'s failure to file Jenkins Act information with the State, but a more general \"systematic scheme to defraud the City of tax revenue.\" Brief for Respondent 42. Having broadly defined the violation, the City contends that it has been directly harmed by reason of that systematic scheme. Ibid.\\n But the City cannot escape the proximate cause requirement merely by alleging that the fraudulent scheme embraced all those indirectly harmed by the alleged conduct. Otherwise our RICO proximate cause precedent would become a mere pleading rule. In Anza, for example, Ideal alleged that National\\'s scheme \"was to give National a competitive advantage over Ideal.\" 547 U. S., at 454-455. But that allegation did not prevent the Court from concluding that National\\'s fraud directly harmed only the State, not Ideal. As the Court explained, Ideal could not \"circumvent the proximate-cause requirement simply by claiming that the defendant\\'s aim was to increase market share at a competitor\\'s expense.\" Id., at 460.1\\n Our precedent makes clear, moreover, that \"the compensable injury flowing from a [RICO] violation ... \\'necessarily is the harm caused by [the] predicate acts.\\' \" Id., at 457 (quoting Sedima, S. P. R. L. v. Imrex Co., 473 U. S. 479, 497 (1985)). In its RICO statement, the City alleged that Hemi\\'s failure to file Jenkins Act reports constituted the predicate act of mail and wire fraud. Record A980. The City went on to allege that this predicate act \"directly caused\" its harm, id., at A996, but that assertion is a legal conclusion about proximate cause — indeed, the very legal conclusion before us. The only fraudulent conduct alleged here is a violation of the Jenkins Act. See 541 F. 3d, at 459 (Winter, J., dissenting). Thus, the City must show that Hemi\\'s failure to file the Jenkins Act reports with the State led directly to its injuries. This it cannot do.\\n The City also relies on Bridge, 553 U. S. ___. Bridge reaffirmed the requirement that there must be \"a sufficiently direct relationship between the defendant\\'s wrongful conduct and the plaintiff\\'s injury.\" Id., at ___ (slip op., at 18). The case involved competing bidders at a county tax-lien auction. Because the liens were profitable even at the lowest possible bid, multiple bidders offered that low bid. (The bidding took the form of the percentage tax penalty the bidder would require the property owner to pay, so the lowest possible bid was 0%.) To decide which bidder would be awarded the lien, the county devised a plan to allocate the liens \"on a rotational basis.\" Id., at ___ (slip op., at 3) (internal quotation marks omitted). But as we noted in that case, this created a \"perverse incentive\": \"Bidders who, in addition to bidding themselves, sen[t] agents to bid on their behalf [would] obtain a disproportionate share of liens.\" Ibid. The county therefore prohibited bidders from using such agents. Ibid.\\n A losing bidder alleged that a competitor had defrauded the county by employing shadow bidders to secure a greater proportion of liens than it was due. We held that the bidder-plaintiff had met RICO\\'s causation requirement. Distinguishing that claim from the one at issue in Anza, we noted that the plaintiff\\'s theory of causation in Bridge was \"straightforward\": Because of the zero-sum nature of the auction, and because the county awarded bids on a rotational basis, each time a fraud-induced bid was awarded, a particular legitimate bidder was necessarily passed over. 553 U. S., at ___ (slip op., at 18). The losing bidders, moreover, \"were the only parties injured by petitioners\\' misrepresentations.\" Ibid. The county was not; it received the same revenue regardless of which bidder prevailed.\\n The City\\'s theory in this case is anything but straightforward: Multiple steps, as we have detailed, separate the alleged fraud from the asserted injury. And in contrast to Bridge, where there were \"no independent factors that account[ed] for [the plaintiff\\'s] injury,\" ibid., here there certainly were: The City\\'s theory of liability rests on the independent actions of third and even fourth parties.\\n The City at various points during the proceedings below described its injury as the lost \"opportunity to tax\" rather than \"lost tax revenue.\" It is not clear that there is a substantive distinction between the two descriptions. In any event, before this Court, the City\\'s argument turned on lost revenue, not a lost opportunity to collect it. See, e.g., Brief for Respondent i (\"Counter-Question Presented[:] Does the City of New York have standing under RICO because lost tax revenue constitutes a direct injury to the City\\'s \\'business or property\\' in accord with the statute, 18 U. S. C. §1964(c), and this Court\\'s authority?\"); id., at 40 (\"[T]he City alleges that it has been injured (the loss of tax revenues) by defendants\\' RICO violations\"). Indeed, in its entire brief on the merits, the City never uses the word \"opportunity\" (or anything similar) to describe its injury.\\n Perhaps the City articulated its argument in terms of the lost revenue itself to meet Hemi\\'s contention that an injury to the mere \"opportunity to collect\" taxes fell short of RICO\\'s injury to \"property\" requirement. Brief for Petitioners 25 (\"The opportunity to collect taxes from those who did owe them ... falls within a class of expectation interests that do not qualify as injury to business or property and therefore do not confer civil RICO standing\" (internal quotation marks omitted)); see Cleveland v. United States, 531 U. S. 12, 15 (2000) (\"It does not suffice ... that the object of the fraud may become property in the recipient\\'s hands; for purposes of the mail fraud statute, the thing obtained must be property in the hands of the victim\").\\n That is not to say, however, that the City would fare any better on the causation question had it framed its argument in terms of a lost opportunity. Hemi\\'s filing obligation would still be to the State, and any harm to the City would still be caused directly by the customers\\' failure to pay their taxes. See 541 F. 3d, at 461 (Winter, J., dissenting). Whatever the City\\'s reasons for framing its merits arguments as it has, we will not reformulate them for it now.2\\n In a final effort to save its claim, the City has shifted course before this Court. In its second amended complaint and RICO statement, the City relied solely on Hemi\\'s failure to file Jenkins Act reports with the State to form the basis of the predicate act mail and wire frauds. See Second Amended Compl. ¶¶99, 101, 118, 125; Record A980-A982. Before this Court, however, the City contends that Hemi made affirmative misrepresentations to City residents, which, the City now argues, comprise part of the RICO predicate mail and wire frauds. See Brief for Respondent 42-43. The City\\'s counsel pressed the point at oral argument, asserting that the City\\'s injury was \"caused by the seller\\'s misrepresentation, which encourages the purchasers not to pay taxes.\" Tr. of Oral Arg. 44.\\n The City, however, affirmatively disavowed below any reliance on misrepresentations to form the predicate RICO violation. The alleged false statements, the City there stated, \"are evidence of the scheme to defraud, but are not part of the fraud itself. ... [T]he scheme to defraud would exist even absent the statements.\" Record A980. The City reiterated the point: \"The scheme consists of the interstate sale of cigarettes and the failure to file Jenkins Act reports indentifying those sales.\" Ibid. \"Related to the fraud, but not a circumstance \\'constituting\\' the fraud, the defendants inform customers that [their] purchases will be concealed, and also seek to convince their customers that no taxes are owed by claiming, falsely, that the sales are tax-free.\" Id., at A982. Not only did the City disclaim any reliance upon misrepresentations to the customers to form the predicate acts under RICO, but the City made clear in its second amended complaint that its two RICO claims rested solely on the Jenkins Act violations as the predicate acts. See Second Amended Compl. ¶¶ 118, 125. Because the City defined the predicate act before the District Court as Hemi\\'s failure to file the Jenkins Act reports, and expressly disavowed reliance on the alleged misrepresentations themselves as predicate acts, we decline to consider Hemi\\'s alleged misstatements as predicate acts at this late stage.\\n* * *\\n It bears remembering what this case is about. It is about the RICO liability of a company for lost taxes it had no obligation to collect, remit, or pay, which harmed a party to whom it owed no duty. It is about imposing such liability to substitute for or complement a governing body\\'s uncertain ability or desire to collect taxes directly from those who owe them. And it is about the fact that the liability comes with treble damages and attorney\\'s fees attached. This Court has interpreted RICO broadly, consistent with its terms, but we have also held that its reach is limited by the \"requirement of a direct causal connection\" between the predicate wrong and the harm. Anza, 547 U. S., at 460. The City\\'s injuries here were not caused directly by the alleged fraud, and thus were not caused \"by reason of\" it. The City, therefore, has no RICO claim.\\n The judgment of the Court of Appeals for the Second Circuit is reversed, and the case is remanded for further proceedings consistent with this opinion.\\nIt is so ordered.\\n Justice Sotomayor took no part in the consideration or decision of this case.\\nHEMI GROUP, LLC and KAI GACHUPIN, PETITIONERS v. CITY OF NEW YORK, NEW YORK\\non writ of certiorari to the united states court of appeals for the second circuit\\n[January 25, 2010]\\n Justice Ginsburg, concurring in part and concurring in the judgment.\\n As the Court points out, this is a case \"about the RICO liability of a company for lost taxes it had no obligation to collect, remit, or pay.\" Ante, at 15. New York City (or City) cannot, consistent with the Commerce Clause, compel Hemi Group, an out-of-state seller, to collect a City sales or use tax. See Quill Corp. v. North Dakota, 504 U. S. 298, 301 (1992); National Bellas Hess, Inc. v. Department of Revenue of Ill., 386 U. S. 753, 758 (1967). Unable to impose its tax on Hemi Group, or to require Hemi Group to collect its tax, New York City is attempting to use the Racketeer Influenced and Corrupt Act (RICO), 18 U. S. C. §1964(c), in combination with the Jenkins Act, 15 U. S. C. §§375-378, to overcome that disability.\\n Hemi Group committed fraud only insofar as it violated the Jenkins Act by failing to report the names and addresses of New York purchasers to New York State. There is no other grounding for the City\\'s charge that it was defrauded by Hemi Group. \"Absent the Jenkins Act, [Hemi Group] would have owed no duty to disclose [its] sales to anyone, and [its] failure to disclose could not conceivably be deemed fraud of any kind.\" City of New York v. Smokes-Spirits.com, Inc., 541 F. 3d 425, 460 (CA2 2008) (Winter, J., dissenting in part and concurring in part).\\n Because \"the alleged fraud is based on violations of ... the Jenkins Act, ... the nature and consequences of the fraud are [properly] determined solely by the scope of that Act.\" Id., at 459. But \"conspicuously absent from the City\\'s pleadings is any claim brought pursuant to the Jenkins Act itself, rather than RICO, seeking enforcement of the Jenkins Act.\" Id., at 460. The City thus effectively admits that its claim is outside the scope of the very statute on which it builds its RICO suit.\\n I resist reading RICO to allow the City to end-run its lack of authority to collect tobacco taxes from Hemi Group or to reshape the \"quite limited remedies\" Congress has provided for violations of the Jenkins Act, see ante, at 13, n. 2. Without subscribing to the broader range of the Court\\'s proximate cause analysis, I join the Court\\'s opinion to the extent it is consistent with the above-stated view, and I concur in the Court\\'s judgment.\\nHEMI GROUP, LLC and KAI GACHUPIN, PETITIONERS v. CITY OF NEW YORK, NEW YORK\\non writ of certiorari to the united states court of appeals for the second circuit\\n[January 25, 2010]\\n Justice Breyer, with whom Justice Stevens and Justice Kennedy join, dissenting.\\n In my view, the Hemi Group\\'s failure to provide New York State with the names and addresses of its New York City cigarette customers proximately caused New York City to lose tobacco tax revenue. I dissent from the Court\\'s contrary holding.\\nI\\nA\\n Although the ultimate legal issue is a simple one, the statutory framework within which it arises is complex. As the majority points out, ante, at 3, the Racketeer Influenced and Corrupt Organizations Act (RICO), 18 U. S. C. §§1961-1968, provides a private cause of action (and treble damages) to \"[a]ny person injured in\" that person\\'s \"business or property by reason of\" conduct that involves a \"pattern of racketeering activity.\" §§1964(c) (emphasis added), 1962. RICO defines \"racketeering activity\" to include violations of various predicate criminal statutes including mail and wire fraud. §1961(1). The \"pattern of racketeering\" at issue here consists of repeated instances of mail fraud, which in turn consist largely of violations of the federal Jenkins Act, 15 U. S. C. §§375-378. That Act seeks to help States collect tobacco taxes by requiring out-of-state cigarette sellers, such as Hemi, to file reports with state tobacco tax administrators identifying the names and addresses of in-state customers and the amounts they purchased. The violations consist of Hemi\\'s intentional failure to do so.\\n As the majority points out, we must assume for present purposes that an intentional failure to file Jenkins Act reports counts as mail fraud (at least where the failure is part of a scheme that includes use of the mails). Ante, at 4. Lower courts have sometimes so held. See United States v. Melvin, 544 F. 2d 767, 773-777 (CA5 1977); United States v. Brewer, 528 F. 2d 492, 497-498 (CA4 1975). The Court of Appeals here so held. City of New York v. Smokes-Spirits.com, Inc., 541 F. 3d 425, 446 (CA2 2008). And no one has challenged that holding.\\n We must also assume that Hemi\\'s \"intentiona[l] conceal[ment]\" of the name/address/purchase information, Second Am. Compl. ¶¶103, 104, is the legal equivalent of an affirmative representation that Hemi had no New York City customers. See Restatement (Second) of Torts §551, p. 119 (1976) (a person \"who fails to disclose ... a fact\" may be \"subject to ... liability\" as if \"he had represented the nonexistence of the matter that he has failed to disclose\"); cf. Stewart v. Wyoming Cattle Ranche Co., 128 U. S. 383, 388 (1888) (concealment or suppression of material fact equivalent to a false representation). On these assumptions, the question before us is whether New York City\\'s loss of tax revenues constitutes an injury to its \"business or property by reason of\" Hemi\\'s Jenkins Act misrepresentations.\\nB\\n The case arises as a result of the District Court\\'s dismissal of New York City\\'s RICO complaint. Fed. Rule Civ. Proc. 12(b)(6). Hence we must answer the question in light of the facts alleged, taking as true the facts pleaded in the complaint (along with the \"RICO statement\" submitted pursuant to the District Court\\'s rule). Bridge v. Phoenix Bond & Indemnity Co., 553 U. S. ___, ___, n. 1 (2008) (slip op., at 1, n. 1). Those facts (as I interpret them) include the following:\\n 1. New York State (or State) and New York City (or City) both impose tobacco taxes on New York cigarette buyers. Second Am. Compl. ¶37.\\n 2. Both City and State normally collect the taxes from in-state cigarette sellers, who, in turn, charge retail customers. Id., ¶¶4, 6.\\n 3. Hemi, an out-of-state company, sells cigarettes over the Internet to in-state buyers at prices that are lower than in-state cigarette prices. The difference in price is almost entirely attributable to the fact that Hemi\\'s prices do not include any charge for New York taxes. Hemi advertises its cigarettes as \"tax free\" and often adds that it \"does not report any sales activity to any State taxing authority.\" Id., ¶¶2, 6, 108b (internal quotation marks omitted; emphasis deleted).\\n 4. New York State normally receives Jenkins Act reports from out-of-state sellers. It is contractually obliged to pass the information on to New York City (and I assume it normally does so). Id., ¶¶8-9, 11, 54-57.\\n 5. When it receives Jenkins-Act-type information, New York City writes letters to resident customers asking them to pay the tobacco tax they owe. As a result, New York City collects about 40% of the tax due. (By doing so, in 2005 the City obtained $400,000 out of $1 million owed.) Id., ¶¶58-59.\\n 6. Hemi has consistently and intentionally failed to file Jenkins Act reports in order to prevent both State and City from collecting the tobacco taxes that Hemi\\'s in-state customers owe and which otherwise many of those customers would pay. Id., ¶¶13, 24, 58.\\nII\\nA\\n The majority asks whether New York City stated a valid cause of action in alleging that it lost tobacco tax revenue \"by reason of\" Hemi\\'s unlawful misrepresentations. The facts just set forth make clear that we must answer that question affirmatively. For one thing, no one denies that Hemi\\'s misrepresentation was a \"but-for\" condition of New York City\\'s loss. In the absence of the misrepresentation, i.e., had Hemi told New York State the truth about its New York City customers, New York City would have written letters to the purchasers and obtained a significant share of the tobacco taxes buyers owed.\\n For another thing, New York City\\'s losses are \"reasonably foreseeable\" results of the misrepresentation. It is foreseeable that, without the name/address/purchase information, New York City would not be able to write successful dunning letters, and it is foreseeable that, with that information, it would be able to write successful dunning letters. Indeed, that is a natural inference from, among other things, the complaint\\'s assertion that Hemi advertised that it did not \"report\" sales information to \"State taxing authorit[ies].\" See, e.g., Smith v. Bolles, 132 U. S. 125, 130 (1889) (for causation purposes, \" \\'those results are proximate which the wrong-doer from his position must have contemplated as the probable consequence of his fraud or breach of contract\\' \" (quoting Crater v. Binninger, 33 N. J. L. 513, 518 (Ct. Errors and Appeals 1869)); see also W. Keeton, D. Dobbs, R. Keeton, & D. Owen, Prosser and Keeton on Law of Torts §110, p. 767 (5th ed. 1984) (hereinafter Prosser and Keeton); 3 S. Speiser, C. Krause, & A. Gans, The American Law of Torts §11:3, p. 68 (2003) (\"By far the most treated and most discussed aspect of the law of proximate or legal cause is the so-called doctrine of foreseeability\"). But cf. ante, at 9 (\"The dissent would have RICO\\'s proximate cause requirement turn on foreseeability ...\").\\n Further, Hemi misrepresented the relevant facts in order to bring about New York City\\'s relevant loss. It knew the loss would occur; it intended the loss to occur; one might even say it desired the loss to occur. It is difficult to find common-law cases denying liability for a wrongdoer\\'s intended consequences, particularly where those consequences are also foreseeable. Cf. Bridge, supra, at ___-___ (slip op., at 9-10) (\"[S]uppose an enterprise that wants to get rid of rival businesses mails representations about them to their customers and suppliers, but not to the rivals themselves. If the rival businesses lose money as a result of the misrepresentations, it would certainly seem that they were injured in their business \\'by reason of\\' a pattern of mail fraud ...\"); N. M. ex rel. Caleb v. Daniel E., 2008 UT 1, ¶7, n. 3, 175 P. 3d 566, 569, n. 3 (\"[I]f an unskilled marksman were to shoot a single bullet at a distant individual with the intent of killing her, that individual\\'s injury or death may not be the natural and probable consequence of the [shooter\\'s] act[,] ... [but] the harm would not be an accident because the shooter intended the harm, even though the likelihood of success was improbable\"); 1 F. Harper & F. James, The Law of Torts, §7.13, p. 584 (1956) (explaining that, ordinarily, \"all intended consequences\" of an intentional act \"are proximate\").\\n In addition, New York City\\'s revenue loss falls squarely within the bounds of the kinds of harms that the Jenkins Act (essentially the predicate statute) seeks to prevent. The statute is entitled \"An Act To assist States in collecting sales and use taxes on cigarettes.\" 63 Stat. 884. I have no reason to believe the Act intends any different result with respect to collection of a city\\'s tobacco tax assessed under the authority of state law. See N. Y. Unconsol. Law Ann. §9436(1) (West Supp. 2009) (authorizing cities with over one million inhabitants to impose their own cigarette taxes). The Restatement (Second) of Torts explains that where\\n\"a statute requires information to be furnished ... for the protection of a particular class of persons, one who makes a fraudulent misrepresentation ... is subject to liability to the persons for pecuniary loss ... in a transaction of the kind in which the statute is intended to protect them.\" §536, at 77 (1976).\\nSee also §536, Appendix (citing supporting cases in the Reporter\\'s Note).\\n Finally, we have acknowledged that \"Congress modeled §1964(c) on the civil-action provision of the federal antitrust laws,\" and we have therefore looked to those laws as an interpretive aid in RICO cases. Holmes v. Securities Investor Protection Corporation, 503 U. S. 258, 267, 268 (1992). I can find no antitrust analogy that suggests any lack of causation here, nor has the majority referred to any such analogical antitrust circumstance.\\n The upshot is that the harm is foreseeable; it is a consequence that Hemi intended, indeed desired; and it falls well within the set of risks that Congress sought to prevent. Neither antitrust analogy nor any statutory policy of which I am aware precludes a finding of \"proximate cause.\" I recognize that some of our opinions may be read to suggest that the words \"by reason of\" in RICO do not perfectly track common-law notions of proximate cause. See, e.g., Bridge, 553 U. S., at ___-___ (slip op., at 14-16). But where so much basic common law argues in favor of such a finding, how can the Court avoid that conclusion here?\\nB\\n The majority bases its contrary conclusion upon three special circumstances and its reading of two of this Court\\'s prior cases. In my view, none of the three circumstances precludes finding causation (indeed two are not even relevant to the causation issue). Nor can I find the two prior cases controlling.\\n The three circumstances are the following: First, the majority seems to argue that the intervening voluntary acts of third parties, namely, the customers\\' own independent failures to pay the tax, cuts the causal chain. Ante, at 8 (\"[T]he City\\'s harm was directly caused by the customers, not Hemi\"); see Saugerties Bank v. Delaware & Hudson Co., 236 N. Y. 425, 430, 141 N. E. 904, 905 (1923) (third party\\'s forgery of a bill of lading an intervening cause); Prosser and Keeton §44, at 313-314 (collecting cases on intervening intentional or criminal acts). But an intervening third-party act, even if criminal, does not cut a causal chain where the intervening act is foreseeable and the defendant\\'s conduct increases the risk of its occurrence. See Lillie v. Thompson, 332 U. S. 459, 462 (1947) (per curiam); Horan v. Watertown, 217 Mass. 185, 186, 104 N. E. 464, 465 (1914); see also Restatement (Second) of Torts §435A, at 454 (1963-1964) (intentional tortfeasor liable for intended harm \"except where the harm results from an outside force the risk of which is not increased by the defendant\\'s act\"). Hemi\\'s act here did increase the risk that New York City would not be paid; and not only was the risk foreseeable, but Hemi\\'s advertising strongly suggests that Hemi actually knew nonreporting would likely bring about this very harm. The majority claims that \"directness,\" rather than foreseeability, should be our guide in assessing proximate cause, and that the lack of a \"direct\" relationship in this case precludes a finding of proximate causation. Ante, at 9-10. But courts used this concept of directness in tort law to expand liability (for direct consequences) beyond what was foreseeable, not to eliminate liability for what was foreseeable. Thus, under the \"directness\" theory of proximate causation, there is liability for both \"all \\'direct\\' (or \\'directly traceable\\') consequences and those indirect consequences that are foreseeable.\" Prosser and Keeton §42, at 273 (emphasis added); see also id., §43, at 294, and n. 17 (citing Nunan v. Bennett, 184 Ky. 591, 212 S. W. 570 (1919)). I do not read this Court\\'s opinions in Holmes or Anza v. Ideal Steel Supply Corp., 547 U. S. 451 (2006), to invoke anything other than this traditional understanding.\\n Second, the majority correctly points out that Hemi misrepresented the situation to the State, not to the City — a circumstance which, the majority believes, significantly separates misrepresentation from harm. Ante, at 8. But how could that be so? New York State signed a contract promising to relay relevant information to the City. In respect to that relevant information, the State is a conduit, indeed roughly analogous to a postal employee. This Court has recognized specifically that \"under the common law a fraud may be established when the defendant has made use of a third party to reach the target of the fraud.\" Tanner v. United States. The treatises say the same. See, e.g., Prosser and Keeton §107, at 743-745; 26 C. J. S., Fraud §47, p. 1121 (1921) (collecting cases); see also Prosser, Misrepresentation and Third Parties, 19 Vand. L. Rev. 231, 240-241, and nn. 56-59, 62-64 (1966) (collecting cases). This Court has never suggested the contrary, namely, that a defendant is not liable for (foreseeable) harm (intentionally) caused to the target of a scheme to defraud simply because the misrepresentation was transmitted via a third (or even a fourth or fifth) party. Cf. Terry, Intent to Defraud, 25 Yale L. J. 87, 93 (1915) (\"When a representation is communicated through one person to another in such circumstances that it can be deemed to be directed to the latter, it makes no difference through how many persons or by how circuitous a route it reaches the latter ...\").\\n Third, the majority places great weight upon its view that Hemi tried to defraud the State, not the City. Ante, at 8-9. Hemi, however, sought to defraud both. Third Amended RICO Statement ¶d (explaining that \"[e]very other State or local government that imposes a use tax on cigarettes and whose residents purchase cigarettes\" from Hemi is a victim of its scheme to defraud). Hemi sought to prevent the State from collecting state taxes; and it sought to prevent the City from collecting city taxes. Here we are concerned only with the latter. In respect to the latter, the State was an information conduit. The fact that state taxes were also involved is beside the point.\\n The two Supreme Court cases to which the majority refers involve significantly different causal circumstances. Ante, at 5-8. The predicate acts in Holmes — the defendant\\'s acts that led to the plaintiff\\'s harm — consisted of securities frauds. The defendant misrepresented the prospects of one company and misled the investing public into falsely believing that it could readily buy and sell the stock of another. When the truth came out, stock prices fell, investors (specifically, stockbrokers) lost money, and since the stockbrokers could not pay certain creditors, those creditors also lost money. 503 U. S., at 262-263. Claiming subrogation to stand in the shoes of the creditors, the Securities Investor Protection Corporation sued. Id., at 270-271.\\n Since the creditors had not bought the securities, there was little reason to believe the defendant intended their harm. And the securities statutes seek, first and foremost, to protect investors, not creditors of those who sell stock to those investors. The latter harm (a broker\\'s creditor\\'s loss) differs in kind from the harm that the \"predicate act\" statute primarily seeks to avoid and that its violation would ordinarily cause (namely, investors\\' stock-related monetary losses). As Part II-A, supra, points out, neither of these circumstances is present here.\\n In Anza, the plaintiff was a business competitor of the defendants. The plaintiff claimed that the defendants falsely told state officials that they did not owe sales tax. The plaintiff added that, had the defendants paid the tax they owed, the defendants would have had less money available to run their business, and the plaintiff consequently would have been able to compete against them more effectively. 547 U. S., at 454, 457-458.\\n Again, in Anza the kind of harm that the plaintiff alleged is not the kind of harm that the tax statutes primarily seek to prevent. Rather, it alleged a kind of harm (competitive injury) that tax violations do not ordinarily cause and which ordinarily flows from the regular operation of a competitive marketplace. Thus, in both Holmes and Anza, unlike the present case, plaintiffs alleged special harm, neither squarely within the class of harms at which the relevant statutes were directed, nor of a kind that typical violators would intend or even foresee.\\n Bridge, which the majority seeks to distinguish, ante, at 11-12, is a more closely analogous case. The defendants in that case directed agents to misrepresent to a county that they qualified as independent bidders at a county-run property auction. They consequently participated in the auction. And the plaintiffs, facing additional bidders, lost some of the property that they otherwise would have won — all to their financial disadvantage. 553 U. S., at ___-___ (slip op., at 3-4). The harm was foreseeable; it was intended; and it was precisely the kind of harm that the county\\'s bidding rules sought to prevent. Thus this Court held that the harm was \"a foreseeable and natural consequence of [the defendants\\'] scheme.\" Id., at ___ (slip op., at 18).\\n In sum, the majority recognizes that \"[p]roximate cause for RICO purposes ... should be evaluated in light of its common law foundations,\" ante, at 6, but those foundations do not support the majority\\'s view. Moreover, the majority\\'s rationale would free from RICO liability defendants who would appear to fall within its intended scope. Consider, for example, a group of defendants who use a marketing firm (in RICO terms, an \"enterprise\") to perpetrate a variation on a \"pump and dump\" scheme. See, e.g., United States v. Salmonese, 352 F. 3d 608, 612 (CA2 2003). They deliberately and repeatedly make egregiously fraudulent misrepresentations to inflate the price of securities that, unbeknownst to investors, they own. After the stock price rises, the defendants sell at an artificial profit. When the fraud is revealed, the price crashes, to the investors\\' detriment. Suppose the defendants have intentionally spoken directly only to intermediaries who simply repeated the information to potential investors, and have not had any contact with the investors themselves. Under the majority\\'s reasoning, these defendants apparently did not proximately cause the investors\\' losses and are not liable under RICO.\\nIII\\n If there is causation, we must decide whether, for RICO purposes, the City\\'s loss of tax revenue is \" \\'business or property\\' under 18 U. S. C. §1964(c).\" Ante, at 5 (acknowledging, but not reaching, this second issue). The question has led to concern among the lower courts. Some fear that an affirmative answer would turn RICO into a tax collection statute, permitting States to bring RICO actions and recover treble damages for behavior that amounts to no more than a failure to pay taxes due. See, e.g., Michigan, Dept. of Treasury, Revenue Div. v. Fawaz, No. 86-1809, 1988 WL 44736, *2 (CA6 1988) (holding that tax revenue is not RICO \"property\" lest district courts become \"collection agencies for unpaid state taxes\"); Illinois Dept. of Revenue v. Phillips, 771 F. 2d 312, 316, 312 (CA7 1985) (holding, \"reluctantly,\" that \"a state\\'s Department of Revenue may file suit in federal court for treble damages under [RICO] against a retailer who files fraudulent state sales tax returns\").\\n In a related context, however, the Department of Justice has taken steps to avoid the \"tax collection agency\" problem without reading all tax-related frauds out of similar federal criminal statutes. The Department\\'s prosecution guidelines require prosecutors considering a tax-related mail fraud or wire fraud or bank fraud prosecution (or a related RICO prosecution) to obtain approval from high-level Department officials. And those guidelines specify that the Department will grant that approval only where there is at issue \"a large fraud loss or a substantial pattern of conduct\" and will not do so, absent \"unusual circumstances,\" in cases involving simply \"one person\\'s tax liability.\" Dept. of Justice, United States Attorneys\\' Manual §6-4.210(A) (2007), online at http://www.justice. gov/usao/eousa/foia_reading_room/usam/title6/4mtax.htm (as visited Jan. 20, 2010, and available in Clerk of Court\\'s case file); see also §6-4.210(B) (explaining that the Department \"will not authorize the use of mail, wire or bank fraud charges to convert routine tax prosecutions into RICO ... cases\").\\n This case involves an extensive pattern of fraudulent conduct, large revenue losses, and many different unrelated potential taxpayers. The Department\\'s guidelines would appear to authorize prosecution in these circumstances. And limiting my consideration to these circumstances, I would find that this RICO complaint asserts a valid harm to \"business or property.\" I need not and do not express a view as to how or whether RICO\\'s civil action provisions apply to simpler instances of individual tax liability.\\n This conclusion is virtually compelled by Pasquantino v. United States, 544 U. S. 349 (2005), a case that we decided only five years ago. We there pointed out that the right to uncollected taxes is an \"entitlement to collect money ... , the possession of which is \\'something of value.\\' \" Id., at 355 (quoting McNally v. United States, 483 U. S. 350, 358 (1987)). Such an entitlement \"has long been thought to be a species of property.\" 544 U. S., at 356 (citing 3 W. Blackstone, Commentaries on the Laws of England 153-155 (1768)). And \"fraud at common law included a scheme to deprive a victim of his entitlement to money.\" 544 U. S., at 356. We observed that tax evasion \"inflict[s] an economic injury no less than\" the \"embezzle[ment] [of] funds from the ... treasury.\" Ibid. And we consequently held that \"Canada\\'s right to uncollected excise taxes on the liquor petitioners imported into Canada\" is \" \\'property\\' \" within the terms of the mail fraud statute. Id., at 355.\\n Hemi points in reply to our decision in Hawaii v. Standard Oil Co. of Cal., 405 U. S. 251 (1972). But that case involved not a loss of tax revenues, but \"injury to the general economy of a State\"--insofar as it was threatened by violations of antitrust law. Id., at 260. Hawaii\\'s interest, both more general and derivative of harm to individual businesses, differs significantly from the particular tax loss at issue in Pasquantino and directly at issue here.\\n We have previously made clear that the compensable injury for RICO purposes is the harm caused by the predicate acts. See generally Sedima, S. P. R. L. v. Imrex Co., 473 U. S. 479, 495-496 (1985); cf. Cleveland v. United States, 531 U. S. 12, 25 (2000). I can find no convincing reason in the context of this case to distinguish in the circumstances present here between \"property\" as used in the mail fraud statute and \"property\" as used in RICO. Hence, I would postpone for another day the question whether RICO covers instances where little more than the liability of an individual taxpayer is at issue. And I would find in the respondent\\'s favor here.\\n With respect, I dissent.\\nFOOTNOTESFootnote 1 Even if we were willing to look to Hemi\\'s intent, as the dissent suggests we should, the City would fare no better. Hemi\\'s aim was not to defraud the City (or the State, for that matter) of tax revenue, but to sell more cigarettes. Hemi itself neither owed taxes nor was obliged to collect and remit them. This all suggests that Hemi\\'s alleged fraud was aimed at Hemi\\'s competitors, not the City. But Anza teaches that the competitors\\' injuries in such a case are too attenuated to state a RICO claim.\\nFootnote 2 The dissent recognizes that its position poses the troubling specter of turning RICO into a tax collection statute. Post, at 11-12 (opinion of Breyer, J.). The dissent\\'s answer looks largely to prosecution policy set forth in the Federal Department of Justice Guidelines, which are, of course, not only changeable, but have no applicability whatever to state or local governments. Under the decision below and the dissent\\'s position, RICO could be used as a tax collection device based solely on the failure to file reports under the Jenkins Act, which itself provides quite limited remedies. See 15 U. S. C. §377 (providing that a violation of the Jenkins Act may be punished as a misdemeanor with a fine up to $1,000 and imprisonment for no more than six months). And that device would be available not only to the State, to which the reports were due, but also to the City, to which Hemi owed no duty under the Act and to which it owed no taxes.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "eSwQ4Rw6dWaT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('New York City cigarette customer proximately', 0.011112899551245775),\n",
       " ('New York City customer', 0.008526029601933447),\n",
       " ('City cigarette taxis', 0.008505197087640361),\n",
       " ('fraudulent state sale tax return', 0.008419949829831859),\n",
       " ('RICO liability defendant', 0.007587677484802361),\n",
       " ('predicate RICO violation', 0.007533524058600452),\n",
       " ('additional cigarette tax revenue', 0.007226747654217487),\n",
       " ('cigarette excise tax revenue', 0.0071943576859784955),\n",
       " ('alleged RICO violation', 0.0068618184021564275),\n",
       " ('federal RICO claim', 0.006860500165834048)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the TextRank algorithm (Mihalcea, R., & Tarau, P., 2004) for a given document\n",
    "\n",
    "key_terms_textrank = ke.textrank(corpus[0])\n",
    "key_terms_textrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRyHfZekewCT"
   },
   "source": [
    "For comparison, we'll take a look at another algorithm, Yake (Campos et al., 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "-HWvJ8ube4Vc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('New York City', 0.002303574630054324),\n",
       " ('New York State', 0.005825936311930557),\n",
       " ('U. S. C.', 0.007495164245429919),\n",
       " ('Jenkins Act', 0.012174166295492686),\n",
       " ('York City customer', 0.022104604195379497),\n",
       " ('RICO', 0.027239031407833788),\n",
       " ('Hemi Group', 0.03290147970895093),\n",
       " ('York City cigarette', 0.03532160832217437),\n",
       " ('Jenkins Act information', 0.03782954647832433),\n",
       " ('RICO claim', 0.044919865631808144)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_terms_yake = ke.yake(corpus[0])\n",
    "key_terms_yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P296E9nxhnQZ"
   },
   "source": [
    "### Activity:\n",
    "Let's combine a few different pieces. Try filtering the corpus on some metadata to construct a sub-corpus. Then use one of the textacy keyword algorithms to determine the most common keywords across your subcorpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9otxxqxx_HI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuk7s1yZlvVi"
   },
   "source": [
    "## Keyword in context\n",
    "\n",
    "Sometimes researchers find it helpful just to see a particular keyword in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "FxgjXFnRla1-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEMI GROUP, LLC AND KAI GACHUPIN v. CITY OF NEW YORK, NEW YORK\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'textacy' has no attribute 'text_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-2e84ad1af296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'case_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKWIC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"judgment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'textacy' has no attribute 'text_utils'"
     ]
    }
   ],
   "source": [
    "for doc in corpus[:5]:\n",
    "    print(doc._.meta.get('case_name'))\n",
    "    for match in textacy.text_utils.KWIC(doc.text, \"judgment\", print_only=False):\n",
    "        print(\" \".join(match).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuSHX_2OVyUb"
   },
   "source": [
    "## Vectorization\n",
    "\n",
    "Let's continue with corpus-level analysis by taking advantage of textacy's vectorizer class, which wraps functionality from `scikit-learn` to count the prevalence of certain tokens in each document of the corpus and to apply weights to these counts if desired. We could just work directly in `scikit-learn`, but it can be nice for mental overhead to learn one library and be able to do a great deal with it.\n",
    "\n",
    "We'll create a vectorizer, sticking with the normal term frequency defaults but discarding words that appear in fewer than 3 documents or more than 95% of documents. We'll also limit our features to the top 500 words according to document frequency. This means our feature set, or columns, will have a higher degree of representation across the corpus. We could further scale these counts according to document frequency (or inverse document frequency) weights, or normalize the weights so that they add up to 1 for each document row (L1 norm), and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LfE48GbVyUb"
   },
   "outputs": [],
   "source": [
    "import textacy.vsm\n",
    "\n",
    "vectorizer = textacy.vsm.Vectorizer(min_df=3, max_df=.95, max_n_terms=500)\n",
    "\n",
    "tokenized_corpus = [[token.orth_ for token in list(textacy.extract.words(doc, filter_nums=True, filter_stops=True, filter_punct=True))] for doc in corpus]\n",
    "\n",
    "dtm = vectorizer.fit_transform(tokenized_corpus)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5lQ10twVyUf"
   },
   "source": [
    "We have now have a matrix representation of our corpus, where rows are documents, and columns (or features) are words from the corpus. The value at any given point is the number of times that the word appears in that document. Once we have a document-term matrix, we could do several things with it just within textacy, though we also can pass it into different algorithms within `scikit-learn` or other libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37KLE01TVyUg"
   },
   "outputs": [],
   "source": [
    "# Let's look at some of the terms\n",
    "vectorizer.terms_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_I6y0JIVyUh"
   },
   "source": [
    "We can see that we are still getting a number of terms which might be filtered out, such as symbols and abbreviations. The most straightforward solutions are to filter the terms against a dictionary during vectorization, which carries the risk of inadvertently filtering words that you'd prefer to keep in the dataset, or curating a custom stopword list, which can be inflexible and time consuming. Otherwise, it is often the case that the corpus analysis tools used with the vectorized texts (e.g., topic modeling or stylistic analysis -- see below) have ways of recognizing and sequestering unwanted terms so that they can be excluded from the results if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7c7uSmdnLOz"
   },
   "source": [
    "## Topic modeling\n",
    "\n",
    "Let's look quickly at one example of what we can do with a vectorized corpus. Topic modeling is very popular for semantic exploration of texts, and there are numerous implementations of it. Textacy uses implementations from scikit-learn. \n",
    "\n",
    "Our corpus is rather small for topic modeling, but just to see how it's done here, we'll go ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12O2kmUOQx43"
   },
   "source": [
    "First, though, topic modeling works best when the texts are divided into approximately equal-sized \"chunks.\" A quick word-count of the corpus will show that the decisions are of quite variable lengths, which will skew the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwoHLaL8RgNC"
   },
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    print(doc._.meta.get('case_name'), doc._.meta.get('decision_date'), doc.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L41Er3jBRhVG"
   },
   "source": [
    "We'll re-chunk the texts into documents of not more than 500 words and then recompute the document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6XnxyCBR0wY"
   },
   "outputs": [],
   "source": [
    "chunked_corpus_unflattened = [ [text[x:x+500] for x in range(0,len(text),500)] for text in tokenized_corpus]\n",
    "chunked_corpus = list(itertools.chain.from_iterable(chunked_corpus_unflattened))\n",
    "chunked_dtm = vectorizer.fit_transform(chunked_corpus)\n",
    "chunked_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFusfd9sm1dv"
   },
   "outputs": [],
   "source": [
    "import textacy.tm\n",
    "\n",
    "model = textacy.tm.TopicModel(\"lda\", n_topics=15)\n",
    "model.fit(chunked_dtm)\n",
    "doc_topic_matrix = model.transform(chunked_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zP_oC22nJLq"
   },
   "outputs": [],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=10):\n",
    "  print(\"topic\", topic_idx, f\"{model.topic_weights(doc_topic_matrix)[topic_idx]:.0%}\", \":\", \"   \".join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiSsVaW3x_HL"
   },
   "source": [
    "## Document similarity with word2vec and clustering\n",
    "\n",
    "textacy provides several built-in methods for measuring the degree of similarity between two documents, including a `word2vec`-based approach that computes the semantic similarity between documents based on the word vector model included with the spaCy language model. This technique is capable of inferring, for example, that two documents are topically related even if they don't share any words but use synonyms for a shared concept.\n",
    "\n",
    "To evaluate this similarity comparison, we'll compute the similarity of each pair of docs in the corpus, and then branch out into `scikit-learn` a bit to look for clusters based on these similarity measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY1Hwcpxx_HL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dim = corpus.n_docs\n",
    "\n",
    "distance_matrix = np.zeros((dim,dim))\n",
    "    \n",
    "for i, doc_i in enumerate(corpus):\n",
    "    for j, doc_j in enumerate(corpus):\n",
    "        if i == j:\n",
    "            continue # defaults to 0\n",
    "        if i > j:\n",
    "            distance_matrix[i,j] = distance_matrix[j,i]\n",
    "        else:\n",
    "            distance_matrix[i,j] = 1 - textacy.similarity.word2vec(doc_i, doc_j)\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WMJNsAJx_HZ"
   },
   "source": [
    "The [OPTICS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) hierarchical density-based clustering algorithm only finds one cluster with its default settings, but an examination of the legal issue types coded to each decision indicates that the `word2vec`-based clustering has indeed produced a group of semantically related documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ac0aESX3x_Ha"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "clustering = OPTICS(metric='precomputed').fit(distance_matrix)\n",
    "print(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4NiQ0BPx_Ha"
   },
   "outputs": [],
   "source": [
    "for i, cluster_id in enumerate(clustering.labels_):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    print(cluster_id, corpus[i]._.meta['us_cite_id'], data.issue_area_codes[corpus[i]._.meta['issue_area']], ':', data.issue_codes[corpus[i]._.meta['issue']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IxCE-RBx_Hb"
   },
   "source": [
    "## Case study: Stylistic analysis of U.S. Supreme Court opinions\n",
    "\n",
    "One of the more impressive affordances of text analysis is the ability to infer the authorship of a text with some degree of confidence based only upon the stylistic attributes observed through statistical analysis of the writer's use of common \"function\" words (conjunctions, articles). Note however that if a writer desires to be anonymous, she can attempt to confuse such algorithms by intentionally adopting a different writing style.\n",
    "\n",
    "The U.S. Supreme Court decisions corpus does usually identify the author of each opinion, but sometimes these are inconsistently described in the text, especially for decisions issued with no majority author listed (e.g., unanimous decisions). Applying stylistic analysis techniques to a substantial subset of the corpus (here we use just the majority author portion of the decisions from 1993 to 2016) also can expose similarities between the writing styles of particular justices -- or perhaps of their clerks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZk5b7zxVyTz"
   },
   "outputs": [],
   "source": [
    "# This code replaces the corpus above with a much larger corpus that may take a long\n",
    "# time to process and which may take up a considerable amount of RAM.\n",
    "\n",
    "# Note that the text of each decision in this corpus is trimmed to include only\n",
    "# the opinion of the majority author (and not any footnotes, supporting or\n",
    "# dissenting opinions, etc.) via regular expression matching of formulaic\n",
    "# phrases in the case file, specifically:\n",
    "# [JUSTICE NAME] delivered the opinion of the Court.\n",
    "# --- MAJORITY AUTHOR'S OPINION HERE ---\n",
    "# It is so ordered.\n",
    "\n",
    "\"\"\"\n",
    "corpus = textacy.Corpus(nlp)\n",
    "import re\n",
    "\n",
    "# 1986 - 2016 = Rehnquist and Roberts courts, but start at 1993 because there was a lot of turnover then\n",
    "for year in range(1993, 2017):\n",
    "    print(\"Finding record(s) from\",year)\n",
    "    year_generator = data.records(date_range=(str(year)+'-01-01', str(year)+'-12-31'))\n",
    "    for record in year_generator:\n",
    "        # There are only 2 opinions in this range by Byron White; exclude them\n",
    "        if record[1]['maj_opinion_author'] == 95:\n",
    "            continue\n",
    "        text = record[0]\n",
    "        # NOTE: These regexs works well on 1993-2016, but it is NOT guaranteed to work on the full corpus\n",
    "        if record[1]['maj_opinion_author'] != -1:\n",
    "            match = re.search(r\"delivered the opinion of the Court\\.(.*)It is so ordered\", text, flags=re.I | re.M | re.DOTALL)\n",
    "        else:\n",
    "            match = re.search(r\"Per Curiam\\.(.*)It is so ordered\", text, flags=re.I | re.M | re.DOTALL)\n",
    "        if match is None:\n",
    "            continue\n",
    "        opinion = match.group(1)\n",
    "        if len(opinion) < 500:\n",
    "            continue\n",
    "        mutable_record = list(record)\n",
    "        mutable_record[0] = opinion\n",
    "        corpus.add_record(mutable_record)\n",
    "\"\"\"\n",
    "\n",
    "# Rather than wait for the corpus to be processed, we'll just download a pre-processed\n",
    "# version from Github. It should take about two minutes to import.\n",
    "\n",
    "!wget https://github.com/sul-cidr/Workshops/raw/master/Text_Analysis_with_Python/data/recent_opinions.bin.gz\n",
    "opinions_corpus = textacy.Corpus.load(nlp, \"recent_opinions.bin.gz\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSGBVDgZx_Hc"
   },
   "outputs": [],
   "source": [
    "# A helper function to extract the non-numeric tokens from a document, then\n",
    "# filter these by function (no nouns) and semantic (no stopwords) tokens,\n",
    "# and to return these lists as well as the sentences in the document.\n",
    "def get_doc_tokens_and_sents(doc):\n",
    "    sents = doc.sents\n",
    "    alpha_tokens = [token for token in doc if token.is_punct == False and token.is_space == False and token.is_alpha == True]\n",
    "    semantic_tokens = [token for token in alpha_tokens if token.is_stop == False]\n",
    "    function_tokens = [token for token in alpha_tokens if token.is_stop == True]\n",
    "    \n",
    "    return [sents, alpha_tokens, semantic_tokens, function_tokens]\n",
    "\n",
    "# This code just builds a list of author names from the corpus and assigns\n",
    "# them to colors and shapes for drawing plots later\n",
    "doc_author_names = []\n",
    "\n",
    "author_opinions = {}\n",
    "\n",
    "author_names = []\n",
    "author_surnames = []\n",
    "\n",
    "for i, doc in enumerate(opinions_corpus):\n",
    "    maj_author_id = doc._.meta[\"maj_opinion_author\"]\n",
    "    maj_author_name = data.opinion_author_codes[maj_author_id]\n",
    "    if maj_author_name is None:\n",
    "        maj_author_name = \"None\"\n",
    "    doc_author_names.append(maj_author_name)\n",
    "    if maj_author_name not in author_opinions:\n",
    "        author_opinions[maj_author_name] = [i]\n",
    "        author_names.append(maj_author_name)\n",
    "        author_surnames.append(maj_author_name.split(',')[0].strip())\n",
    "    else:\n",
    "        author_opinions[maj_author_name].append(i)\n",
    "    \n",
    "from matplotlib import cm\n",
    "cmap = cm.get_cmap('gist_rainbow', len(author_names))\n",
    "\n",
    "available_markers = ['o', 'v', '^', '<', '>', 's', 'P', '*', '+', 'X', 'D', '1', '2', '3', '4', 'p', 'x', '|', '_']\n",
    "\n",
    "author_name_to_color = {}\n",
    "author_name_to_marker = {}\n",
    "for i, name in enumerate(author_names):\n",
    "    author_name_to_color[name] = cmap(i)\n",
    "    author_name_to_marker[name] = available_markers[i]\n",
    "\n",
    "doc_author_colors = [author_name_to_color[name] for name in doc_author_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXGFo1OTx_Hc"
   },
   "source": [
    "We'll create a vectorized version of the majority author decisions 1993-2016 subcorpus in which all nouns have been removed, leaving mostly \"functional\" rather than \"semantic\" words behind. Although it's not entirely foolproof, this filtering step is meant to ensure that stylistic, rather than content-based attributes of the documents remain as material for comparison and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYI_18ORx_Hc"
   },
   "outputs": [],
   "source": [
    "import textacy.vsm\n",
    "\n",
    "function_vectorizer = textacy.vsm.Vectorizer(min_df=1, max_df=1.0) # This will count all words in the corpus\n",
    "# Remove the nouns\n",
    "function_corpus = [[token.lower_ for token in list(textacy.extract.words(doc, filter_nums=True, filter_stops=False, filter_punct=True, exclude_pos=['NOUN', 'PROPN']))] for doc in opinions_corpus]\n",
    "\n",
    "function_dtm = function_vectorizer.fit_transform(function_corpus)\n",
    "# For comparison, the full corpus contains 65,840 terms\n",
    "print(function_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dEHrXm_x_Hc"
   },
   "outputs": [],
   "source": [
    "# This is a helper function to compute pairwise cosine (dis)similarities of docs in the corpus.\n",
    "# cosine = similarity based on proportions of shared word frequencies\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def get_distance_matrix(method, corpus_doc_vectors):\n",
    "    dim = len(corpus_doc_vectors)\n",
    "    distance_matrix = np.zeros((dim,dim))\n",
    "    \n",
    "    for i, vec1 in enumerate(corpus_doc_vectors):\n",
    "        print(\"row\",i)\n",
    "        for j, vec2 in enumerate(corpus_doc_vectors):\n",
    "            if i == j:\n",
    "                continue # defaults to 0\n",
    "            if i > j:\n",
    "                distance_matrix[i,j] = distance_matrix[j,i]\n",
    "            else:\n",
    "                #print(vec1, vec2)\n",
    "                distance_matrix[i,j] = method(vec1, vec2)\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F35Wzwiix_Hc"
   },
   "outputs": [],
   "source": [
    "# This reorders the corpus documents by opinion author, rather than case ID/date,\n",
    "# then computes a distance matrix based on the cosine similarity of the\n",
    "# \"non-semantic\" (function) word set for each pair of documents in the corpus\n",
    "\n",
    "dim = len(opinions_corpus)\n",
    "\n",
    "reordered_corpus = [None] * dim\n",
    "reordered_author_docs = []\n",
    "\n",
    "author_start_indices = []\n",
    "author_ranges = {}\n",
    "\n",
    "new_index=0\n",
    "for i, name in enumerate(author_names):\n",
    "    author_ranges[name] = [new_index, -1]\n",
    "    author_start_indices.append(new_index)\n",
    "    for doc_index in author_opinions[name]:\n",
    "        reordered_corpus[new_index] = opinions_corpus[doc_index]\n",
    "        reordered_author_docs.append(name)\n",
    "        author_ranges[name][1] = new_index\n",
    "        new_index += 1\n",
    "\n",
    "reordered_functions = [[token.orth_ for token in list(textacy.extract.words(doc, filter_nums=True, filter_stops=False, filter_punct=True, exclude_pos=['NOUN', 'PROPN']))] for doc in reordered_corpus]\n",
    "reordered_dtm = function_vectorizer.fit_transform(reordered_functions)\n",
    "reordered_function_matrix = get_distance_matrix(distance.cosine, reordered_dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyCTO9Xyx_Hc"
   },
   "outputs": [],
   "source": [
    "# Plot a correlation heatmap matrix view of the corpus, ordered by opinion author.\n",
    "\n",
    "maxval = np.amax(reordered_function_matrix)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12, 12), dpi=80)\n",
    "plt.imshow(reordered_function_matrix, cmap=\"viridis\", vmax=maxval/2, interpolation='bicubic')\n",
    "plt.colorbar()\n",
    "ax = fig.gca()\n",
    "ax.set_xticks(author_start_indices)\n",
    "ax.set_xticklabels(author_surnames)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "# Including the left-hand labels causes the plot to be shifted incorrectly, for some reason.\n",
    "#ax.set_yticks(author_start_indices)\n",
    "#ax.set_yticklabels(author_surnames)\n",
    "\n",
    "plt.show()\n",
    "for author in author_names:\n",
    "    print(author,str(author_ranges[author][0]) + '-' + str(author_ranges[author][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUNhABB7x_Hd"
   },
   "source": [
    "The document-level stylistic similarity relationships from the matrix above\n",
    "also can be viewed as a two-dimensional scatterplot via techniques like PCA\n",
    "(Principal Component Analysis) and MDS (Multi-Dimensional Scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzTG5KNvx_Hd"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "mds = MDS(dissimilarity='precomputed')\n",
    "pos = mds.fit_transform(reordered_function_matrix)\n",
    "#pca = PCA()\n",
    "#pos = pca.fit_transform(reordered_function_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOiqWWIlEwGC"
   },
   "source": [
    "The resulting plot does not immediately reveal obvious document clusterings, but this doesn't necessarily indicate that the stylistic features are wholly uninformative..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nCKbm0xx_Hd"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12,8), dpi=72)\n",
    "\n",
    "for name in author_names:\n",
    "    color = author_name_to_color[name]\n",
    "    marker = author_name_to_marker[name]\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i, doc_name in enumerate(doc_author_names):\n",
    "        if doc_name != name:\n",
    "            continue\n",
    "        xs.append(pos[i,0])\n",
    "        ys.append(pos[i,1])\n",
    "\n",
    "    sc = plt.scatter(xs, ys, c=[color], label=name, alpha=0.7, marker=marker, edgecolors='none')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5y0lO9ex_Hd"
   },
   "source": [
    "Another approach is to train a document classifier on the observed stylistic\n",
    "features (i.e., function word frequencies) and then test the classifier to see\n",
    "how well it can differentiate between authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_r6dReJjx_Hd"
   },
   "outputs": [],
   "source": [
    "# We'll use sklearn's own vectorizer for this part\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "doc_texts = []\n",
    "\n",
    "for doc in opinions_corpus:\n",
    "    sents, alpha_tokens, semantic_tokens, function_tokens = get_doc_tokens_and_sents(doc)\n",
    "    doc_texts.append(\" \".join(token.lower_ for token in function_tokens))\n",
    "\n",
    "vectorizer.fit(doc_texts)\n",
    "\n",
    "X_texts = np.array(doc_texts)\n",
    "y_labels = np.array(doc_author_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLgJNPnhx_Hd"
   },
   "source": [
    "If we train a Naive Bayes document classifier (which is perhaps most famous for its effectiveness as a spam filter) so that it learns various word frequency associations with the author labels, we can then run the classifier against a \"held-out\" portion of the corpus to see how well it performs.\n",
    "\n",
    "Note that the resulting classification accuracy is around 50%, which is much higher than the 7% you'd exect if the classifier was just choosing one of the 15 author names at random. Not too bad considering the features it is working with -- mostly counts of words like \"a\", \"and\", and \"the\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2E8y0_y6x_He"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "def train_test_nb(random_seed):\n",
    "    (X_texts_train, X_texts_test,\n",
    "     y_labels_train, y_labels_test) = train_test_split(X_texts, y_labels, test_size=0.33, random_state=random_seed)\n",
    "\n",
    "    X_features_train = vectorizer.fit_transform(X_texts_train)\n",
    "    X_features_test = vectorizer.transform(X_texts_test)\n",
    "\n",
    "    classifier.fit(X_features_train, y_labels_train)\n",
    "\n",
    "    accuracy = classifier.score(X_features_test, y_labels_test)\n",
    "    print(\"NB accuracy:\",accuracy)\n",
    "\n",
    "# Run a few times with different train/test sets, to make sure it's not a fluke\n",
    "train_test_nb(42)\n",
    "train_test_nb(16)\n",
    "train_test_nb(97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDmG99fcx_He"
   },
   "source": [
    "A related approach is to train a classifier on the full corpus and then to run this classifier on the full corpus. Normally, this kind of \"overfitting\" produces artificially high accuracy, but it also can be used to expose cases in which the classifier gets certain classifications consistently wrong. This type of classifier \"confusion\" is a sign that the writing styles of the authors being confused are in fact similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ca_tr-exx_He"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_features = vectorizer.fit_transform(X_texts)\n",
    "\n",
    "classifier.fit(X_features, y_labels)\n",
    "\n",
    "y_labels_pred = classifier.predict(X_features)\n",
    "cm = confusion_matrix(y_labels, y_labels_pred, labels=author_names)\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "# Read the confusion matrix as\n",
    "# left-hand labels = the \"true\" author\n",
    "# bottom-row labels = the author predicted by the classifier\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, cbar=False, xticklabels=author_surnames, yticklabels=author_surnames, cmap=\"Blues\", fmt='d')\n",
    "b, t = plt.ylim()\n",
    "b += 0.5\n",
    "t -= 0.5\n",
    "plt.ylim(b, t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or8waiF7x_He"
   },
   "outputs": [],
   "source": [
    "# This code displays a few of the features (words) that the classifier\n",
    "# found most helpful when making its classification decisions. These are\n",
    "# indeed very common \"function\" words (the, of, to, that, and, in, is, for)\n",
    "\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "def get_feature_counts(dtm, labels, categories, term, vocab):\n",
    "  category_counts = {}\n",
    "  for category in categories:\n",
    "    category_counts[category] = 0\n",
    "    for i, label in enumerate(labels):\n",
    "      if label == category:\n",
    "        vocab_position = np.where(vocab == term)[0][0]\n",
    "        category_counts[category] += dtm[i, vocab_position]\n",
    "  return category_counts\n",
    "\n",
    "def most_informative_features(classifier, vectorizer, categories, n=20):\n",
    "    class_labels = classifier.classes_\n",
    "    if vectorizer is None:\n",
    "        feature_names = classifier.steps[0].get_feature_names()\n",
    "    else:\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_log_prob_[0], feature_names))[-n:]\n",
    "    topn_class2 = sorted(zip(classifier.feature_log_prob_[1], feature_names))[-n:]\n",
    "    for prob, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], prob, feat)\n",
    "        print(str(get_feature_counts(X_features, y_labels, categories, feat, vocab)))\n",
    "    print()\n",
    "    for prob, feat in reversed(topn_class1):\n",
    "        print(class_labels[0], prob, feat)\n",
    "        print(str(get_feature_counts(X_features, y_labels, categories, feat, vocab)))\n",
    "\n",
    "most_informative_features(classifier, vectorizer, author_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiXDcIM9x_Hf"
   },
   "source": [
    "Here we'll compute and visualize a few more style-related aspects of the texts associated with each majority author.\n",
    "\n",
    "The \"entropy\" of the words used by an author in a given document, which roughly corresponds to the \"unpredictability\" or \"variety\" in an author's word choices, has proven to be effective in differentiating authors stylistically in the absence of any other identifying data. This seems to be the case here as well, at least for some of the justices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaqYhsITx_Hf"
   },
   "outputs": [],
   "source": [
    "author_function_entropies = []\n",
    "author_alpha_entropies = []\n",
    "author_opinion_lengths = []\n",
    "\n",
    "for i, author_name in enumerate(author_opinions):\n",
    "    author_function_entropies.append([])\n",
    "    author_alpha_entropies.append([])\n",
    "    author_opinion_lengths.append([])\n",
    "    for doc_id in author_opinions[author_name]:\n",
    "        doc = opinions_corpus[doc_id]\n",
    "        sents, alpha_tokens, semantic_tokens, function_tokens = get_doc_tokens_and_sents(doc)\n",
    "        function_entropy = textacy.text_stats.basics.entropy(function_tokens)\n",
    "        alpha_entropy = textacy.text_stats.basics.entropy(alpha_tokens)\n",
    "        opinion_length = len(alpha_tokens)\n",
    "\n",
    "        author_function_entropies[i].append(function_entropy)\n",
    "        author_alpha_entropies[i].append(alpha_entropy)\n",
    "        author_opinion_lengths[i].append(opinion_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1wOTm3Lx_Hf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "bplots = {\"Function Word Entropies\": author_function_entropies,\n",
    "          \"Semantic + Function Word Entropies\": author_alpha_entropies,\n",
    "          \"Opinion Lengths\": author_opinion_lengths}\n",
    "\n",
    "for bplot in bplots:\n",
    "    fig = plt.figure(figsize=(12,6), dpi=72)\n",
    "    fig.gca().set_title(bplot)\n",
    "    plt.boxplot(bplots[bplot], labels=author_surnames, notch=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS3JuVG6awew"
   },
   "source": [
    "## Additional Topics and Resources\n",
    "\n",
    "**Sentiment analysis**: This workshop has touched on most of the main features of spaCy and textacy. One popular type of text analysis not covered is sentiment analysis, which neither spaCy nor textacy provides as a built-in feature (yet), and which anyway is not directly applicable to the historical and legal corpora used in this workshop. Among the available Python packages, [TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html) provides an especially straightforward implementation of sentiment analysis, albeit only for English texts.\n",
    "\n",
    "**State of the field**: Text analysis is one of the foundational areas of inquiry for computational scholarship in the humanities and social sciences. Research continues to progress at a rapid pace, particularly involving deep-learning approaches to language translation, comprehension and generation. These are now quite likely to involve the productive exchange of methods with research into computational analysis of images, audio and video. Surveying the other software packages listed at the beginning of this workshop is a good way to stay up to date on what is currently available.\n",
    "\n",
    "**Ethical concerns**: Although this workshop covers fairly basic topics in text analysis, the more advanced methods discussed here already begin to engage with issues of bias, privacy, and automation related to computational models and AI. The research reports and essays at https://ainowinstitute.org/research.html provide illuminating perspectives on these topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QW2MkH8h-ZEV"
   },
   "source": [
    "Here's the link to the evaluation survey again:\n",
    "- https://evaluations.cidr.link/Text_Analysis_with_Python/\n",
    "\n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Text_Analysis_with_Python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "206px",
    "width": "555px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
