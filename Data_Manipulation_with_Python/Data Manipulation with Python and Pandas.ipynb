{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "CIDR :: Data Manipulation with Python and Pandas",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqYGliuBcGsj"
      },
      "source": [
        "<center>\n",
        "  <h1>Digital Tools and Methods for the Humanities and Social Sciences</h1>\n",
        "  <img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/cidr-logo.no-text.240x140.png\" alt=\"Center for Interdisciplinary Digital Research @ Stanford\"/>\n",
        "</center>\n",
        "\n",
        "# Data Manipulation with Python and Pandas\n",
        "\n",
        "### Instructors\n",
        "- Simon Wiles (CIDR), <em>simon.wiles@stanford.edu</em>\n",
        "- Peter Broadwell (CIDR), <em>broadwell@stanford.edu</em>\n",
        "\n",
        "### Sign In\n",
        "Please sign in for this workshop at: https://signin.cidr.link/Python_and_Pandas/ -- when you've submitted the sign-in form, please keep your browser tab open on the evaluation form as a reminder to complete it when the workshop is over.\n",
        "\n",
        "\n",
        "### Goal\n",
        "By the end of this workshop, we hope you'll be able to load in data into a Pandas `DataFrame`, perform basic cleaning and analysis, and create visualizations of some relevant aspects of a dataset.  For most of this workshop we will work with a dataset prepared from the [IMDb Datasets](https://www.imdb.com/interfaces/) and the [OMDb API](https://www.omdbapi.com/).\n",
        "\n",
        "### Topics\n",
        "- What is Pandas?\n",
        "  - What does Pandas do?\n",
        "  - Where can I get more help with Pandas?\n",
        "- Introduction to `DataFrame`s and `Series`\n",
        "- Creating `DataFrame`s and loading data\n",
        "  - Creating `DataFrame`s from data\n",
        "  - Reading data from persistent storage\n",
        "  - Writing `DataFrames` back out to persistent storage\n",
        "- Working with `DataFrames`\n",
        "  - Exploring `DataFrames`\n",
        "  - Slicing and sub-setting\n",
        "  - Operations, filtering, and assignment\n",
        "  - Cleaning and manipulating `DataFrames`\n",
        "  - Aggregating data\n",
        "- Visualization\n",
        "\n",
        "### Evaluation survey\n",
        "At the end of the workshop, we would be very grateful if you can, please, spend 1 minute answering a few questions that will help us to continue our workshop series.\n",
        "- https://evaluations.cidr.link/Python_and_Pandas/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb-bvT4Trg2Y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh2k4ismreBP"
      },
      "source": [
        "### Jupyter Notebooks and Google Colaboratory\n",
        "\n",
        "Jupyter notebooks are a way to write and run Python code in an interactive way. They're quickly becoming a standard way of putting together data, code, and written explanations or visualizations into a single document and sharing that. There are a lot of ways that you can run Jupyter notebooks, including just locally on your computer, but we've decided to use Google's Colaboratory notebook platform for this workshop.  Colaboratory is “a Google research project created to help disseminate machine learning education and research.”  If you would like to know more about Colaboratory in general, you can visit the [Welcome Notebook](https://colab.research.google.com/notebooks/welcome.ipynb).\n",
        "\n",
        "Using the Google Colaboratory platform allows us to focus on learning and writing Python in the workshop rather than on setting up Python, which can sometimes take a bit of extra work depending on platforms, operating systems, and other installed applications. If you'd like to install a Python distribution locally, though, we have some instructions (with gifs!) on installing Python through the Anaconda distribution, which will also help you handle virtual environments: https://github.com/sul-cidr/Workshops/wiki/Installing-and-Configuring-Anaconda-and-Jupyter-Notebooks\n",
        "\n",
        "If you run into problems, or would like to look into other ways of installing Python or handling virtual environments, feel free to send us an email (contact-cidr@stanford.edu).\n",
        "\n",
        "### Environment\n",
        "If you would prefer to use Anaconda or your own local installation of python or Jupyter Notebooks, for this workshop you will need an environment with the following packages installed and available:\n",
        "- `pandas`\n",
        "- `matplotlib`\n",
        "- `sqlalchemy`\n",
        "\n",
        "Please note that we will likely not have time during the workshop to support you with problems related to a local environment, and we do recommend using the Colaboratory notebooks if you are at all unsure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu5EV600rifp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWnfLObcGsl"
      },
      "source": [
        "## 1. What is Pandas?\n",
        "\n",
        "Pandas is a high-level data manipulation tool first created in 2008 by Wes McKinney.  The name is derived from the term “panel data,” an econometrics term for data sets that include observations over multiple time periods for the same individuals.<sup>[[wikipedia](https://en.wikipedia.org/wiki/Pandas_(software))]</sup>\n",
        "\n",
        "From Jake Vanderplas’ book [**Python Data Science Handbook**](http://shop.oreilly.com/product/0636920034919.do):\n",
        "\n",
        "> Pandas is a newer package built on top of NumPy, and provides an efficient implementation of a `DataFrame`. `DataFrame`s are essentially multidimensional arrays with attached row and column labels, and often with heterogeneous types and/or missing data. As well as offering a convenient storage interface for labeled data, Pandas implements a number of powerful data operations familiar to users of both database frameworks and spreadsheet programs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUbC3sd8cGsm"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# The two lines below configure how our outputs are shown in this notebook\n",
        "#  environment.  They need not concern us now.\n",
        "pd.set_option('display.max_rows', 20)\n",
        "pd.DataFrame._repr_html_ = \\\n",
        "    lambda self: ('<style>table.dataframe td {white-space: nowrap}</style>' +\n",
        "                  self.to_html(max_rows=10, show_dimensions=True, notebook=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uinxl7e-cGsw"
      },
      "source": [
        "### 1.1. What does Pandas *do*?\n",
        "\n",
        "* Reading and writing data from persistent storage\n",
        "* Cleaning, filtering, and otherwise preparing data\n",
        "* Calculating statistics and analyzing data\n",
        "* Visualization with help from Matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZmumCyKcGsy"
      },
      "source": [
        "... but perhaps we should let Pandas introduce itself:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sam7CHBncGsz"
      },
      "source": [
        "pd?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvt7-ClTcGs4"
      },
      "source": [
        "### 1.2. Where can I get more help with Pandas?\n",
        "\n",
        "The [Pandas website](https://pandas.pydata.org/) and [online documentation](http://pandas.pydata.org/pandas-docs/stable/) are useful resources, and of course the indispensible [Stack Overflow has a \"pandas\" tag](https://stackoverflow.com/questions/tagged/pandas).  There is also a (much younger, much smaller) [sister site dedicated to Data Science questions that has a \"pandas\" tag](https://datascience.stackexchange.com/questions/tagged/pandas) too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcn3nbaocGs_"
      },
      "source": [
        "pd.isnull?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCDLHdEmcGtE"
      },
      "source": [
        "## 2. Introduction to `DataFrame`  s and `Series`\n",
        "\n",
        "The main data structure that Pandas implements is the `DataFrame`, and a `DataFrame` is composed of one or more `Series` and, optionally, an `Index`.  \n",
        "\n",
        "A `DataFrame` is a two-dimensional array with flexible row indices and flexible column names. It can be thought of as a generalization of a two-dimensional NumPy array, or a specialization of a dictionary in which each column name maps to a `Series` of column data.\n",
        "\n",
        "A `Series` is a one-dimensional array of indexed data. It can be thought of as a specialized dictionary or a generalized NumPy array.\n",
        "\n",
        "A `DataFrame` is made up of `Series` in a similar way in which a table is made up of columns. The only restriction is that each column must be of the same data type.  Many of the operations that can be performed on a `DataFrame` can also be performed on an individual `Series`.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/Data_Manipulation_with_Python/assets/dataframes.png\" alt=\"DataFrames are composed of Series\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDBoQ3NfcGtF"
      },
      "source": [
        "## 3. Creating `DataFrame`s and loading data\n",
        "\n",
        "There are a great many ways to create a Pandas `DataFrame` -- we can build one ourselves in lower-level Python datatypes, of course, but Pandas also provides methods to load data in from common storage and serialization formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePCXZqi_cGtG"
      },
      "source": [
        "<a title=\"PerryPlanet [Public domain], via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Bayarea_map.svg\" style=\"float:right\"><img width=\"256\" alt=\"Bayarea map\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Bayarea_map.svg/512px-Bayarea_map.svg.png\"></a>\n",
        "### 3.1. Creating `DataFrame`s from data\n",
        "\n",
        "The simplest way to generate a `DataFrame` is to create it directly from a `dict` of `list`s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7leHVnHPcGtG"
      },
      "source": [
        "data = {\n",
        "    \"county\": [\"Alameda\", \"Contra Costa\", \"Marin\", \"Napa\", \"San Francisco\", \"San Mateo\", \"Santa Clara\", \"Solano\", \"Sonoma\"],\n",
        "    \"county seat\": [\"Oakland\", \"Martinez\", \"San Rafael\", \"Napa\", \"San Francisco\", \"Redwood City\", \"San Jose\", \"Fairfield\", \"Santa Rosa\"],\n",
        "    \"population\": [1494876, 1037817, 250666, 135377, 870887, 711622, 1762754, 411620, 478551],\n",
        "    \"area\": [2130, 2080, 2140, 2040, 600.59, 1930, 3380, 2350, 4580]\n",
        "}\n",
        "bay_area_counties = pd.DataFrame(data)\n",
        "bay_area_counties"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLGmKhD6cGtL"
      },
      "source": [
        "Pandas has automatically created an `Index` on this `DataFrame` ([0..8]), but we can also specify our own `Index` when we instantiate the frame ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fJ25eqqcGtM"
      },
      "source": [
        "bay_area_counties = pd.DataFrame(data, index=[\"Ala\", \"Con\", \"Mar\", \"Nan\", \"SF\", \"SM\", \"SC\", \"Sol\", \"Son\"])\n",
        "bay_area_counties"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0MPN5CrcGtR"
      },
      "source": [
        "This allows us to `loc`ate a specific reference using the key in the `Index`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU7nnpqecGtR"
      },
      "source": [
        "bay_area_counties.loc['Ala']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lxV7XZecGtV"
      },
      "source": [
        "We can also set an `Index` at any time after the `DataFrame` has been created, either by adding a new index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9ZKxrOWcGtW"
      },
      "source": [
        "bay_area_counties = pd.DataFrame(data)\n",
        "bay_area_counties.index = [\"Ala\", \"Con\", \"Mar\", \"Nan\", \"SF\", \"SM\", \"SC\", \"Sol\", \"Son\"]\n",
        "bay_area_counties"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEegakbwcGtZ"
      },
      "source": [
        "or by choosing one of the existing columns to become the index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ga7unhmcGtb"
      },
      "source": [
        "bay_area_counties = pd.DataFrame(data)\n",
        "# note the use of `inplace=True`\n",
        "bay_area_counties.set_index('county', inplace=True)\n",
        "bay_area_counties"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zA4HHkNcGte"
      },
      "source": [
        "bay_area_counties.loc['Santa Clara']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPhDyBuEcGti"
      },
      "source": [
        "### 3.2. Reading data from persistent storage\n",
        "\n",
        "However, most of the time we're more likely to be reading data in from an external source of some kind, and Pandas has us well covered here.\n",
        "\n",
        "First, let's grab some data into our Colaboratory Notebook environment so that we can work with it locally:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WGKrRy_cGtj"
      },
      "source": [
        "!mkdir -p workshop_data\n",
        "!wget https://raw.githubusercontent.com/sul-cidr/Workshops/master/Data_Manipulation_with_Python/sample_data/imdb_top_1000.csv -O workshop_data/imdb_top_1000.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2ZpverXcGtn"
      },
      "source": [
        "#### 3.2.1. CSV files\n",
        "Reading in data from CSV files is as simple as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaxqj2Y6cGto"
      },
      "source": [
        "df = pd.read_csv('workshop_data/imdb_top_1000.csv')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfWAK2eZcGtr"
      },
      "source": [
        "Notice again that Pandas has created a default `Index` for this `DataFrame` -- we probably want the `imdbID` column to be the `Index`, and we can set that after import, as we did above with the `bay_area_counties` data, or we can specify it when loading the CSV initially:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lscnw_uLcGtr"
      },
      "source": [
        "df = pd.read_csv('workshop_data/imdb_top_1000.csv', index_col='imdbID')\n",
        "df.head(4) # the `head` method defaults to five if called without an argument\n",
        "           # a `tail` method is also available with the same semantics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv7_7HmhcGtv"
      },
      "source": [
        "#### 3.2.2. Reading data from JSON Files\n",
        "\n",
        "JSON files can be loaded in a similarly straightforward way.\n",
        "\n",
        "There are two things to note here:\n",
        "\n",
        "1. the nature of JSON as a file format is such that the `Index` is explicit, and Pandas will set it correctly for us initially.\n",
        "2. We're loading the data directly over HTTP(S) here -- Pandas `read_...` methods can accept a local file path or a URL, and Pandas will take care of fetching the data for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOqP0h3fcGtv"
      },
      "source": [
        "pd.read_json('https://raw.githubusercontent.com/sul-cidr/Workshops/master/Data_Manipulation_with_Python/sample_data/imdb_top_1000.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nb8XBWLcGty"
      },
      "source": [
        "#### 3.2.3. Reading data via a SQL query\n",
        "\n",
        "We can also load data from relational databases or other datastores that export a SQL-compatible interface.  For this example we'll download a simple SQLite database file to operate on, but Pandas’ `read_sql*` methods can accept a `connection` object that is predicated on a remote database server if required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws6dj57rcGtz"
      },
      "source": [
        "!pip install sqlalchemy\n",
        "!mkdir -p workshop_data\n",
        "!wget https://raw.githubusercontent.com/sul-cidr/Workshops/master/Data_Manipulation_with_Python/sample_data/imdb_top_1000.sqlite -O workshop_data/imdb_top_1000.sqlite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIsuo5QecGt1"
      },
      "source": [
        "from sqlalchemy import create_engine\n",
        "engine = create_engine(\"sqlite:///workshop_data/imdb_top_1000.sqlite\", echo=False)\n",
        "pd.read_sql_query(\"SELECT * FROM imdb_top_1000;\", con=engine, index_col='imdbID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OMQtRP-cGt4"
      },
      "source": [
        "#### 3.2.3. Other input formats\n",
        "\n",
        "Pandas also has methods that allow it to read data directly from other formats, including those used by Microsoft Excel, Stata, SAS, and Google Big Query.  More details are available from the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-e3_x0icGt5"
      },
      "source": [
        "### 3.3. Writing `DataFrames` back out to persistent storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFqPRBMwcGt5"
      },
      "source": [
        "Pandas makes writing data to persistent storage formats similarly convenient.  Methods are available to write to most of the formats Pandas can read, including all those demonstrated above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ONMjCt9cGt6"
      },
      "source": [
        "df.to_csv('workshop_data/imdb_data_2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXMPQHLlcGt9"
      },
      "source": [
        "## 4. Working with `DataFrame`s\n",
        "\n",
        "Let's begin by loading our dataset of the top 1,000 ranked films on imDB.\n",
        "\n",
        "In the same way that it's common to `import pandas as pd`, it's common to use `df` as an identifier for generic `DataFrame`s, especially in tutorials and demos.  For anything other than interactive sessions or throw-away scripts, however, we strongly recommend using good descriptive identifiers for your `DataFrame`s!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DluSJFzncGt-"
      },
      "source": [
        "df = pd.read_csv('workshop_data/imdb_top_1000.csv', index_col='imdbID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoIUbRS7cGuC"
      },
      "source": [
        "### 4.1. Exploring `DataFrame`s\n",
        "\n",
        "Pandas provides a host of ways to explore our data, which is especially useful when we're getting to know a new dataset or source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZToBG0-cGuC"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37kEDG3mcGuF"
      },
      "source": [
        "df.at['tt0111161', 'Title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LabIUXaIcGuI"
      },
      "source": [
        "df.iat[0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hfSMq3kcGuL"
      },
      "source": [
        "Accessing columns can be done using the dot notation, `df.column_name`, or the dictionary notation, `df['column_name']`.  This returns a `Series` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOMKwfLvcGuM"
      },
      "source": [
        "df.Title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm6YL7njcGuO"
      },
      "source": [
        "type(df.Title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIo_2NvJcGuR"
      },
      "source": [
        "Pandas allows us to very easily explore aspects of the dataset with a host of information methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRTcjc1AcGuS"
      },
      "source": [
        "df.Rated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig0aO6H8cGuU"
      },
      "source": [
        "df.Rated.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPBcnCmbcGuX"
      },
      "source": [
        "df.Rated.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDy41eMxcGua"
      },
      "source": [
        "For continuous variables, especially, we can quickly get a good handle on the distribution of the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxl7_Vm5cGub"
      },
      "source": [
        "df.imdbVotes.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGGOwiuThGpU"
      },
      "source": [
        "df.imdbVotes.describe().apply(\"{:.1f}\".format)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YSwq3FmcGue"
      },
      "source": [
        "df[['Title', 'Director']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NeKNoQqcGuf"
      },
      "source": [
        "type(df[['Title', 'Director']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzQDbWS-cGui"
      },
      "source": [
        "`Series` object and other subsets of `DataFrame`s preserve the indices of the `DataFrame` from which they are derived, which makes further operations such as merging or columns manipulation possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIJoKtqGcGui"
      },
      "source": [
        "`DataFrame`s are designed to operate at the column level, not at the row level. However, a subset of rows can be returned using the same slice notation that will be familiar from regular Python lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV7l6pVrcGuj"
      },
      "source": [
        "df[10:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9nuxlwWcGul"
      },
      "source": [
        "We can chain these operations together, as long as we remember what we are returning in each link of the chain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAxDrAijcGum"
      },
      "source": [
        "df[10:15].Production"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd-CujnUcGup"
      },
      "source": [
        "df.Production[10:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cd365J_cGuq"
      },
      "source": [
        "df[['Production']][10:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUoVE-DMcGus"
      },
      "source": [
        "We briefly saw the `.loc` attribute above which allows selection by “label” -- that is, by value(s) in the `Index`.  `DataFrame`s also expose the `.iloc` attribute, which is for selection by (integer-based) position:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs9GzNQicGut"
      },
      "source": [
        "# .reset_index() returns a copy of the DataFrame with the default Index of simple ordinals.\n",
        "df[[\"Title\", \"Plot\"]].reset_index().iloc[2:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_G1FeEMcGuu"
      },
      "source": [
        "df[[\"Title\", \"Plot\"]].loc['tt7286456':'tt0071562']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1xtS8KmcGuw"
      },
      "source": [
        "Notice how in the `.iloc` example, record #5 is omitted, in usual python slice fashion, but in the `.loc` example, the observation with `imdbID == tt0071562` is *included* in the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCPy6uz5cGuw"
      },
      "source": [
        "#### 4.2.1 Copying and sorting `DataFrame`s\n",
        "\n",
        "`DataFrame`s provide the `.sort_values()` method to allow sorting on multiple columns.  Typically we want to sort our data temporarily at time of output -- either as we’re displaying or saving it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j53_BfcrcGux"
      },
      "source": [
        "df.sort_values('Year', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkwXahSacGuz"
      },
      "source": [
        "However, sometimes we actually want to change the way in which the observations are ordered in a `DataFrame` in a way which persists, and we can do this using the `inplace=True` argument to the `.sort_values()` method.\n",
        "\n",
        "To demonstrate this, we're going to reset the index on the `DataFrame` so that observations are given a simple ordinal index.  Since we don't want to change our `DataFrame` for the following examples, we'll make a `.copy()` to operate on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-UEBeh3cGuz"
      },
      "source": [
        "df_copy = df.copy()\n",
        "df_copy.reset_index(inplace=True)\n",
        "df_copy.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNrj1OY8cGu2"
      },
      "source": [
        "df_copy.sort_values(['Year'], ascending=False, inplace=True)\n",
        "df_copy.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4G1sMuCcGu4"
      },
      "source": [
        "It is also possible to sort by multiple columns at once:\n",
        "\n",
        "(Notice how the `ascending` kwarg takes a list that applies to the list of columns to sort on in corresponding order.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPdT8nWhcGu4"
      },
      "source": [
        "df_copy.sort_values(['Year', 'Title'], ascending=[False, True]).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4rrv3JBcGu7"
      },
      "source": [
        "Notice too that since we didn't use `inplace=True` in the previous example, our `DataFrame` remains sorted as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzhLzARYcGu7"
      },
      "source": [
        "df_copy.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3EYVsLPcGu9"
      },
      "source": [
        "#### 4.2.2 Activity\n",
        "\n",
        "Given the `DataFrame` `df` defined above, write an expression to extract a `DataFrame` with the columns `Title`, `Year`, `imdbRating`, and `imdbVotes`. Show the first 5 rows in order of highest rating and then most votes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6f5KvSBcGu-"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kplTziGOqjJ0",
        "cellView": "form"
      },
      "source": [
        "#@title → Double-click Here to Show/Hide a Prepared Solution\n",
        "df[['Title','Year','imdbRating','imdbVotes']]\\\n",
        "    .sort_values(['imdbRating', 'imdbVotes'], ascending=[False, False])\\\n",
        "    .head(5)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfZIHBjqcGvA"
      },
      "source": [
        "### 4.3. Operations, filtering, and assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWy37YQLcGvA"
      },
      "source": [
        "Operations performed on a column, or `Series`, are broadcast to each of the elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCrzI4uWcGvA"
      },
      "source": [
        "# convert the running times to hours\n",
        "df.Runtime / 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4yA2EbucGvC"
      },
      "source": [
        "Simple string concatenation can be performed in the same manner:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIZk5uXXcGvD"
      },
      "source": [
        "df.Title + '(' + df.Rated + '), directed by: ' + df.Director"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAH-220VcGvF"
      },
      "source": [
        "as can boolean operations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUQwnrEzcGvG"
      },
      "source": [
        "df.Year < 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXD8bQuJcGvI"
      },
      "source": [
        "By itself this is not terribly useful, but expressions of this kind are very powerful when passed to a `DataFrame` to select content:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM7Fr4cEcGvI"
      },
      "source": [
        "df[df.Production == 'Paramount Pictures']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTDsq3VHcGvK"
      },
      "source": [
        "df[df.Year < 2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PVnp1f3cGvN"
      },
      "source": [
        "Any expression that evaluates to a `Series` of boolean values (`True` or `False`) and shares the same `Index` as the source `DataFrame` can be used.  Complex conditions can be assembled using bitwise logical operators `&`, `|`, and `~` to create simple but powerful filters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sp-z1x_cGvN"
      },
      "source": [
        "# returns a `Series`\n",
        "df[(df.Year < 2000) & (df.imdbRating > 8)].Title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWxDETVocGvP"
      },
      "source": [
        "# returns a `DataFrame`\n",
        "df.loc[(df.Year < 2000) & (df.imdbRating > 8), ['Title', 'Year', 'imdbRating']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcjzobj3cGvR"
      },
      "source": [
        "The `.str` property gives access to string variables in a broadcast fashion, such that they can be manipulated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bFbBu1lcGvR"
      },
      "source": [
        "df.Actors.str.split(', ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CbomnSKcGvT"
      },
      "source": [
        "We can make use of `.str` in expressions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQQy84aZcGvT"
      },
      "source": [
        "# select records with Oscar nominations or wins\n",
        "df[df.Awards.str.contains('Oscar') == True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP5uARVNcGvW"
      },
      "source": [
        "df[df.Director.apply(lambda a: len(a.split(', ')) > 3)][['Title', 'Director']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNOFpTcScGvY"
      },
      "source": [
        "#### 4.3.1. Assignment to `DataFrame`s\n",
        "\n",
        "It is also possible to assign values directly to columns or cells in a data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTzm7bSXcGvZ"
      },
      "source": [
        "df_copy = df.copy()\n",
        "\n",
        "# create a new column based on an existing one\n",
        "df_copy['Runtime (hrs)'] = df_copy.Runtime / 60\n",
        "\n",
        "# edit a single value\n",
        "df_copy.at['tt0111161', 'Title'] = 'Rita Hayworth and Shawshank Redemption'\n",
        "\n",
        "df_copy[['Title', 'Runtime', 'Runtime (hrs)']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM-oIqZlcGva"
      },
      "source": [
        "#### 4.3.2. Activity\n",
        "\n",
        "Returning to our “Bay Area Counties” data from earlier, write an expression to calculate the population density of each county, and assign it to a new column on the `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xVoYuX7cGva"
      },
      "source": [
        "data = {\n",
        "    \"county\": [\"Alameda\", \"Contra Costa\", \"Marin\", \"Napa\", \"San Francisco\", \"San Mateo\", \"Santa Clara\", \"Solano\", \"Sonoma\"],\n",
        "    \"county seat\": [\"Oakland\", \"Martinez\", \"San Rafael\", \"Napa\", \"San Francisco\", \"Redwood City\", \"San Jose\", \"Fairfield\", \"Santa Rosa\"],\n",
        "    \"population\": [1494876, 1037817, 250666, 135377, 870887, 711622, 1762754, 411620, 478551],\n",
        "    \"area\": [2130, 2080, 2140, 2040, 600.59, 1930, 3380, 2350, 4580]  # data is in km²\n",
        "}\n",
        "bay_area_counties = pd.DataFrame(data)\n",
        "bay_area_counties"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InGylsebcGvm"
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIUSkOZ8wn9L",
        "cellView": "form"
      },
      "source": [
        "#@title → Double-click Here to Show/Hide a Prepared Solution\n",
        "bay_area_counties['pop. density'] = bay_area_counties.population / bay_area_counties.area"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tQCmZCHcGvn"
      },
      "source": [
        "### 4.4. Cleaning and manipulating `DataFrame`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OJ1PFdocGvn"
      },
      "source": [
        "#### 4.4.1. Converting data types\n",
        "\n",
        "Pandas does a pretty good job of inferring data types when we load data into a `DataFrame`, but sometimes we want or need to change the types it selects.  We can do this using the `.astype()` method on a `Series` object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L-1cWrOcGvo"
      },
      "source": [
        "Those coming from an `R` or a statistical background may be used to working with categorical data, and we can force pandas to treat a column as categorical using `.astype('category')`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGy0uPR5cGvo"
      },
      "source": [
        "df['Rated'] = df['Rated'].astype('category')\n",
        "df['Rated']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zn4NH7icGvq"
      },
      "source": [
        "Using `.astype('category')` invokes the default behavior, by which categories are inferred from the data and are unordered.  The only real advantage to using categories is if they are ordered, so let's quickly convert them to ordered categories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQPH-jKFcGvq"
      },
      "source": [
        "from pandas.api.types import CategoricalDtype\n",
        "df['Rated'] = df['Rated'].astype(CategoricalDtype(ordered=True))\n",
        "df['Rated']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRNxgcRrcGvr"
      },
      "source": [
        "A common need is to parse dates -- if loaded from, e.g., a CSV file, they will often be interpreted as strings, as is the case with our “Released” column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkNkAE8vcGvs"
      },
      "source": [
        "df['Released'].astype('datetime64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VtTWSxtcGvt"
      },
      "source": [
        "pd.to_datetime(df['Released'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL7yQnHOcGvw"
      },
      "source": [
        "df['Released'].apply(pd.to_datetime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsDYG5c8x4X3"
      },
      "source": [
        "df[['Released']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJABoB_pcGvy"
      },
      "source": [
        "#### 4.4.2. `.apply()`\n",
        "\n",
        "The `apply()` method has appeared a couple of times above. This is the fundamental way of manipulating the contents of `DataFrame`s.  `apply()` takes a function as an argument, and `apply`s the function to each element in the container it’s called on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTLriCkxcGvy"
      },
      "source": [
        "def count_genres_naive(text):\n",
        "    genres = text.split(\", \")\n",
        "    return len(genres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h0tw2HtcGv0"
      },
      "source": [
        "df.Genre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTbVBZSPcGv4"
      },
      "source": [
        "# Note: This won't work...\n",
        "df.Genre.apply(count_genres_naive)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBkp42_WcGv6"
      },
      "source": [
        "For the purposes of this workshop, the version of the dataset we are currently working with has had some data deliberately removed, and unfortunately our `count_genres_naive` function does not know how to handle missing data.\n",
        "\n",
        "Pandas uses the `NaN` datatype from the underlying `numpy` package to represent missing data, and this datatype is based on the primitive `float` type, which is why the `Attribute` error reads as it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQkJ8H-xcGv6"
      },
      "source": [
        "df[pd.isna(df.Genre)][['Title', 'Genre']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzJko0g6cGv8"
      },
      "source": [
        "We could handle this problem in a number of ways:\n",
        "1. we could drop the observations with missing values immediately (and temporarily) before we apply our count function;\n",
        "2. we could modify our `count_genres_naive` function to handle the missing values appropriately;\n",
        "3. we could fix the data manually by assigning values where they are missing; or\n",
        "4. we could clean the dataset when we initially import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YbGw7YjcGv8"
      },
      "source": [
        "print(len(df.Genre),len(df.Genre.dropna()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0FRrZtScGv9"
      },
      "source": [
        "df.Genre.dropna().apply(count_genres_naive)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9C7h36PcGv_"
      },
      "source": [
        "One possible problem with this approach is that we don't get a count for the observations with missing values.  This means that if we want to enrich our original `DataFrame` with a new column containing the genre count, our count column will also be populated with `NaN`s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdegQyuGcGv_"
      },
      "source": [
        "df['Genre Count'] = df.Genre.dropna().apply(count_genres_naive)\n",
        "df[pd.isna(df.Genre)][['Genre', 'Genre Count']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6-QNKG8cGwB"
      },
      "source": [
        "In this case, we might conclude that since the data is missing, the appropriate value for “genre count” is `0`, and we can write a less naïve operator function that handles this for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGz-cGvicGwB"
      },
      "source": [
        "def count_genres(text):\n",
        "    if pd.isna(text):\n",
        "        return 0\n",
        "    genres = text.split(\", \")\n",
        "    return len(genres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oo3Rif5cGwC"
      },
      "source": [
        "df['Genre Count'] = df.Genre.apply(count_genres)\n",
        "df[pd.isna(df.Genre)][['Genre', 'Genre Count']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq8vve5QcGwE"
      },
      "source": [
        "For the sake of our workshop, however, the dataset has been modified so that `Genre` has been removed where it should be the single value “Comedy”.  Since we know, therefore, what the value should be for all observations where `Genre` is missing, we have the option to simply update the `DataFrame` directly in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld5gdjgwcGwE"
      },
      "source": [
        "df.loc[pd.isna(df.Genre), 'Genre'] = 'Comedy'\n",
        "df[pd.isna(df.Genre)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdnGN5hpcGwF"
      },
      "source": [
        "We might be done at this point, but it might also be a good idea to check if there are other missing values in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ng8_6lCcGwF"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLfaXg7cGwH"
      },
      "source": [
        "#### 4.4.3. Manipulating / cleaning on import\n",
        "\n",
        "With this in mind, then, and especially when creating re-usable scripts for repetitive tasks or processes we want to iterate over, a common ‘best practice’ is to handle many data transformations when the `DataFrame` is first populated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY2enTRdcGwH"
      },
      "source": [
        "from pandas.api.types import CategoricalDtype\n",
        "\n",
        "column_types = {\n",
        "    'Rated': CategoricalDtype(ordered=True)\n",
        "}\n",
        "\n",
        "fill_nans = {\n",
        "    'Genre': 'Comedy',\n",
        "    'imdbVotes': 0,\n",
        "    'Rated': 'Unrated',\n",
        "    'Awards': '',\n",
        "    'Metascore': '',\n",
        "    'BoxOffice': '',\n",
        "    'Production': '',\n",
        "    'RottenTomatoes': ''\n",
        "}\n",
        "\n",
        "date_columns = ['Released']\n",
        "\n",
        "df = pd.read_csv(\n",
        "    'workshop_data/imdb_top_1000.csv',\n",
        "    parse_dates=date_columns,\n",
        "    index_col='imdbID',\n",
        "    dtype=column_types\n",
        ")\n",
        "\n",
        "df.fillna(value=fill_nans, inplace=True)\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8kC_bfEcGwJ"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytPu39I-cGwL"
      },
      "source": [
        "#### 4.4.4. Activity\n",
        "\n",
        "Add a new column `language_count` to the `DataFrame` and show movies with more than 1 language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyt_pXFncGwL"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGFLAIZsgZz4",
        "cellView": "form"
      },
      "source": [
        "#@title → Double-click Here to Show/Hide Hints\n",
        "\n",
        "# 1. You'll probably want to write a helper\n",
        "#    function to count the languages\n",
        "\n",
        "# 2. Create a new column to the DataFrame by \n",
        "#    apply()-ing the helper function, and add\n",
        "#    it to the dataframe\n",
        "\n",
        "# 3. Filter the dataframe based on the contents\n",
        "#    of the new column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VazjC0TV1fch",
        "cellView": "form"
      },
      "source": [
        "#@title → Double-click Here to Show/Hide a Prepared Solution\n",
        "\n",
        "def count_languages(langs):\n",
        "    return len(langs.split(', '))\n",
        "    \n",
        "df['language_count'] = df.Language.apply(count_languages)\n",
        "df[df.language_count > 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxObOc48cGwM"
      },
      "source": [
        "### 4.5. Aggregating data\n",
        "\n",
        "But what about the most-featured Director in the top 1000 list?  Or the average rating for movies classified as “Comedy”?  For these kinds of operations we need to compute across aggregations of the dataset.  In Pandas this may be accomplished by means of `.groupby()` operation followed by an `.aggregate()` function. Aggregates can be considered a form of `.apply()` that operates on a collection of observations at once (in these cases, on the collections created by `.groupby()`.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/Data_Manipulation_with_Python/assets/aggregation.png\" alt=\"Aggregation\">\n",
        "\n",
        "Those who are familiar with the `R` programming language, may recognize this as the “Split-Apply-Combine” approach:[<sup>*</sup>](#fn)\n",
        "- *Split* a dataset into relevant subsets;\n",
        "- *Apply* a function to each subset;\n",
        "- *Combine* all the pieces back together.\n",
        "\n",
        "<hr>\n",
        "<span id=\"fn\">* A classic paper on the Split-Apply-Combine methodology is available here: <a href=\"https://www.jstatsoft.org/article/view/v040i01/v40i01.pdf\">“The Split-Apply-Combine Strategy for Data Analysis”</a></span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSPYE9e0cGwN"
      },
      "source": [
        "df.groupby('Director')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIbzbSvgcGwQ"
      },
      "source": [
        "df.groupby('Director')[['Title']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thTfSuLWcGwR"
      },
      "source": [
        "df.groupby('Director')[['Title']].aggregate(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqxikpnLcGwS"
      },
      "source": [
        "We can then sort on the aggregated values to show the highest (and lowest) ranking Directors among our top 1,000 movies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmsDq6RecGwS"
      },
      "source": [
        "df.groupby('Director')[['Title']].aggregate(len).sort_values(['Title'], ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfpTwmZ5cGwT"
      },
      "source": [
        "If we want to aggregate across the combination of, e.g., Director and Production credit, the `.group_by()` method will accept a list of columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyZZd5GwcGwU"
      },
      "source": [
        "(df.groupby(['Director', 'Production'])[['Title']]\n",
        "   .aggregate(len)\n",
        "   .sort_values(['Title'], ascending=False)\n",
        "   .head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPAiQT-lidtB"
      },
      "source": [
        "It also is possible to perform this grouping, modification and presentation (and to generate other summaries of `DataFrame` contents) via the spreadsheet-style `pivot_table()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TyxB9AHcGwV"
      },
      "source": [
        "df.pivot_table(\n",
        "    index=['Director', 'Production'],\n",
        "    values=['Title'],\n",
        "    aggfunc=len\n",
        ").sort_values('Title', ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIZnRDb0cGwX"
      },
      "source": [
        "### 4.5.1. Activity\n",
        "\n",
        "Show the top five years for movies categorized as \"Comedy\".\n",
        "\n",
        "(Hint: don't try to parse-out the different values in the “Genre” column -- just test for the presence of the word “Comedy”, e.g. `df.Genre.str.contains('Comedy')`.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROyHzweccGwX"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt6r1emytZgo",
        "cellView": "form"
      },
      "source": [
        "#@title → Double-click Here to Show/Hide a Prepared Solution\n",
        "df[df.Genre.str.contains('Comedy')].groupby('Year')[['Title']].aggregate(len).sort_values(['Title'], ascending=False).head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOxEiD2PcGwd"
      },
      "source": [
        "## 5. Visualization\n",
        "\n",
        "Pandas also provides some utilities to create basic plots just by calling `plot()` on a `Series` or `DataFrame`. But first we need to tell Jupyter that we are going to plot some charts using the plotting library `matplotlib`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xAowRB1cGwd"
      },
      "source": [
        "# enables inline plotting in Jupyter using matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlCMe8PCcGwe"
      },
      "source": [
        "df.groupby('Year')[['Year']].aggregate(len).plot()\n",
        "df.groupby('Year')[['Year']].aggregate(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-OPITmwvL0Y"
      },
      "source": [
        "Notice that several years are missing from the aggregated DataFrame, especially in the early years of cinema. This gives us an opportunity to explore how visualizations can reveal or hide the presence of missing values in the data, and how the `interpolate()` method can be used to fill in missing time-series values, when appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUIKoT7ovPy2"
      },
      "source": [
        "# Reindexing by the full year range, step value=1, assigns NaN for the missing years\n",
        "df_gapped = df.groupby('Year')[['Year']].aggregate(len)\n",
        "df_gapped = df_gapped.reindex(range(df_gapped.index.min(),df_gapped.index.max()+1,1))\n",
        "df_gapped.plot()\n",
        "df_gapped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHJkXdTzvP_g"
      },
      "source": [
        "df_gapped.fillna(value=0).plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuA8TGPIvQFz"
      },
      "source": [
        "interp_method = \"quadratic\" # Other options: slinear, cubic, spline\n",
        "df_yearly = df_gapped.interpolate(method=interp_method)\n",
        "df_yearly.plot()\n",
        "df_yearly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B_XURftcGwf"
      },
      "source": [
        "Each time you call `plot()` an `AxesSubplot` object is returned, and these are automatically rendered by the Jupyter notebook environment. `AxesSubplot` objects are objects of the underlying `matplotlib` library for plotting in Python, and as such, lots of different options can be passed to control the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DMgqqyKcGwf"
      },
      "source": [
        "ax = df.groupby('Year')[['Year']].aggregate(len).plot(\n",
        "    kind=\"bar\",\n",
        "    figsize=(15, 5),\n",
        "    title=\"# Movies per Year\",\n",
        "    legend=None\n",
        ")\n",
        "ax.set_ylabel(\"# Movies\")\n",
        "ax.set_xlabel(\"Year of Release\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g56JRp26cGwg"
      },
      "source": [
        "plt.style.available"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SipNbPBScGwi"
      },
      "source": [
        "with plt.style.context('ggplot'):\n",
        "    df_yearly.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YooD5chfcGwj"
      },
      "source": [
        "with plt.xkcd():\n",
        "    df_yearly.plot()\n",
        "plt.rcdefaults()  # this is needed as the XKCD style is a special case"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WG6kF7mcGwl"
      },
      "source": [
        "fig, ax = plt.subplots(1, figsize=(15, 5))\n",
        "plt.ticklabel_format(style='plain')\n",
        "df['imdbVotes'].hist(ax=ax, bins=15, density=True, color='lightseagreen')\n",
        "df['imdbVotes'].plot(ax=ax, kind='kde', xlim=(0, 2200000), style='r--')\n",
        "ax.set_title('Histogram of # votes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy0nEpfjcGwn"
      },
      "source": [
        "fig, ax = plt.subplots(1, figsize=(6, 6))\n",
        "plt.ticklabel_format(style='plain')\n",
        "df.boxplot(column='imdbVotes', by='Rated', grid=False, ax=ax, sym='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRZtBuOVcGwo"
      },
      "source": [
        "import seaborn as sns\n",
        "fig, ax = plt.subplots(1, figsize=(6, 6))\n",
        "plt.ticklabel_format(style='plain')\n",
        "sns.violinplot(y=df['imdbVotes'], grid=False, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of0B5JcecGwp"
      },
      "source": [
        "fig, ax = plt.subplots(1, figsize=(15, 5))\n",
        "plt.ticklabel_format(style='plain')\n",
        "ax.scatter(x=df.Year, y=df.imdbVotes)\n",
        "ax.set_ylabel('# Votes')\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_title('# Votes by Year')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0pBZjducGwr"
      },
      "source": [
        "## Evaluation survey\n",
        "At the end of the workshop, we would be very grateful if you can, please, spend 1 minute answering a few questions that will help us to continue our workshop series.\n",
        "- https://evaluations.cidr.link/Python_and_Pandas/"
      ]
    }
  ]
}