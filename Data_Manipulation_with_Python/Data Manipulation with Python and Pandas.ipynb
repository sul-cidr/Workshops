{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqYGliuBcGsj"
   },
   "source": [
    "<center>\n",
    "  <h1>Digital Tools and Methods for the Humanities and Social Sciences</h1>\n",
    "  <img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/cidr-logo.no-text.240x140.png\" alt=\"Center for Interdisciplinary Digital Research @ Stanford\"/>\n",
    "</center>\n",
    "\n",
    "# Data Manipulation with Python and Pandas\n",
    "\n",
    "### Instructors\n",
    "- Simon Wiles (CIDR), <em>simon.wiles@stanford.edu</em>\n",
    "- Peter Broadwell (CIDR), <em>broadwell@stanford.edu</em>\n",
    "\n",
    "### Goal\n",
    "By the end of this workshop, we hope you'll be able to load in data into a Pandas `DataFrame`, perform basic cleaning and analysis, and create visualizations of some relevant aspects of a dataset. For most of this workshop we will work with a dataset prepared from the [IMDb Datasets](https://www.imdb.com/interfaces/) and the [OMDb API](https://www.omdbapi.com/).\n",
    "\n",
    "### Topics\n",
    "- What is Pandas?\n",
    "  - What does Pandas do?\n",
    "  - Where can I get more help with Pandas?\n",
    "- Introduction to `DataFrame`s and `Series`\n",
    "- Creating `DataFrame`s and loading data\n",
    "  - Creating `DataFrame`s from data\n",
    "  - Reading data from persistent storage\n",
    "  - Writing `DataFrames` back out to persistent storage\n",
    "- Working with `DataFrames`\n",
    "  - Exploring `DataFrames`\n",
    "  - Slicing and sub-setting\n",
    "  - Operations, filtering, and assignment\n",
    "  - Cleaning and manipulating `DataFrames`\n",
    "  - Aggregating data\n",
    "- Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb-bvT4Trg2Y"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gh2k4ismreBP"
   },
   "source": [
    "### Jupyter Notebooks and Google Colaboratory\n",
    "\n",
    "Jupyter notebooks are a way to write and run Python code in an interactive way. They're quickly becoming a standard way of putting together data, code, and written explanations or visualizations into a single document and sharing that. There are a lot of ways that you can run Jupyter notebooks, including just locally on your computer, but we've decided to use Google's Colaboratory notebook platform for this workshop. Colaboratory is “a Google research project created to help disseminate machine learning education and research.” If you would like to know more about Colaboratory in general, you can visit the [Welcome Notebook](https://colab.research.google.com/notebooks/welcome.ipynb).\n",
    "\n",
    "Using the Google Colaboratory platform allows us to focus on learning and writing Python in the workshop rather than on setting up Python, which can sometimes take a bit of extra work depending on platforms, operating systems, and other installed applications. If you'd like to install a Python distribution locally, though, we have some instructions (with gifs!) on installing Python through the Anaconda distribution, which will also help you handle virtual environments: https://github.com/sul-cidr/Workshops/wiki/Installing-and-Configuring-Anaconda-and-Jupyter-Notebooks\n",
    "\n",
    "If you run into problems, or would like to look into other ways of installing Python or handling virtual environments, feel free to send us an email (contact-cidr@stanford.edu).\n",
    "\n",
    "### Environment\n",
    "If you would prefer to use Anaconda or your own local installation of Python or Jupyter Notebooks, for this workshop you will need an environment with the following packages installed and available:\n",
    "- `pandas`\n",
    "- `matplotlib`\n",
    "- `sqlalchemy`\n",
    "\n",
    "Please note that we will likely not have time during the workshop to support you with problems related to a local environment, and we do recommend using the Colaboratory notebooks if you are at all unsure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu5EV600rifp"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OWnfLObcGsl"
   },
   "source": [
    "## 1. What is Pandas?\n",
    "\n",
    "Pandas is a high-level data manipulation tool first created in 2008 by Wes McKinney. The name is derived from the term “panel data,” an econometrics term for data sets that include observations over multiple time periods for the same individuals.<sup>[[wikipedia](https://en.wikipedia.org/wiki/Pandas_(software))]</sup>\n",
    "\n",
    "From Jake Vanderplas’ book [**Python Data Science Handbook**](http://shop.oreilly.com/product/0636920034919.do):\n",
    "\n",
    "> Pandas is a newer package built on top of NumPy, and provides an efficient implementation of a `DataFrame`. `DataFrame`s are essentially multidimensional arrays with attached row and column labels, and often with heterogeneous types and/or missing data. As well as offering a convenient storage interface for labeled data, Pandas implements a number of powerful data operations familiar to users of both database frameworks and spreadsheet programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUbC3sd8cGsm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Copy-on-Write should be enabled by default\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# The two lines below configure how our outputs are shown in this notebook\n",
    "# environment. They need not concern us now.\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.float_format\", \"{:,.2f}\".format)\n",
    "pd.DataFrame._repr_html_ = \\\n",
    "    lambda self: (\"<style>table.dataframe td {white-space: nowrap}</style>\" +\n",
    "                  self.to_html(max_rows=10, show_dimensions=True, notebook=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uinxl7e-cGsw"
   },
   "source": [
    "### 1.1. What does Pandas *do*?\n",
    "\n",
    "* Reading and writing data from persistent storage\n",
    "* Cleaning, filtering, and otherwise preparing data\n",
    "* Calculating statistics and analyzing data\n",
    "* Visualization with help from Matplotlib\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZmumCyKcGsy"
   },
   "source": [
    "... but perhaps we should let Pandas introduce itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sam7CHBncGsz"
   },
   "outputs": [],
   "source": [
    "pd?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gvt7-ClTcGs4"
   },
   "source": [
    "### 1.2. Where can I get more help with Pandas?\n",
    "\n",
    "The [Pandas website](https://pandas.pydata.org/) and [online documentation](http://pandas.pydata.org/pandas-docs/stable/) are useful resources, and of course the indispensible [Stack Overflow has a \"pandas\" tag](https://stackoverflow.com/questions/tagged/pandas). There is also a (much younger, much smaller) [sister site dedicated to Data Science questions that has a \"pandas\" tag](https://datascience.stackexchange.com/questions/tagged/pandas) too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xcn3nbaocGs_"
   },
   "outputs": [],
   "source": [
    "pd.isnull?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. A word about Copy-on-Write (CoW)\n",
    "\n",
    "Pandas (since v1.5) has supported Copy-on-Write mode, which simplifies the indexing API and offers improved performance in many tasks.  It will be the default setting in the soon-to-be-released Pandas 3.0, and the [Pandas development team have been recommending enabling it](https://pandas.pydata.org/docs/user_guide/copy_on_write.html) by default for some time.\n",
    "\n",
    "We enabled CoW mode above (using `pd.options.mode.copy_on_write = True`), and we recommend doing the same at the top of any script or notebook where you're importing Pandas.  Unfortunately there is a chance this will mean that code examples you find in old tutorials/answers/blog posts (or get from so-called \"AI\" coding assistants) will need updating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCDLHdEmcGtE"
   },
   "source": [
    "## 2. Introduction to `DataFrame`s and `Series`\n",
    "\n",
    "The main data structure that Pandas implements is the `DataFrame`, and a `DataFrame` is composed of one or more `Series` and, optionally, an `Index`. \n",
    "\n",
    "A `DataFrame` is a two-dimensional array with flexible row indices and flexible column names. It can be thought of as a generalization of a two-dimensional NumPy array, or a specialization of a dictionary in which each column name maps to a `Series` of column data.\n",
    "\n",
    "A `Series` is a one-dimensional array of data with an `Index`. It can be thought of as a specialized dictionary or a generalized NumPy array.\n",
    "\n",
    "A `DataFrame` is made up of `Series` which share the same `Index`, in a similar way in which a table is made up of columns. The only restriction is that each column must be of the same data type. Many of the operations that can be performed on a `DataFrame` can also be performed on an individual `Series`.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/Data_Manipulation_with_Python/assets/dataframes.png\" alt=\"DataFrames are composed of Series\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDBoQ3NfcGtF"
   },
   "source": [
    "## 3. Creating `DataFrame`s and loading data\n",
    "\n",
    "There are a great many ways to create a Pandas `DataFrame` -- we can build one ourselves in lower-level Python datatypes, of course, but Pandas also provides methods to load data in from common storage and serialization formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePCXZqi_cGtG"
   },
   "source": [
    "<a title=\"PerryPlanet [Public domain], via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Bayarea_map.svg\" style=\"float:right\"><img width=\"256\" alt=\"Bayarea map\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Bayarea_map.svg/512px-Bayarea_map.svg.png\"></a>\n",
    "### 3.1. Creating `DataFrame`s from data\n",
    "\n",
    "The simplest way to generate a `DataFrame` is to create it directly from a `dict` of `list`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7leHVnHPcGtG"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"county\": [\"Alameda\", \"Contra Costa\", \"Marin\", \"Napa\", \"San Francisco\", \"San Mateo\", \"Santa Clara\", \"Solano\", \"Sonoma\"],\n",
    "    \"county seat\": [\"Oakland\", \"Martinez\", \"San Rafael\", \"Napa\", \"San Francisco\", \"Redwood City\", \"San Jose\", \"Fairfield\", \"Santa Rosa\"],\n",
    "    \"population\": [1494876, 1037817, 250666, 135377, 870887, 711622, 1762754, 411620, 478551],\n",
    "    \"area\": [2130, 2080, 2140, 2040, 600.59, 1930, 3380, 2350, 4580]\n",
    "}\n",
    "bay_area_counties = pd.DataFrame(data)\n",
    "bay_area_counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLGmKhD6cGtL"
   },
   "source": [
    "Pandas has automatically created an `Index` on this `DataFrame` ([0..8]), but we can also specify our own `Index` when we instantiate the frame ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fJ25eqqcGtM"
   },
   "outputs": [],
   "source": [
    "bay_area_counties = pd.DataFrame(data, index=[\"Ala\", \"Con\", \"Mar\", \"Nap\", \"SF\", \"SM\", \"SC\", \"Sol\", \"Son\"])\n",
    "bay_area_counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0MPN5CrcGtR"
   },
   "source": [
    "This allows us to `loc`ate a specific reference using the key in the `Index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sU7nnpqecGtR"
   },
   "outputs": [],
   "source": [
    "bay_area_counties.loc['Ala']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lxV7XZecGtV"
   },
   "source": [
    "We can also set an `Index` at any time after the `DataFrame` has been created, either by adding a new index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9ZKxrOWcGtW"
   },
   "outputs": [],
   "source": [
    "bay_area_counties = pd.DataFrame(data)\n",
    "bay_area_counties.index = [\"Ala\", \"Con\", \"Mar\", \"Nap\", \"SF\", \"SM\", \"SC\", \"Sol\", \"Son\"]\n",
    "bay_area_counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEegakbwcGtZ"
   },
   "source": [
    "or by choosing one of the existing columns to become the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ga7unhmcGtb"
   },
   "outputs": [],
   "source": [
    "bay_area_counties = pd.DataFrame(data)\n",
    "# note the use of `inplace=True`\n",
    "bay_area_counties.set_index('county', inplace=True)\n",
    "bay_area_counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zA4HHkNcGte"
   },
   "outputs": [],
   "source": [
    "bay_area_counties.loc['Santa Clara']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPhDyBuEcGti"
   },
   "source": [
    "### 3.2. Reading data from persistent storage\n",
    "\n",
    "However, most of the time we're more likely to be reading data in from an external source of some kind, and Pandas has us well covered here.\n",
    "\n",
    "First, let's grab some data into our Colaboratory Notebook environment so that we can work with it locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WGKrRy_cGtj"
   },
   "outputs": [],
   "source": [
    "!mkdir -p workshop_data\n",
    "!wget https://raw.githubusercontent.com/sul-cidr/Workshops/master/Data_Manipulation_with_Python/sample_data/imdb_top_1000.csv -O workshop_data/imdb_top_1000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2ZpverXcGtn"
   },
   "source": [
    "#### 3.2.1. CSV files\n",
    "Reading in data from CSV files is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaxqj2Y6cGto"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('workshop_data/imdb_top_1000.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfWAK2eZcGtr"
   },
   "source": [
    "Notice again that Pandas has created a default `Index` for this `DataFrame` -- we probably want the `imdbID` column to be the `Index`, and we can set that after import, as we did above with the `bay_area_counties` data, or we can specify it when loading the CSV initially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lscnw_uLcGtr"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('workshop_data/imdb_top_1000.csv', index_col='imdbID')\n",
    "df.head()  # the `head` method defaults to five if called without an argument\n",
    "           # a `tail` method is also available with the same semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv7_7HmhcGtv"
   },
   "source": [
    "#### 3.2.2. Reading data from JSON Files\n",
    "\n",
    "JSON files can be loaded in a similarly straightforward way.\n",
    "\n",
    "There are two things to note here:\n",
    "\n",
    "1. The nature of JSON as a file format is such that the `Index` is explicit, and Pandas will set it correctly for us initially.\n",
    "2. We're loading the data directly over HTTP(S) here -- Pandas `read_...` methods can accept a local file path or a URL, and Pandas will take care of fetching the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOqP0h3fcGtv"
   },
   "outputs": [],
   "source": [
    "pd.read_json('https://raw.githubusercontent.com/sul-cidr/Workshops/master/Data_Manipulation_with_Python/sample_data/imdb_top_1000.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nb8XBWLcGty"
   },
   "source": [
    "#### 3.2.3. Reading data via a SQL query\n",
    "\n",
    "We can also load data from relational databases or other datastores that export a SQL-compatible interface. For this example we'll download a simple SQLite database file to operate on, but Pandas’ `read_sql*` methods can accept a `connection` object that is predicated on a remote database server if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ws6dj57rcGtz"
   },
   "outputs": [],
   "source": [
    "!pip install sqlalchemy\n",
    "!mkdir -p workshop_data\n",
    "!wget https://raw.githubusercontent.com/sul-cidr/Workshops/master/Data_Manipulation_with_Python/sample_data/imdb_top_1000.sqlite -O workshop_data/imdb_top_1000.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIsuo5QecGt1"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "engine = create_engine(\"sqlite:///workshop_data/imdb_top_1000.sqlite\", echo=False)\n",
    "pd.read_sql_query(text(\"SELECT * FROM imdb_top_1000;\"), con=engine.connect(), index_col='imdbID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OMQtRP-cGt4"
   },
   "source": [
    "#### 3.2.3. Other input formats\n",
    "\n",
    "Pandas also has methods that allow it to read data directly from other formats, including those used by Microsoft Excel, Stata, SAS, and Google Big Query. More details are available from the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-e3_x0icGt5"
   },
   "source": [
    "### 3.3. Writing `DataFrames` back out to persistent storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFqPRBMwcGt5"
   },
   "source": [
    "Pandas makes writing data to persistent storage formats similarly convenient. Methods are available to write to most of the formats Pandas can read, including all those demonstrated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ONMjCt9cGt6"
   },
   "outputs": [],
   "source": [
    "df.to_csv('workshop_data/imdb_data_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXMPQHLlcGt9"
   },
   "source": [
    "## 4. Working with `DataFrame`s\n",
    "\n",
    "Let's begin by loading our dataset of the top 1,000 ranked films on imDB.\n",
    "\n",
    "In the same way that it's common to `import pandas as pd`, it's common to use `df` as an identifier for generic `DataFrame`s, especially in tutorials and demos. For anything other than interactive sessions or throw-away scripts, however, we strongly recommend using good descriptive identifiers for your `DataFrame`s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DluSJFzncGt-"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('workshop_data/imdb_top_1000.csv', index_col='imdbID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoIUbRS7cGuC"
   },
   "source": [
    "### 4.1. Exploring `DataFrame`s\n",
    "\n",
    "Pandas provides a host of ways to explore our data, which is especially useful when we're getting to know a new dataset or source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .head() returns the first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZToBG0-cGuC"
   },
   "outputs": [],
   "source": [
    "# .info() gives us a summary of some structural information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .describe() gives some statistical information about the DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .loc allows us to access a particular object (\"row\") by its index\n",
    "df.loc['tt0111161']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .iloc allows us to access a particular object (\"row\") by its position\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37kEDG3mcGuF"
   },
   "outputs": [],
   "source": [
    "# using .at returns a single value (\"cell\")\n",
    "df.at['tt0111161', 'Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LabIUXaIcGuI"
   },
   "outputs": [],
   "source": [
    "# and there's an equivalent .iat method for doing the same by position\n",
    "df.iat[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hfSMq3kcGuL"
   },
   "source": [
    "Accessing \"columns\" can be done using the dot notation, `df.column_name`, or the dictionary notation, `df['column_name']`. This returns a `Series` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOMKwfLvcGuM"
   },
   "outputs": [],
   "source": [
    "df.Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wm6YL7njcGuO"
   },
   "outputs": [],
   "source": [
    "type(df['Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIo_2NvJcGuR"
   },
   "source": [
    "Pandas allows us to very easily explore aspects of the dataset with a host of information methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRTcjc1AcGuS"
   },
   "outputs": [],
   "source": [
    "df.Rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ig0aO6H8cGuU"
   },
   "outputs": [],
   "source": [
    "df.Rated.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPBcnCmbcGuX"
   },
   "outputs": [],
   "source": [
    "df.Rated.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDy41eMxcGua"
   },
   "source": [
    "For continuous variables, especially, we can quickly get a good handle on the distribution of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxl7_Vm5cGub"
   },
   "outputs": [],
   "source": [
    "df.imdbVotes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YSwq3FmcGue"
   },
   "outputs": [],
   "source": [
    "df[['Title', 'Director']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Slicing and sub-setting\n",
    "\n",
    "`DataFrame`s can be sliced to return a subset of the available columns -- this returns a new `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the double bracket notation we can return a DataFrame\n",
    "df[[\"Title\", \"Director\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NeKNoQqcGuf"
   },
   "outputs": [],
   "source": [
    "type(df[['Title', 'Director']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzQDbWS-cGui"
   },
   "source": [
    "`Series` object and other subsets of `DataFrame`s preserve the indices of the `DataFrame` from which they are derived, which makes further operations such as merging or columns manipulation possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIJoKtqGcGui"
   },
   "source": [
    "`DataFrame`s are designed to be operated on at the column level, not at the row level. However, a subset of rows can be returned using the same slice notation that will be familiar from regular Python lists.  This will return another `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pV7l6pVrcGuj"
   },
   "outputs": [],
   "source": [
    "df[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9nuxlwWcGul"
   },
   "source": [
    "We can chain these operations together, as long as we remember what we are returning in each link of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAxDrAijcGum"
   },
   "outputs": [],
   "source": [
    "# here we first take a slice of rows 10 to 15 and then return the corresponding \"Production\" series\n",
    "df[10:15].Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the same operation as selecting the entire \"Production\" series and then slicing\n",
    "df.Production[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cd365J_cGuq"
   },
   "outputs": [],
   "source": [
    "# in this example, however, we return a DataFrame with just the \"Production\" series, and then slice that\n",
    "df[['Production']][10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUoVE-DMcGus"
   },
   "source": [
    "Above we saw the `.loc` and `.iloc` attributes which allow selection by “label” and “integer position” respectively.  These two attributes can also be used to return a subset of rows as a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remind ourselves of what the DataFrame looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qs9GzNQicGut"
   },
   "outputs": [],
   "source": [
    "# here first select just the \"Title\" and \"Plot\" columns, and then the rows from \"tt7286456\" to \"tt0071562\"\n",
    "#  (note that these are not sorted in a lexical order)\n",
    "df[[\"Title\", \"Plot\"]].loc[\"tt7286456\":\"tt0071562\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do the same thing using integer positions\n",
    "df[[\"Title\", \"Plot\"]].iloc[2:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1xtS8KmcGuw"
   },
   "source": [
    "Notice how in the `.iloc` example, record #5 is omitted, in usual python slice fashion, but in the `.loc` example, the observation with `imdbID == tt0071562` is *included* in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .reset_index() returns a copy of the DataFrame with the default Index of simple ordinals.\n",
    "df[[\"Title\", \"Plot\"]].reset_index().iloc[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCPy6uz5cGuw"
   },
   "source": [
    "#### 4.2.1 Copying and sorting `DataFrame`s\n",
    "\n",
    "`DataFrame`s provide the `.sort_values()` method to allow sorting on multiple columns. Commonly we want to sort our data temporarily at time of output -- either as we’re displaying or saving it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j53_BfcrcGux"
   },
   "outputs": [],
   "source": [
    "df.sort_values('Year', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkwXahSacGuz"
   },
   "source": [
    "However, sometimes we actually want to change the way in which the rows are ordered in a `DataFrame` in a persistent way, and we can do this using the `inplace=True` argument to the `.sort_values()` method.\n",
    "\n",
    "To demonstrate this, we're going to reset the index on the `DataFrame` so that rows are given a simple ordinal index. Since we don't want to change our `DataFrame` for the following examples, we'll make a `.copy()` to operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-UEBeh3cGuz"
   },
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a new DataFrame that we're simply throwing away,\n",
    "#  so df_copy is unchanged\n",
    "df_copy.sort_values([\"Year\"], ascending=False)\n",
    "df_copy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNrj1OY8cGu2"
   },
   "outputs": [],
   "source": [
    "# passing `inplace=True` changes (\"mutates\") the DataFrame\n",
    "df_copy.sort_values(['Year'], ascending=False, inplace=True)\n",
    "df_copy.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4G1sMuCcGu4"
   },
   "source": [
    "It is also possible to sort by multiple columns at once:\n",
    "\n",
    "(Notice how the `ascending` kwarg takes a list that applies to the list of columns to sort on in corresponding order.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPdT8nWhcGu4"
   },
   "outputs": [],
   "source": [
    "df_copy.sort_values(['Year', 'Title'], ascending=[False, True]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4rrv3JBcGu7"
   },
   "source": [
    "Notice too that since we didn't use `inplace=True` in the previous example, our `DataFrame` remains sorted as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzhLzARYcGu7"
   },
   "outputs": [],
   "source": [
    "df_copy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3EYVsLPcGu9"
   },
   "source": [
    "#### 4.2.2 Activity\n",
    "\n",
    "Given the `DataFrame` `df` defined above, write an expression to return a `DataFrame` with the columns `Title`, `Year`, `imdbRating`, and `imdbVotes` ordered by highest rating and then most votes.  Show the top five row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6f5KvSBcGu-"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kplTziGOqjJ0"
   },
   "outputs": [],
   "source": [
    "#@title → Double-click Here to Show/Hide a Prepared Solution { form-width: \"20%\" }\n",
    "df[['Title','Year','imdbRating','imdbVotes']]\\\n",
    "    .sort_values(['imdbRating', 'imdbVotes'], ascending=[False, False])\\\n",
    "    .head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfZIHBjqcGvA"
   },
   "source": [
    "### 4.3. Operations, filtering, and assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWy37YQLcGvA"
   },
   "source": [
    "Operations performed on a column, or `Series`, are broadcast to each of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCrzI4uWcGvA"
   },
   "outputs": [],
   "source": [
    "# \"Runtime\" is given in minutes; let's convert it to hours\n",
    "df.Runtime / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4yA2EbucGvC"
   },
   "source": [
    "Simple string concatenation can be performed in the same manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIZk5uXXcGvD"
   },
   "outputs": [],
   "source": [
    "df.Title + '(' + df.Rated + '), directed by ' + df.Director"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAH-220VcGvF"
   },
   "source": [
    "as can boolean operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUQwnrEzcGvG"
   },
   "outputs": [],
   "source": [
    "df.Year < 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXD8bQuJcGvI"
   },
   "source": [
    "By itself this is not terribly useful, but expressions of this kind are very powerful when passed to a `DataFrame` to select content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yM7Fr4cEcGvI"
   },
   "outputs": [],
   "source": [
    "df[df.Production == 'Paramount Pictures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTDsq3VHcGvK"
   },
   "outputs": [],
   "source": [
    "df[df.Year < 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PVnp1f3cGvN"
   },
   "source": [
    "Any expression that evaluates to a `Series` of boolean values (`True` or `False`) and shares the same `Index` as the source `DataFrame` can be used. Complex conditions can be assembled using bitwise logical operators `&`, `|`, and `~` to create simple but powerful filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sp-z1x_cGvN"
   },
   "outputs": [],
   "source": [
    "# returns a `Series`\n",
    "df[(df.Year < 2000) & (df.imdbRating > 8)].Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWxDETVocGvP"
   },
   "outputs": [],
   "source": [
    "# returns a `DataFrame`\n",
    "df.loc[(df.Year < 2000) & (df.imdbRating > 8), ['Title', 'Year', 'imdbRating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcjzobj3cGvR"
   },
   "source": [
    "The `.str` property gives access to string variables in a broadcast fashion, such that they can be manipulated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bFbBu1lcGvR"
   },
   "outputs": [],
   "source": [
    "df.Actors.str.split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CbomnSKcGvT"
   },
   "source": [
    "We can make use of `.str` (and all the built-in Python string methods) in expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQQy84aZcGvT"
   },
   "outputs": [],
   "source": [
    "# select records with Oscar nominations or wins\n",
    "df[df.Awards.str.contains('Oscar') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aP5uARVNcGvW"
   },
   "outputs": [],
   "source": [
    "# show \"Title\" and \"Director\" fields for records with more than three directors\n",
    "def directors_gt_3(director_value):\n",
    "    return len(director_value.split(', ')) > 3\n",
    "\n",
    "df[df.Director.apply(directors_gt_3)][[\"Title\", \"Director\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNOFpTcScGvY"
   },
   "source": [
    "#### 4.3.1. Assignment to `DataFrame`s\n",
    "\n",
    "It is also possible to assign values directly to columns or cells in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTzm7bSXcGvZ"
   },
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "# create a new column based on an existing one\n",
    "df_copy['Runtime (hrs)'] = df_copy.Runtime / 60\n",
    "\n",
    "# edit a single value\n",
    "df_copy.at['tt0111161', 'Title'] = 'Rita Hayworth and Shawshank Redemption'\n",
    "\n",
    "df_copy[['Title', 'Runtime', 'Runtime (hrs)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM-oIqZlcGva"
   },
   "source": [
    "#### 4.3.2. Activity\n",
    "\n",
    "Returning to our “Bay Area Counties” data from earlier, write an expression to calculate the population density of each county, and assign it to a new column on the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xVoYuX7cGva"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"county\": [\"Alameda\", \"Contra Costa\", \"Marin\", \"Napa\", \"San Francisco\", \"San Mateo\", \"Santa Clara\", \"Solano\", \"Sonoma\"],\n",
    "    \"county seat\": [\"Oakland\", \"Martinez\", \"San Rafael\", \"Napa\", \"San Francisco\", \"Redwood City\", \"San Jose\", \"Fairfield\", \"Santa Rosa\"],\n",
    "    \"population\": [1494876, 1037817, 250666, 135377, 870887, 711622, 1762754, 411620, 478551],\n",
    "    \"area\": [2130, 2080, 2140, 2040, 600.59, 1930, 3380, 2350, 4580] # in km²\n",
    "}\n",
    "bay_area_counties = pd.DataFrame(data)\n",
    "bay_area_counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InGylsebcGvm"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "HIUSkOZ8wn9L"
   },
   "outputs": [],
   "source": [
    "#@title → Double-click Here to Show/Hide a Prepared Solution { form-width: \"20%\" }\n",
    "bay_area_counties['pop. density'] = bay_area_counties.population / bay_area_counties.area\n",
    "bay_area_counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tQCmZCHcGvn"
   },
   "source": [
    "### 4.4. Cleaning and manipulating `DataFrame`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OJ1PFdocGvn"
   },
   "source": [
    "#### 4.4.1. Converting data types\n",
    "\n",
    "Pandas does a pretty good job of inferring data types when we load data into a `DataFrame`, but sometimes we want or need to change the types it selects. We can do this using the `.astype()` method on a `Series` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L-1cWrOcGvo"
   },
   "source": [
    "Those coming from an `R` or a statistical background may be used to working with categorical data, and we can force pandas to treat a column as categorical using `.astype('category')`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGy0uPR5cGvo"
   },
   "outputs": [],
   "source": [
    "df['Rated'] = df['Rated'].astype('category')\n",
    "df['Rated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zn4NH7icGvq"
   },
   "source": [
    "Using `.astype('category')` invokes the default behavior, by which categories are inferred from the data and are unordered. The only real advantage to using categories is if they are ordered, so let's quickly convert them to ordered categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQPH-jKFcGvq"
   },
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "ordered_ratings = [\n",
    "    'Not Rated',\n",
    "    'Unrated',\n",
    "    'Passed',\n",
    "    'Approved',\n",
    "    'G',\n",
    "    'GP',\n",
    "    'PG',\n",
    "    'PG-13',\n",
    "    'R',\n",
    "    'NC-17',\n",
    "]\n",
    "df['Rated'] = df['Rated'].astype(CategoricalDtype(categories=ordered_ratings, ordered=True))\n",
    "df['Rated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rated'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('Rated', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRNxgcRrcGvr"
   },
   "source": [
    "A common need is to parse dates -- if loaded from, e.g., a CSV file, they will often be interpreted as strings, as is the case with our “Released” column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkNkAE8vcGvs"
   },
   "outputs": [],
   "source": [
    "df['Released'].astype('datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VtTWSxtcGvt"
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(df['Released'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nL7yQnHOcGvw"
   },
   "outputs": [],
   "source": [
    "df['Released'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsDYG5c8x4X3"
   },
   "outputs": [],
   "source": [
    "df[['Released']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJABoB_pcGvy"
   },
   "source": [
    "#### 4.4.2. `.apply()`\n",
    "\n",
    "The `apply()` method has appeared a couple of times above. This is the fundamental way of manipulating the contents of `DataFrame`s. `apply()` takes a function as an argument, and `apply`s the function to each element in the container it’s called on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2h0tw2HtcGv0"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the Genre series\n",
    "df.Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTLriCkxcGvy"
   },
   "outputs": [],
   "source": [
    "# Based on a quick look, we might think we can use something like this:\n",
    "def count_genres_naive(text):\n",
    "    genres = text.split(\", \")\n",
    "    return len(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTbVBZSPcGv4"
   },
   "outputs": [],
   "source": [
    "# Note: This won't work...\n",
    "df.Genre.apply(count_genres_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBkp42_WcGv6"
   },
   "source": [
    "It turns out that the `Dataframe` we're working with has some missing data in the Genre column, and this `count_genres_naive` function does not know how to handle missing data.\n",
    "\n",
    "Pandas uses the `NaN` datatype from the underlying `numpy` package to represent missing data, and this datatype is based on the primitive `float` type, which is why the `Attribute` error reads as it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQkJ8H-xcGv6"
   },
   "outputs": [],
   "source": [
    "# Let's go looking for these missing values\n",
    "df[pd.isna(df.Genre)][['Title', 'Genre']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzJko0g6cGv8"
   },
   "source": [
    "We could handle this problem in a number of ways:\n",
    "1. we could drop the observations with missing values immediately (and temporarily) before we apply our count function;\n",
    "2. we could modify our `count_genres_naive` function to handle the missing values appropriately;\n",
    "3. we could fix the data manually by assigning values where they are missing; or\n",
    "4. we could clean the dataset when we initially import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0FRrZtScGv9"
   },
   "outputs": [],
   "source": [
    "# Pandas exposes the `.dropna()` method on `Series` objects for this purpose\n",
    "df.Genre.dropna().apply(count_genres_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9C7h36PcGv_"
   },
   "source": [
    "One possible problem with this approach is that we don't get a count for the observations with missing values. This means that if we want to enrich our original `DataFrame` with a new column containing the genre count, our count column will also be populated with `NaN`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdegQyuGcGv_"
   },
   "outputs": [],
   "source": [
    "df['Genre Count'] = df.Genre.dropna().apply(count_genres_naive)\n",
    "df[pd.isna(df.Genre)][['Genre', 'Genre Count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6-QNKG8cGwB"
   },
   "source": [
    "In this case, we might conclude that since the data is missing, the appropriate value for “genre count” is `0`, and we can write a less naïve operator function that handles this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGz-cGvicGwB"
   },
   "outputs": [],
   "source": [
    "def count_genres(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    genres = text.split(\", \")\n",
    "    return len(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oo3Rif5cGwC"
   },
   "outputs": [],
   "source": [
    "df['Genre Count'] = df.Genre.apply(count_genres)\n",
    "df[pd.isna(df.Genre)][['Genre', 'Genre Count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fq8vve5QcGwE"
   },
   "source": [
    "For the sake of our workshop, however, the dataset has been modified so that `Genre` has been removed where it should be the single value “Comedy”. Since we know, therefore, what the value should be for all observations where `Genre` is missing, we have the option to simply update the `DataFrame` directly in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld5gdjgwcGwE"
   },
   "outputs": [],
   "source": [
    "df.loc[pd.isna(df.Genre), 'Genre'] = 'Comedy'\n",
    "df[pd.isna(df.Genre)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdnGN5hpcGwF"
   },
   "source": [
    "We might be done at this point, but it might also be a good idea to check if there are other missing values in our dataset.  To do this we could consult the `.info()` method we saw in §4.1 above, but here's another way we might use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ng8_6lCcGwF"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaLfaXg7cGwH"
   },
   "source": [
    "#### 4.4.3. Manipulating / cleaning on import\n",
    "\n",
    "With this in mind, then, and especially when creating re-usable scripts for repetitive tasks or processes we want to iterate over, a common ‘best practice’ is to handle many data transformations when the `DataFrame` is first populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kY2enTRdcGwH"
   },
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "ordered_ratings = [\n",
    "    'Not Rated',\n",
    "    'Unrated',\n",
    "    'Passed',\n",
    "    'Approved',\n",
    "    'G',\n",
    "    'GP',\n",
    "    'PG',\n",
    "    'PG-13',\n",
    "    'R',\n",
    "    'NC-17',\n",
    "]\n",
    "\n",
    "column_types = {\n",
    "    'Rated': CategoricalDtype(categories=ordered_ratings, ordered=True)\n",
    "}\n",
    "\n",
    "fill_nans = {\n",
    "    'Genre': 'Comedy',\n",
    "    'imdbVotes': 0,\n",
    "    'Rated': 'Unrated',\n",
    "    'Awards': '',\n",
    "    'Metascore': '',\n",
    "    'BoxOffice': '',\n",
    "    'Production': '',\n",
    "    'RottenTomatoes': ''\n",
    "}\n",
    "\n",
    "date_columns = ['Released']\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'workshop_data/imdb_top_1000.csv',\n",
    "    parse_dates=date_columns,\n",
    "    index_col='imdbID',\n",
    "    dtype=column_types\n",
    ")\n",
    "\n",
    "df.fillna(value=fill_nans, inplace=True)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8kC_bfEcGwJ"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytPu39I-cGwL"
   },
   "source": [
    "#### 4.4.4. Activity\n",
    "\n",
    "Add a new column `language_count` to the `DataFrame` and show movies with more than 1 language -- your solution should return a `DataFrame` with 485 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dyt_pXFncGwL"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kGFLAIZsgZz4"
   },
   "outputs": [],
   "source": [
    "#@title → Double-click Here to Show/Hide Hints { form-width: \"20%\" }\n",
    "\n",
    "# 1. You'll probably want to write a helper\n",
    "#    function to count the languages\n",
    "\n",
    "# 2. Create a new column to the DataFrame by \n",
    "#    apply()-ing the helper function, and add\n",
    "#    it to the dataframe\n",
    "\n",
    "# 3. Filter the dataframe based on the contents\n",
    "#    of the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VazjC0TV1fch"
   },
   "outputs": [],
   "source": [
    "#@title → Double-click Here to Show/Hide a Prepared Solution { form-width: \"20%\" }\n",
    "\n",
    "def count_languages(langs):\n",
    "    return len(langs.split(', '))\n",
    "    \n",
    "df['language_count'] = df.Language.apply(count_languages)\n",
    "df[df.language_count > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxObOc48cGwM"
   },
   "source": [
    "### 4.5. Grouping and Aggregating data\n",
    "\n",
    "But what about the most-featured Director in the top 1000 list? Or the average rating for movies classified as “Comedy”? For these kinds of operations we need to compute across aggregations of the dataset. In Pandas this may be accomplished by means of `.groupby()` operation followed by an `.aggregate()` function. Aggregates can be considered a form of `.apply()` that operates on a collection of observations at once (in these cases, on the collections created by `.groupby()`.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sul-cidr/Workshops/master/Data_Manipulation_with_Python/assets/aggregation.png\" alt=\"Aggregation\">\n",
    "\n",
    "Those who are familiar with the `R` programming language, may recognize this as the “Split-Apply-Combine” approach:[<sup>*</sup>](#fn)\n",
    "- *Split* a dataset into relevant subsets;\n",
    "- *Apply* a function to each subset;\n",
    "- *Combine* all the pieces back together.\n",
    "\n",
    "<hr>\n",
    "<span id=\"fn\">* A classic paper on the Split-Apply-Combine methodology is available here: <a href=\"https://www.jstatsoft.org/article/view/v040i01/v40i01.pdf\">“The Split-Apply-Combine Strategy for Data Analysis”</a></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSPYE9e0cGwN"
   },
   "outputs": [],
   "source": [
    "# Calling `.groupby()` returns a `DataFrameGroupBy` object\n",
    "df.groupby('Director')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `DataFrameGroupBy` object has a `.groups` attribute that returns a dictionary\n",
    "df.groupby(\"Director\").groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and also exposes a couple of methods that are helpful for exploratory purposes, such as `.first()`,\n",
    "#  which returns the first row of each group\n",
    "df.groupby(\"Director\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also get all the rows for a given group\n",
    "df.groupby(\"Director\").get_group(\"Steven Spielberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or take a maximum number of two rows for each \"Director\" value\n",
    "df.groupby('Director').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas makes a number of methods available on these `DataFrameGroupBy` object that can conveniently perform common tasks such as ranking (`.rank()`), returning a random sample of each group (`.sample`), caluculating standard deviations (`.std()`) or standard errors (`.sem()`) per group, and many more.  [A full list is available in the documentation.](https://pandas.pydata.org/docs/reference/groupby.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIbzbSvgcGwQ"
   },
   "outputs": [],
   "source": [
    "# This `DataFrameGroupBy` object can be be filtered and otherwise manipulated,\n",
    "#  but remains a `DataFrameGroupBy` object\n",
    "df.groupby('Director')[['Title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps most commonly, though, we will want to perform some sort of aggregation across these groups.  Pandas provides some off-the-shelf aggregation methods such as `.sum()`, `.max()`, `.mean()` etc. (see the link above), and we can also pass our own arbitrary functions to `.aggregate()` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thTfSuLWcGwR"
   },
   "outputs": [],
   "source": [
    "# Once an aggregation is performed, the `DataFrameGroupBy` object collapses\n",
    "#  back to a `DataFrame` we can print or otherwise use\n",
    "df.groupby('Director')[['Title']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling `.count()` is the same operation as passing the python built-in `len()` function\n",
    "#  to the `.aggregate()` method\n",
    "df.groupby('Director')[['Title']].aggregate(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqxikpnLcGwS"
   },
   "source": [
    "We can then sort on the aggregated values to show the highest (and lowest) ranking Directors among our top 1,000 movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmsDq6RecGwS"
   },
   "outputs": [],
   "source": [
    "df.groupby('Director')[['Title']].count().sort_values(['Title'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfpTwmZ5cGwT"
   },
   "source": [
    "If we want to aggregate across the combination of, e.g., Director and Production credit, the `.group_by()` method will accept a list of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyZZd5GwcGwU"
   },
   "outputs": [],
   "source": [
    "df.groupby(['Director', 'Production'])[['Title']]\\\n",
    "   .count()\\\n",
    "   .sort_values(['Title'], ascending=False)\\\n",
    "   .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPAiQT-lidtB"
   },
   "source": [
    "It also is possible to perform this grouping, modification and presentation (and to generate other summaries of `DataFrame` contents) via the spreadsheet-style `pivot_table()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TyxB9AHcGwV"
   },
   "outputs": [],
   "source": [
    "df.pivot_table(\n",
    "    index=['Director', 'Production'],\n",
    "    values=['Title'],\n",
    "    aggfunc=len\n",
    ").sort_values('Title', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIZnRDb0cGwX"
   },
   "source": [
    "### 4.5.1. Activity\n",
    "\n",
    "Show the top five years for movies categorized as \"Comedy\".\n",
    "\n",
    "(Hint: don't try to parse-out the different values in the “Genre” column -- just test for the presence of the word “Comedy”, e.g. `df.Genre.str.contains('Comedy')`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROyHzweccGwX"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nt6r1emytZgo"
   },
   "outputs": [],
   "source": [
    "#@title → Double-click Here to Show/Hide a Prepared Solution { form-width: \"20%\" }\n",
    "df[df.Genre.str.contains('Comedy')].groupby('Year')[['Title']].count().sort_values(['Title'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title → Double-click Here to Show/Hide an Annotated Solution { form-width: \"20%\" }\n",
    "\n",
    "(\n",
    "    df                                            # Original, unaltered DataFrame \n",
    "        [df.Genre.str.contains(\"Comedy\")]         # filter for rows where .Genre contains the string \"Comedy\"\n",
    "        .groupby(\"Year\")                          # group by .Year\n",
    "        [[\"Title\"]]                               # select the .Title series\n",
    "        .count()                                  # aggregate, counting the number of rows per group\n",
    "        .sort_values([\"Title\"], ascending=False)  # sort by the resulting count, in descending order\n",
    "        .head(5)                                  # take the first 5 rows\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOxEiD2PcGwd"
   },
   "source": [
    "## 5. Visualization\n",
    "\n",
    "Pandas also provides some utilities to create basic plots just by calling `plot()` on a `Series` or `DataFrame`. But first we need to tell Jupyter that we are going to plot some charts using the plotting library `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xAowRB1cGwd"
   },
   "outputs": [],
   "source": [
    "# enables inline plotting in Jupyter using matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlCMe8PCcGwe"
   },
   "outputs": [],
   "source": [
    "df.groupby('Year')[['Year']].aggregate(len).plot(ylabel=\"Films\", legend=False)\n",
    "df.groupby('Year')[['Year']].aggregate(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-OPITmwvL0Y"
   },
   "source": [
    "Notice that several years are missing from the aggregated DataFrame, especially in the early years of cinema. This gives us an opportunity to explore how visualizations can reveal or hide the presence of missing values in the data, and how the `interpolate()` method can be used to fill in missing time-series values, when appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUIKoT7ovPy2"
   },
   "outputs": [],
   "source": [
    "# Reindexing by the full year range, step value=1, assigns NaN for the missing years\n",
    "df_gapped = df.groupby('Year')[['Year']].aggregate(len)\n",
    "df_gapped = df_gapped.reindex(range(df_gapped.index.min(),df_gapped.index.max()+1,1))\n",
    "df_gapped.plot(ylabel=\"Films\", legend=False)\n",
    "df_gapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHJkXdTzvP_g"
   },
   "outputs": [],
   "source": [
    "df_gapped.fillna(value=0).plot(ylabel=\"Films\", legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuA8TGPIvQFz"
   },
   "outputs": [],
   "source": [
    "interp_method = \"quadratic\" # Other options: slinear, cubic, spline\n",
    "df_yearly = df_gapped.interpolate(method=interp_method)\n",
    "df_yearly.plot(ylabel=\"Films\", legend=False)\n",
    "df_yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B_XURftcGwf"
   },
   "source": [
    "Each time you call `plot()` an `AxesSubplot` object is returned, and these are automatically rendered by the Jupyter notebook environment. `AxesSubplot` objects are objects of the underlying `matplotlib` library for plotting in Python, and as such, lots of different options can be passed to control the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DMgqqyKcGwf"
   },
   "outputs": [],
   "source": [
    "ax = df.groupby('Year')[['Year']].aggregate(len).plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(15, 5),\n",
    "    title=\"# Movies per Year\",\n",
    "    legend=False\n",
    ")\n",
    "ax.set_ylabel(\"# Movies\")\n",
    "ax.set_xlabel(\"Year of Release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g56JRp26cGwg"
   },
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SipNbPBScGwi"
   },
   "outputs": [],
   "source": [
    "with plt.style.context('ggplot'):\n",
    "    df_yearly.plot(ylabel=\"Films\", legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YooD5chfcGwj"
   },
   "outputs": [],
   "source": [
    "with plt.xkcd():\n",
    "    df_yearly.plot(ylabel=\"Films\", legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WG6kF7mcGwl"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(15, 5))\n",
    "ax.set_title('Histogram of # votes')\n",
    "ax.set_xlabel(\"Votes\")\n",
    "plt.ticklabel_format(style='plain')\n",
    "df['imdbVotes'].hist(ax=ax, bins=15, density=True, color='lightseagreen')\n",
    "df['imdbVotes'].plot(ax=ax, kind='kde', xlim=(0, 2200000), style='r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fy0nEpfjcGwn"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 6))\n",
    "plt.ticklabel_format(style='plain')\n",
    "df.boxplot(column='imdbVotes', by='Rated', grid=False, ax=ax, sym='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of0B5JcecGwp"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(15, 5))\n",
    "plt.ticklabel_format(style='plain')\n",
    "ax.scatter(x=df.Year, y=df.imdbVotes)\n",
    "ax.set_ylabel('# Votes')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_title('# Votes by Year')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CIDR :: Data Manipulation with Python and Pandas",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
