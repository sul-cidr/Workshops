{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sul-cidr/Workshops/blob/master/Intro_to_ML_with_Python/Intro_to_ML_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbIcZ4u5Bwo0"
   },
   "source": [
    "# Machine Learning with Python: Digital Tools and Methods for the Humanities and Social Sciences\n",
    "\n",
    "## Authors\n",
    "- Peter Broadwell ([CIDR](https://library.stanford.edu/research/cidr)), *broadwell@stanford.edu*\n",
    "- Simon Wiles ([CIDR](https://library.stanford.edu/research/cidr)), *simon.wiles@stanford.edu*\n",
    "\n",
    "## Sign in\n",
    "Please sign in for this workshop at https://signin.cidr.link/ML_with_Python/. When you've submitted the sign-in form, please keep your browser tab open on the evaluation form as a reminder to complete it when the workshop is over.\n",
    "\n",
    "## Roadmap\n",
    "1. Concepts: ML basics, key challenges, the Python software ecosystem\n",
    "2. Overview: Classification workflow\n",
    "3. Hands-on: text classification with `scikit-learn`\n",
    "4. Hands-on: image classification with `scikit-learn` and TensorFlow\n",
    "\n",
    "## Goals\n",
    "\n",
    "By the end of the workshop, we hope you'll have a basic understanding of the main steps involved in typical workflows of machine learning using `scikit-learn` and TensorFlow with Python, including dataset management, algorithm selection, and model training, evaluation, and interpretability.\n",
    "\n",
    "## Evaluation survey\n",
    "\n",
    "At the end of the workshop, we would be very grateful if you could, please, spend 1 minute answering a few questions that will help us to continue our workshop series: https://evaluations.cidr.link/ML_with_Python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2Ffn-tABwo7"
   },
   "source": [
    "## The obligatory Venn diagram\n",
    "\n",
    "<img src=\"https://github.com/sul-cidr/Workshops/raw/master/Intro_to_ML_with_Python/images/venn_diagram.jpeg\" width=\"50%\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Machine learning**: computational methods that learn (extract useful information) from data. This learned information may then be used to discern/predict qualities of other data or the same data.\n",
    "\n",
    "- ML combines aspects of **probability theory**, the mathematics of the likelihood of an event or proposition, and **statistics**, which measures and tests numerical aspects of data and relationships among variables, including making **inferences** from sampled data to develop models.\n",
    "\n",
    "- ML benefits from the availability of large data sets. **Data mining** is a related concept, but in practice it tends to be more exploratory rather than predictive, with its fundamental goal the exploration of large data sets (i.e., **big data**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLi2ct03McL7"
   },
   "source": [
    "## Why Python?\n",
    "\n",
    "<img src='https://github.com/sul-cidr/Workshops/raw/master/Intro_to_ML_with_Python/images/ML_staircase.png' />\n",
    "\n",
    "Python is popular across the range of \"data science\" fields; the features of the language and ecosystem help one to climb the \"AI/ML staircase\":\n",
    "- clean syntax\n",
    "- open-source components\n",
    "- straightforward data structures\n",
    "- support for numerical computation and text processing\n",
    "- easily integrated into web apps (usually)\n",
    "- fairly intuitive mix of functional, imperative, and object-oriented paradigms\n",
    "- notebook integration\n",
    "\n",
    "It's also one of the main interface languages for popular machine-learning libraries like `scikit-learn` and TensorFlow, even if their implementations are sometimes in more performance-oriented languages. Python is also a mainstay interface language of cloud-based systems like Google [AutoML](https://cloud.google.com/automl/), Amazon [SageMaker](https://aws.amazon.com/sagemaker/), Microsoft [Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/) and IBM [Watson Studio](https://www.ibm.com/cloud/machine-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GanqFO8UBwo_"
   },
   "source": [
    "## Types of machine learning\n",
    "\n",
    "Considering the type of *input*: \n",
    "- **Supervised learning**: building a mapping between **labeled inputs** and **labeled outputs** that can then be applied to new unlabeled inputs.\n",
    "- **Unsupervised learning**: discovering patterns and structure in **unlabeled inputs**.\n",
    "\n",
    "Considering the fundamental *techniques* used:\n",
    "- **Reinforcement learning**: using *feedback* to refine behavior in a dynamic environment in order to maximize some kind of *reward*.\n",
    "- **Deep learning**: applies optimization techniques based on theories of neural perception to perform enhanced model **feature selection** given labeled inputs; the resulting models typically are used for classification and clustering.\n",
    "\n",
    "Considering the *purpose* of the trained model (enter the \"estimator zoo\"): \n",
    "- Supervised learning applications:\n",
    "  - **Classification**: inputs are labeled with one or more classes, and the learner, called the **classifier**, must produce a model that assigns unseen inputs to one or more of these classes. *Example*: Spam filtering -- the inputs are email messages and the output classes are \"spam\" and \"not spam\".\n",
    "    \n",
    "    - *Algorithms*: Support Vector Machines, Decision Trees, AdaBoost, Gradient Boosting, Random Forest (ensemble), Logistic Regression, Maximum Entropy Classifier, k-Nearest Neighbor, Na√Øve Bayesian, Discriminant Analysis\n",
    "\n",
    "  - **Regression**: inputs have one or more *continuous* attributes (e.g., they lie along a range of real numbers, rather than being in a few *discrete* classes), and the learner, aka the **regressor**, generates a model that estimates the values of one or more *response* variables based on the input variables. *Example*: If you know someone's income, predict how much money they will spend on a car.\n",
    "    \n",
    "    - *Algorithms*: Support Vector Regression, Gaussian Process, Regression Trees, Gradient Boosting, Random Forest, RBF Networks, OLS, LASSO, Ridge Regression\n",
    "\n",
    "- Unsupervised learning applications:\n",
    "  - **Clustering**: inputs' group memberships are not known, and the learner must divide the inputs into groups, or **clusters**, based on some notion of the similarity between items.\n",
    "    \n",
    "    - *Algorithms*: DBScan, K-Means, Hierarchical Clustering, Self-Organizing Maps, Spectral Clustering, Minimum Entropy Clustering\n",
    "\n",
    "  - **Dimensionality reduction**: inputs' properties are reduced to a smaller number by keeping the most informative (**feature selection**), or by transforming (projecting) them into a space of fewer dimensions (**feature extraction**). \n",
    "    \n",
    "    - *Algorithms*: Principal Components Analysis, Kernel PCA, Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Qeqt6JKW9qO"
   },
   "source": [
    "## `scikit-learn`\n",
    "\n",
    "In Python, one solid choice for machine learning is the library [`scikit-learn`](http://scikit-learn.org/stable/):\n",
    "- Simple and efficient tools for data mining and data analysis\n",
    "- Open source, commercially usable (BSD license), and reusable in various contexts\n",
    "- Built on other popular Python libraries:\n",
    "  - NumPy (core numerical processing tools and data structures)\n",
    "  - SciPy (scientific computing functions, including clustering algorithms)\n",
    "  - Matplotlib (plotting and data visualization, intended to resemble MATLAB's viz features, but free)\n",
    "\n",
    "<img  src=\"https://github.com/sul-cidr/Workshops/raw/master/Intro_to_ML_with_Python/images/ml_map.png\" />\n",
    "\n",
    "<div align=\"left\" style=\"padding-top: 4px;\">Source: <a href=\"http://scikit-learn.org/stable/tutorial/machine_learning_map/\">`scikit-learn`: Choosing the right estimator</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIxctXHq1_9T"
   },
   "source": [
    "### scikit-learn's concise and unified API\n",
    "`scikit-learn` provides classes for most of the central machine learning tasks and methods, including those in the classification workflow above. These classes share many of the same interface points, with the goal of making it easier to swap or chain algorithms.\n",
    "\n",
    "For example, all `Estimators` (classes containing the actual learning algorithms)  provide the following interfaces:\n",
    "- `fit()`: load (labeled) training data and compute/\"learn\" various qualities (parameters) of the data\n",
    "- `predict()`: make predictions/estimations about other data (e.g., test data) after training\n",
    "\n",
    "Some `Estimators` and all feature extractors/`Vectorizers` also provide a `transform()` interface, which modifies and outputs data based on the parameters learned by running `fit()` on the data so that it can be used in subsequent calculations. The interface `fit_transform()` runs both of these steps on the same input data. \n",
    "\n",
    "Note also that `transform()` sometimes must be run on test (unlabeled) data prior to running `predict()` on it, but `fit()` is not run on test data because such unlabeled data is not used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-L6B3yoZBwpC"
   },
   "source": [
    "# A typical ML classification workflow: supervised text classification\n",
    "\n",
    "\n",
    "Let's walk through the steps of training a classifier to assign a category to a new text based on a set of labeled input texts. This follows the same general workflow as any other supervised classification task:\n",
    "1. **Collect or create labeled data.** In the case of Twitter data, for example, you'd download a bunch of tweets using the Twitter API, extract their texts from the JSON format of the API response, and then assign one or more labels to each tweet (the easiest way is just to use the tweet's hashtags as labels). **NOTE:** Data collection and labeling can be quite time consuming. `scikit-learn` won't always help you very much with this stage, but other Python libraries can prove useful.\n",
    "2. **Transform.** The data must be converted to a computable numeric representation. For text data, there are many strategies for turning words or characters into numbers, and `scikit-learn` has built-in libraries for most of them. Usually you'll begin by making a list of the words that apear in a document, but probably you'll also want to count the *frequencies* of the words in each document (a \"bag of words\" with counts). In `scikit-learn`, this is done by a type of `transform`er called a `CountVectorizer`. We also must choose whether to exclude uncommon words (i.e., words that only appear in a few documents) or very common words (\"stopwords\"). These high-level settings as a whole are called **hyperparameters** (different from **parameters**, which are the values learned by the model from the training data that enable it to make predictions). Each numeric value representing a characteristic of the data is called a **feature**, and the set of all features for an input text and its label is called a **feature vector.**\n",
    "3. **Train.** The whole labeled data set is split into at least two parts to train, evaluate and refine the model: a **training set** and a **test set**. You'll need to choose which learning model/algorithm to use to train the model on the training set, either by reading the documentation or talking to your friendly neighborhood data scientist.\n",
    "4. **Test.** We then use the trained model to predict the labels of the remainder of the labeled input data (the test/validation set) and tally its hits and misses (correct and incorrect labels).\n",
    "5. **Assess.** Apply one or more metrics (scoring methods) to quantify the model's ability to guess the right labels for the test/validation set. If the performance is unsatisfactory, we'll need to backtrack, possibly all the way to step #1, getting more labeled data and applying different transformations/hyper-parameters as needed, and/or trying a different model (this workflow often becomes a \"cycle\").\n",
    "\n",
    "<img src=\"https://github.com/sul-cidr/Workshops/raw/master/Intro_to_ML_with_Python/images/zoolander.png\" />\n",
    "\n",
    "<div align=\"left\" style=\"padding-top: 4px;\">The importance of choosing appropriate hyperparameters for your model. Hat tip: <a href=\"https://mimno.infosci.cornell.edu/\">David Mimno</a>.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84SvTn1T2V9r"
   },
   "source": [
    "## Loading and transforming text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6Czz0YKBwpD"
   },
   "source": [
    "Let's begin by installing `scikit-learn`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that this is already installed if you're using Google Colab\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and an extra code library needed to visualize our classification results. Then import `scikit-learn`'s `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KBru9F9XRMa"
   },
   "outputs": [],
   "source": [
    "!pip install lime\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6130y_8BwpL"
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJOk2kqVBwpP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(documents)\n",
    "print(\"Vocabulary size:\", len(count_vectorizer.vocabulary_))\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEUpCgJ3BwpV"
   },
   "outputs": [],
   "source": [
    "counts = count_vectorizer.transform(documents)\n",
    "print(\"  doc word_id count\\n   |  |         |\")\n",
    "print(counts)\n",
    "print(\"Here's the document-term matrix\")\n",
    "print(counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQxwQ4iABwpb"
   },
   "outputs": [],
   "source": [
    "# This will go through the entire vocabulary, but only show counts from the first doc\n",
    "doc = 0\n",
    "for word, word_id in count_vectorizer.vocabulary_.items():\n",
    "    print(word, \":\", counts[doc, word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9s5LZhqBwpg"
   },
   "outputs": [],
   "source": [
    "counts = count_vectorizer.fit_transform(documents)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjP9lHpGQNPk"
   },
   "outputs": [],
   "source": [
    "# Which word is missing from these counts, and why?\n",
    "new_counts = count_vectorizer.transform(['this is the fourth document'])\n",
    "print(new_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTa0EjUwBwpp"
   },
   "source": [
    "`CountVectorizer` also has some options to disregard stopwords, count ngrams (multiple adjacent words) instead of single words, cap the maximum number of words in each bag, normalize spelling, or count terms within a frequency range. It is worth exploring the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRU2Q5BaBwpq"
   },
   "source": [
    "### Activity\n",
    "\n",
    "Fill in the text in the code cell below so after fitting and transforming it using a `CountVectorizer`, the counts are as shown (order of words is not important).\n",
    "\n",
    "```\n",
    "flowers : 1\n",
    "garden : 1\n",
    "up : 1\n",
    "some : 1\n",
    "the : 1\n",
    "morning : 1\n",
    "place : 1\n",
    "every : 1\n",
    "pick : 1\n",
    "from : 1\n",
    "my : 1\n",
    "by : 1\n",
    "```\n",
    "\n",
    "**Hint**: No special parameters are needed to get this output.\n",
    "\n",
    "**Stretch goal**: What word is missing from the token counts? How would you figure out why it's missing, and how to get it back (if desired)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WfQn7KbxBwpr"
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Every morning, I pick up some flowers from the garden by my place\",\n",
    "]\n",
    "# Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "lYrOp4po4Dnd"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "vectorizer = CountVectorizer()\n",
    "# token_pattern=(r'(?u)\\b\\w+\\b')\n",
    "counts = vectorizer.fit_transform(documents)\n",
    "for word, word_id in vectorizer.vocabulary_.items():\n",
    "    print(word, \":\", counts[0, word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rh_MmbTOBwpv"
   },
   "source": [
    "## Training and testing\n",
    "\n",
    "`scikit-learn` provides  functions to split a labeled dataset into training and testing sets.\n",
    "\n",
    "**Note**: Many machine learning approaches call for splitting the labeled data into three sets:\n",
    "- **training** data (usually the largest set) for the initial model training\n",
    "- **validation** data, which is then used to evaluate the initial performance of the model and subsequently fine-tune the model settings and **hyperparameters** in the hopes of getting better results\n",
    "- **testing** data is \"held out\" until all model tuning is completed and then is used to give a final evaluation score or *benchmark* of the model's performance. It is *never* sufficient to evaluate a model by running it on data that has been used to train it.\n",
    "\n",
    "![train test validation split](https://github.com/sul-cidr/Workshops/raw/master/Intro_to_ML_with_Python/images/train_validate_test.png)\n",
    "\n",
    "*Train, test, and validation splits*  \n",
    "*Source: Tarang Shah, [About Train, Validation and Test Sets in Machine Learning](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)*\n",
    "\n",
    "To keep things simple for this tutorial, we'll mostly just use training and test sets.\n",
    "\n",
    "Additionally, in real-world applications, it is highly recommended to split the data set randomly in several different ways (*folds*) and then to compare the performance of the model on the validation/test data across all of these. This approach is called **cross-validation.** The result from a single split can be a fluke or outlier, leading to an unrealistic evaluation of the model. Cross-validation gives us a much clearer picture of the likely performance of the model given arbitrary data, and also can be a way to \"stretch\" the training data when the available training set is small.\n",
    "\n",
    "![4-fold cross validation](https://github.com/sul-cidr/Workshops/raw/master/Intro_to_ML_with_Python/images/K-fold_cross_validation.jpeg)\n",
    "\n",
    "4-fold cross validation<br>Source: Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIIvxWYw2oX9"
   },
   "source": [
    "## The text corpus\n",
    "\n",
    "For our text classification example, we will be using the [Brown corpus](https://www.nltk.org/book/ch02.html) included in the Natural Language Toolkit, which contains more than a million words of English from 500 texts, where each text is categorized into one of 15 genres. We will consider two of these genre categories: `news` (e.g., the Chicago *Tribune*'s society reportage), and `adventure` (e.g., Peter Field, *Rattlesnake Ridge* (1961)). The goal will be to create a classifier able to assign a text to `news` or `adventure` solely based on its textual contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6GJ6Yk1QBwpx"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Npm59leeprR"
   },
   "outputs": [],
   "source": [
    "for category in brown.categories():\n",
    "    print(category,len(brown.fileids(category)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9_mpxGvBwp0"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "dataset = []\n",
    "categories = ['adventure', 'news']\n",
    "examples = dict(zip(categories, []))\n",
    "for category in categories:\n",
    "    for i, fileid in enumerate(brown.fileids(category)):\n",
    "        text = \" \".join(brown.words(fileids=fileid))\n",
    "        dataset.append((text, category))\n",
    "        if i == 0:\n",
    "            examples[category] = \"...\" +  text[int(len(text)*.33):min(int(len(text)*.33)+200,len(text)-1)] + \"...\"\n",
    "\n",
    "random.shuffle(dataset)\n",
    "print(len(dataset), \"documents:\", \", \".join(\" \".join((str(len(brown.fileids(c))), c)) for c in categories))\n",
    "\n",
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pn4AL1MfBwp5"
   },
   "source": [
    "From the dataset, we can now separate the labels and the texts into two different variables. Usually, the variable containing the labels is named `y`, and the one containing the input features (in our case, the texts) is named `X`. But using arbitrary letters is confusing when you're trying to learn a new concept, so we'll add some explanatory info to the variable names after `X_` and `y_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDPS0bNWBwp5"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # scikit-learn works internally with NumPy arrays\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for text, label in dataset:\n",
    "    texts.append(text)\n",
    "    labels.append(label)\n",
    "    \n",
    "X_texts = np.array(texts)\n",
    "y_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9HxEv3zBwp9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_texts_train, X_texts_test,\n",
    " y_labels_train, y_labels_test) = train_test_split(X_texts, y_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"{} training documents\".format(*X_texts_train.shape))\n",
    "print(\"{} testing documents\".format(*X_texts_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpKxktETBwqC"
   },
   "outputs": [],
   "source": [
    "# Don't forget to fit and transform the texts as appropriate\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X_features_train = vectorizer.fit_transform(X_texts_train)\n",
    "X_features_test = vectorizer.transform(X_texts_test)\n",
    "\n",
    "print(\"{} training documents with {} features per document\".format(*X_features_train.shape))\n",
    "print(\"{} testing documents with {} features per document\".format(*X_features_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geo1wLVGBwqH"
   },
   "source": [
    "## Classification\n",
    "\n",
    "Let's start with one of the Na√Øve Bayes classifiers.\n",
    "\n",
    "**Na√Øve Bayes** is a family of classifiers based on Bayes' theorem of probability, which describes the probability of an event based on prior knowledge of possibly relevant conditions. Although its formulation can get confusing, all the math boils down to counting, multiplication and division, making Na√Øve Bayes (NB) classifiers very fast. On the other hand, NB makes the assumption that all of the features in the data set are equally important and independent, which is obviously not true for words in natural language. Despite this, Na√Øve Bayes classifiers are generally very accurate as text classifiers.  \n",
    "\n",
    "<div align=\"left\"><b>\"All models are wrong but some are useful\" - George Box (1978)</b></div>\n",
    "\n",
    "There are three Na√Øve Bayes algorimths in `scikit-learn`: \n",
    "- Gaussian: assumes that features follow a normal distribution.\n",
    "- Multinomial: good for discrete counts, like in text classification problems using counts of words.\n",
    "- Bernoulli: useful for feature vectors that are binary (i.e. zeros and ones), like word presence or absence.\n",
    "\n",
    "Given that our feature vectors are counts of the words in each document with some additional vocabulary constraints, we will use `MultinomialNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFdpCUWvBwqJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "classifier.fit(X_features_train, y_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NufZ9Yf0Jeay"
   },
   "outputs": [],
   "source": [
    "np.shape(classifier.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QlvgASU1BwqQ"
   },
   "source": [
    "Now we can predict the categories of previously unseen texts and assess how good our classifier is at classifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FntA7BqEBwqR"
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    \"This issue raises new and troubling questions.\",\n",
    "    \"Suddenly the cave entrance collapsed, trapping them inside.\"\n",
    "]\n",
    "transformed_samples = vectorizer.transform(samples)\n",
    "print(classifier.predict(transformed_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_jpMRATBwqV"
   },
   "source": [
    "### Model interpretability\n",
    "\n",
    "It's important to be able to explain how the model makes its decisions, especially given anxieties about \"black box\" models. Understanding which words are most associated with a text being `news` or `adventure`, for example, also gives us more insights about the text corpus. Most `scikit-learn` models provide ways to identify the most influential features (vocabulary words, in this case) as calculated by the model at training time.\n",
    "\n",
    "External libraries such as the `LimeTextExplainer` also can be used to explain which features were most influential in producing a particular classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8gDmzaPBwqW"
   },
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Get counts of how many times a feature (word) appears in documents\n",
    "# assigned to a specific label, as well as its mean document frequency\n",
    "# in those documents\n",
    "def get_feature_counts(dtm, labels, categories, term, vocab):\n",
    "    category_counts = {}\n",
    "    for category in categories:\n",
    "        category_appearances = list(labels).count(category)\n",
    "        category_counts[category] = {'raw': 0, 'per_doc': 0}\n",
    "        for i, label in enumerate(labels):\n",
    "            if label == category:\n",
    "                vocab_position = np.where(vocab == term)[0][0]\n",
    "                category_counts[category]['raw'] += dtm[i, vocab_position]\n",
    "        if category_appearances > 0:\n",
    "            category_counts[category]['per_doc'] = category_counts[category]['raw'] / category_appearances\n",
    "    return category_counts\n",
    "\n",
    "def most_informative_features(classifier, vectorizer=None, n=20, top_labels=2):\n",
    "    class_labels = classifier.classes_\n",
    "    # It's possible to get the feature names directly from the classifier\n",
    "    if vectorizer is None:\n",
    "        feature_names = classifier.steps[0].get_feature_names_out()\n",
    "    else:\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "    for label_i in reversed(range(top_labels)):\n",
    "        print(\"TOP TERMS ASSSOCIATED WITH\", class_labels[label_i])\n",
    "        topn_features = sorted(zip(classifier.feature_log_prob_[label_i], feature_names))[-n:]\n",
    "        for prob, feat in reversed(topn_features):\n",
    "            print(\"Class:\",class_labels[label_i], \"| feature:\", feat, \"| probability:\", prob)\n",
    "            print(str(get_feature_counts(X_features_train, y_labels_train, categories, feat, vocab)) + \"\\n\")\n",
    "        print()\n",
    "\n",
    "most_informative_features(classifier, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76zrwGx7Bwqc"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# Don't worry about this particular chunk of code\n",
    "\n",
    "%matplotlib inline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def explain(entry, clf, vectorizer=None, n=10):\n",
    "    if vectorizer is None:\n",
    "        class_names = clf.steps[1].classes_.tolist()\n",
    "        pipeline = clf\n",
    "    else:\n",
    "        class_names = clf.classes_.tolist()\n",
    "        pipeline = make_pipeline(vectorizer, clf)\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    exp = explainer.explain_instance(entry, pipeline.predict_proba, num_features=n)\n",
    "    exp.show_in_notebook()\n",
    "\n",
    "explain(\"Reports indicated that the cave entrance collapsed, trapping them inside.\", classifier, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KHsx-bbiTRXY"
   },
   "source": [
    "The results for the test sentence look quite promising, but to see how well the classifier really works, we should run it on all 19 test texts. The results are not that interesting: as it turns out, the classifier does classify all of them correctly. Many machine learning classification tasks are not so straightforward, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimators can run on multiple inputs in a single call\n",
    "classifications = classifier.predict(X_features_test)\n",
    "print(\"TRUE PREDICTED\")\n",
    "for i, true_label in enumerate(y_labels_test):\n",
    "    print(true_label, classifications[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZ2eqZ3fWkpR"
   },
   "source": [
    "## [Bonus] Feature inspection via statistical tests\n",
    "\n",
    "This is more of a data/text mining activity, but it's also possible to use statistical methods like a chi-squared test to inspect which words are most significantly related to (\"dependent on\") the labels. This gives us a sense of which words might reward attention in further analyses, such as building lexicons of words that seem indicative of specific categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LWIFxwsnFRXH"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "n_features = 20\n",
    "\n",
    "feature_selector = SelectKBest(chi2, k=n_features)\n",
    "feature_selector.fit_transform(X_features_train, y_labels_train)\n",
    "best_features = feature_selector.get_support(indices=True)\n",
    "for feature in best_features:\n",
    "    term = vocab[feature]\n",
    "    print(\"Feature:\", term, \"| chi-squared score:\", feature_selector.scores_[feature], \"| p-value:\", feature_selector.pvalues_[feature])\n",
    "    category_counts = get_feature_counts(X_features_train, y_labels_train, categories, term, vocab)\n",
    "    print(str(category_counts),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic chi-squared score only indicates how much a word's appearances diverge from the expectation that it is equally likely to appear in either set of documents. Further statistical tests can quantify the word's affinity for one category or the other. These tests involve tallying a 2x2 \"contingency table\" that counts how many times the word appears in documents with either label, compared to how all other words appear in those document groups. The tests include the log-likelihood ratio (\"G-test\") and Fisher's exact test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy # Already installed on Google Colab\n",
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "\n",
    "# Get base counts of all terms in the corpus and in each document\n",
    "document_term_totals = []\n",
    "total_corpus_terms = 0\n",
    "\n",
    "for i in range(X_features_train.shape[0]):\n",
    "    terms_in_doc = np.sum(X_features_train[i])\n",
    "    document_term_totals.append(terms_in_doc)\n",
    "    total_corpus_terms += terms_in_doc\n",
    "\n",
    "def get_feature_termness(dtm, labels, category, feature_index, vocab, document_term_totals, total_corpus_terms):\n",
    "    # Build the 2x2 contingency table\n",
    "    feature_in_category = 0\n",
    "    other_features_in_category = 0\n",
    "    feature_not_in_category = 0\n",
    "    other_features_not_in_category = 0\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        if label == category:\n",
    "            feature_in_category += dtm[i, feature_index]\n",
    "            other_features_in_category += document_term_totals[i] - dtm[i, feature_index]\n",
    "        else:\n",
    "            feature_not_in_category += dtm[i, feature_index]\n",
    "            other_features_not_in_category += document_term_totals[i] - dtm[i, feature_index]\n",
    "            \n",
    "    obs = np.array([[feature_in_category, other_features_in_category], [feature_not_in_category, other_features_not_in_category]])\n",
    "\n",
    "    # Run a G-test and Fisher's exact test on the contingency table\n",
    "    g, p, dof, expctd = chi2_contingency(obs, lambda_=\"log-likelihood\")\n",
    "    ratio_to_expected = float(feature_in_category) / expctd[0,0]\n",
    "    oddsratio, pvalue = fisher_exact(obs)\n",
    "    \n",
    "    return [ratio_to_expected, p, oddsratio, pvalue]\n",
    "\n",
    "for feature in best_features:\n",
    "    term = vocab[feature]\n",
    "    print(\"\\nFeature:\", term, \"| chi-squared score:\", feature_selector.scores_[feature], \"| p-value:\", feature_selector.pvalues_[feature])\n",
    "\n",
    "    for category in categories:\n",
    "        ratio_to_expected, p, oddsratio, pvalue = get_feature_termness(X_features_train, y_labels_train, category, feature, vocab, document_term_totals, total_corpus_terms)\n",
    "        print(category,\"- Fisher's ratio:\",oddsratio,\"p-value\",pvalue,\"Log-likelihoood:\",ratio_to_expected,\"p-value\",p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpCAKnxAHxjR"
   },
   "source": [
    "### Activity\n",
    "\n",
    "The lists of \"most informative features\" from the code above seem to include a lot of really common words, aka \"stopwords.\" We might get more meaningful results if we ignore them. Which code block above would we modify to exclude stopwords from the feature set? (Hint: it's pretty far back). Consulting the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) might also be helpful. \n",
    "\n",
    "Make this modification and then see how it changes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8zK11JrHzDQ"
   },
   "outputs": [],
   "source": [
    "# Nothing to see here, just a placeholder for the activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-Gk6F0UiO8J"
   },
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdoykO6MjEEK"
   },
   "source": [
    "For this next section, we'll use a simplified image classification task to consider how to evaluate the performance of a machine learning model.\n",
    "\n",
    "Machine learning has been applied quite successfully to image analysis, especially in recent years with the application of deep-learning techniques. Because computers see images as large lists (or arrays) of numbers, though, the computational \"features\" of images may not be as intuitively understandable as the \"features\" of texts -- i.e., words.\n",
    "\n",
    "On the other hand, it is usually quite easy to tell, literally at a glance, whether a machine-learning based classification of any unlabeled image has been successful or not.\n",
    "\n",
    "Rigorous evaluation of either text or image classifier performance, however, involves the same steps: counting classification \"hits\" and \"misses\" for the test set and then describing these results with various statistics. More on this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vyHnKkhAJKyp"
   },
   "source": [
    "## The image corpus\n",
    "\n",
    "We'll be using [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist), Zalando Research's open corpus of small images of clothing items. It is intended as a drop-in replacement for the original [MNIST](http://yann.lecun.com/exdb/mnist/index.html) set of handwritten digits, which Yann LeCun et al. used in the late 90s to illustrate the power of machine learning for computer vision applications.\n",
    "\n",
    "Fashion-MNIST for Python is availabe from the [Keras](https://keras.io/) deep-learning library that runs on top of [TensorFlow](https://github.com/tensorflow/tensorflow), the open-source machine learning framework from Google (and others).\n",
    "\n",
    "Like its handwriting-oriented predecessor, Fashion-MNIST  contains 60,000 training and 10,000 test images, with 10 classes of clothing items in place of the 10 digits from MNIST, and is available pre-split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is also already installed if you're using Google Colab\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1A418DlThr5"
   },
   "outputs": [],
   "source": [
    "# Suppress some warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(X_images_train, y_labels_train), (X_images_test, y_labels_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "print(X_images_train.shape,X_images_test.shape)\n",
    "print(set(y_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1eut0PNZcKpC"
   },
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
    "LABEL_ID = dict(zip(LABEL_NAMES, range(10)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# This is just a helper function to plot the images and their labels; no need\n",
    "# to examine it too closely\n",
    "def viz_image_labels(images, labels, true_labels=[]):\n",
    "    n = images.shape[0]\n",
    "    columns = int(np.ceil(n / 4))\n",
    "    rows = n // columns\n",
    "\n",
    "    fig = plt.figure(figsize=(columns, rows*5))\n",
    "\n",
    "    ax = []\n",
    "\n",
    "    for i in range(rows*columns):\n",
    "        ax.append(fig.add_subplot(columns, rows, i+1))\n",
    "        plt.imshow(images[i])\n",
    "        ax[-1].axis('off')\n",
    "        if type(labels[i]) == np.uint8:\n",
    "            label = LABEL_NAMES[labels[i]]\n",
    "        else:\n",
    "            label = LABEL_NAMES[np.argmax(labels[i])]\n",
    "            label += \"\\n%.3f\" % np.max(labels[i])\n",
    "        if len(true_labels) > 0:\n",
    "            label += \"\\n\" + LABEL_NAMES[true_labels[i]].upper()\n",
    "     \n",
    "        ax[-1].set_title(label)\n",
    "    plt.show()\n",
    "\n",
    "viz_image_labels(np.squeeze(X_images_train[:32]), y_labels_train[:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "165W2jcN-ppt"
   },
   "source": [
    "## Transform\n",
    "\n",
    "We'll first run the Fashion-MNIST images through some of `scikit-learn`'s preprocessing functions. Just as with our text corpus, the images need to be transformed to make the computation run more smoothly, even though they're already stored as sets of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yl0sVbjFviLL"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert each image from a 28x28 array into a 784-element vector\n",
    "X_features_train = X_images_train.reshape((X_images_train.shape[0], -1))\n",
    "X_features_test = X_images_test.reshape((X_images_test.shape[0], -1))\n",
    "\n",
    "# Normalize the values of each image vector from 0-255 (the usual range of\n",
    "# integer values for a grayscale pixel) to a real-valued distribution with\n",
    "# mean = 0. Scaling numeric values in this manner is often crucial for ML.\n",
    "scaler = StandardScaler()\n",
    "X_scaled_train = scaler.fit_transform(X_features_train.astype(np.float64))\n",
    "# To simulate real-word conditions, scale the test data based on the observed\n",
    "# training distribution only\n",
    "X_scaled_test = scaler.transform(X_features_test.astype(np.float64))\n",
    "\n",
    "# Convert an image into a simple 1,0 bitmap for demonstration purposes\n",
    "def binarize(x):\n",
    "    if (x != 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "def bitmapper(xvec):\n",
    "    return np.vectorize(binarize)(xvec)\n",
    "\n",
    "bitmapped = np.apply_along_axis(bitmapper, 1, X_images_train[0])  \n",
    "\n",
    "# Can you see the boot?\n",
    "print(bitmapped)\n",
    "print(X_images_train[0].shape)\n",
    "print(X_images_train[0])\n",
    "print(X_features_train[0].shape)\n",
    "print(X_features_train[0])\n",
    "print(X_scaled_train[0].shape)\n",
    "print(X_scaled_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQv8fUSJ3iJ-"
   },
   "source": [
    "## Train (fit) the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuOu5kzwAz4R"
   },
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "# The flowchart tells us to try LinearSVC first, but it takes *way* too long\n",
    "#from sklearn.svm import LinearSVC\n",
    "#classifier = LinearSVC(C=1,loss=\"squared_hinge\", multi_class=\"crammer_singer\", penalty=\"l1\")\n",
    "\n",
    "# So we'll use a logistic regression classifier instead, using the multinomial\n",
    "# LBFGS \"solver\" because we have 10 possible categories, not just 2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\n",
    "\n",
    "print(\"fitting the classifier, this might take a while\")\n",
    "\n",
    "# We can ignore the \"ConvergenceWarning\" that this produces\n",
    "start = time.time()\n",
    "classifier.fit(X_scaled_train, y_labels_train)\n",
    "end = time.time()\n",
    "\n",
    "print(datetime.timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hMIK6ZZ3uSP"
   },
   "source": [
    "## Predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyTa_uX63v_p"
   },
   "outputs": [],
   "source": [
    "print(\"predicting labels of test images\")\n",
    "\n",
    "y_labels_pred = classifier.predict(X_scaled_test)\n",
    "\n",
    "# Visualize the first 32 image label predictions\n",
    "viz_image_labels(np.squeeze(X_images_test[:32]), \n",
    "                 y_labels_pred[:32],\n",
    "                 y_labels_test[:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24tpjoI-aD_T"
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "Although individual predictions may be correct, the only real way to assess a model's overall performance is by comparing all of the predicted labels for the test set to their true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-BZTe7LKrGm"
   },
   "outputs": [],
   "source": [
    "print(\"Label\".ljust(11), \"Predicted\".ljust(11), \"Result\")\n",
    "print(\"-----\".ljust(11), \"---------\".ljust(11), \"------\")\n",
    "for i, true_label in enumerate(y_labels_test[:30]):\n",
    "    predicted_label = y_labels_pred[i]\n",
    "    if true_label == predicted_label:\n",
    "        result =  \"hit\"\n",
    "    else:\n",
    "        result = \"miss\"\n",
    "    print(LABEL_NAMES[true_label].ljust(11), LABEL_NAMES[predicted_label].ljust(11), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sNpJdcuuI_eC"
   },
   "source": [
    "\n",
    "**Accuracy** is one of the most important metrics for evaluating classifiers. It's defined as the ratio of correct predictions (\"hits\") to the total number of predictions. We could code it up in a line of Python, but why not just use `scikit-learn`'s [built-in function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9HB-bL0b5Dz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_labels_test, y_labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiSn7Az2cR7R"
   },
   "source": [
    "While accuracy gives an idea of how well the classifier works, it doesn't tell us much about the nature of the failures. For example, were sneakers being misclassified as sandals, or the other way around?\n",
    "\n",
    "The logistic regression classifier does pretty well, so the misclassified clothes items might not seem very important. If instead of classifying clothes you were trying to screen for a serious disease, though, you might want to err on the side of caution and use a classifier with lower overall accuracy but a very low false negative rate (that is, a very low probability of not detecting the disease at all, even if it's sometimes overly pessimistic). \n",
    "\n",
    "Fortunately, `scikit-learn` has a function to calculate a table, called a **confusion matrix**, that reveals the counts for all of the possible types of hits and misses among all of the classification categories. A confusion matrix can be a bit perplexing to read at first (the name is appropriate) but they are quite informative.\n",
    "\n",
    "Which Fashion-MNIST categories seem to confuse the classifier the most?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDJ6C734NEZJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Install the Seaborn visualization library, if not already installed\n",
    "#!pip install seaborn\n",
    "import seaborn as sn\n",
    "\n",
    "cm = confusion_matrix(y_labels_test, y_labels_pred)\n",
    "\n",
    "# This prints the matrix as text -- see below for a graphical version\n",
    "#print(\"\".ljust(12) + \"\".join([label.ljust(9) for label in LABEL_NAMES]))\n",
    "#for i, row in enumerate(cm):\n",
    "#  print(LABEL_NAMES[i].ljust(12) + \"\".join([str(cell).ljust(9) for cell in row]))\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, cbar=False, xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, cmap='Blues', fmt='d')\n",
    "# The labels along the y axis, from top to bottom, are the same as the x axis from left to right\n",
    "# Read: \"[number at x,y] were predicted as [x_label] when the true label is [y_label]\"\n",
    "#\n",
    "# So, for a given row, all of the values off the main diagonal are the false negatives for the [y_label]\n",
    "# And for a given column, all of the values off the diagonal are the false positives for the [x_label]\n",
    "\n",
    "# Correct various weirdnesses with the Seaborn visualization\n",
    "plt.yticks(rotation=0) \n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRnSlsxCljA-"
   },
   "source": [
    "Two other evaluation metrics that often prove useful are *precision* and *recall* (aka *sensitivity*). Like accuracy, these can be calculated (averaged) for the entire model, or considered separately for each category. We will use the latter approach here. In this context, the metrics can be defined as follows:\n",
    "\n",
    "- **Precision**: out of the test images the model classified as sneakers, what fraction of them were actually sneakers?\n",
    "- **Recall**: out of the total number of sneakers in the test image set, what fraction of them did the model find (i.e., correctly classify as sneakers)?\n",
    "\n",
    "Note also that the flip side of precision is *specificity*: out of all of the images the model did **not** classify as sneakers, what fraction of them really weren't sneakers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQM_NU1amNad"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "per_label_precision = precision_score(y_labels_test, y_labels_pred, average=None)\n",
    "per_label_recall = recall_score(y_labels_test, y_labels_pred, average=None)\n",
    "\n",
    "for i, label in enumerate(LABEL_NAMES):\n",
    "    print(label + \" precision: \" + str(per_label_precision[i]))\n",
    "    print(label + \" recall: \" + str(per_label_recall[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmF6SBXJarFP"
   },
   "source": [
    "### Model interpretability\n",
    "\n",
    "In this example, each of the 28x28 = 784 pixels in the Fashion-MNIST images becomes a feature of the model. Training the model involves computing **coefficient** values for each pixel that are combined with the input pixel's normalized grayscale value, adding to (if positive) or subtracting from (if negative) the likelihood of the image being assigned a particular classification label. `scikit-learn` makes it fairly easy to visualize these coefficients.\n",
    "\n",
    "For this model, it is also possible to inspect the **intercept** value of each possible label, which indicates the degree of overall bias learned in favor of or against the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DxlUAmGX9PrZ"
   },
   "outputs": [],
   "source": [
    "def viz_coefficients(classifier):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 15))\n",
    "    ax = []\n",
    "\n",
    "    for i, label in enumerate(LABEL_NAMES):\n",
    "        coefficients = np.array(np.split(classifier.coef_[i], 28))\n",
    "\n",
    "        ax.append(fig.add_subplot(4, 3, i+1))\n",
    "        plt.imshow(coefficients)\n",
    "        plt.colorbar()\n",
    "        ax[-1].axis('off')\n",
    "        ax[-1].set_title(label)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "viz_coefficients(classifier)\n",
    "\n",
    "for i, label in enumerate(LABEL_NAMES):\n",
    "    print(label, str(classifier.intercept_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_Epp0Wxn9tA"
   },
   "source": [
    "### Activity\n",
    "\n",
    "Follow the `scikit-learn` [machine learning flowchart](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) for the Fashion-MNIST data set and classification challenge, but **assume that we have >100K samples** instead of just 60K samples. Fit and run the classifier the flowchart would suggest in that case, and evaluate the performance of the model.\n",
    "\n",
    "Tip: if there's an \"early stopping\" parameter, you may wish to enable it, just so we don't have to wait too long for the training to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZfro2UloeiA"
   },
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Gu83zyF3FeRP"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import time, datetime\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "classifier = SGDClassifier(early_stopping=True)\n",
    "\n",
    "print(\"Training!\")\n",
    "\n",
    "start = time.time()\n",
    "classifier.fit(X_scaled_train, y_labels_train)\n",
    "end = time.time()\n",
    "\n",
    "print(datetime.timedelta(seconds=end-start))\n",
    "\n",
    "y_labels_pred = classifier.predict(X_scaled_test)\n",
    "\n",
    "# Visualize the first 32 image label predictions\n",
    "viz_image_labels(np.squeeze(X_images_test[:32]), \n",
    "                 y_labels_pred[:32],\n",
    "                 y_labels_test[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Ein-dGpi1u3"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",accuracy_score(y_labels_test, y_labels_pred))\n",
    "per_label_precision = precision_score(y_labels_test, y_labels_pred, average=None)\n",
    "per_label_recall = recall_score(y_labels_test, y_labels_pred, average=None)\n",
    "\n",
    "for i, label in enumerate(LABEL_NAMES):\n",
    "    print(label + \" precision: \" + str(per_label_precision[i]))\n",
    "    print(label + \" recall: \" + str(per_label_recall[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0jmYQIaUjjt"
   },
   "outputs": [],
   "source": [
    "viz_coefficients(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zr1OKMQ3bVoj"
   },
   "source": [
    "## Building a model evaluation \"pipeline\"\n",
    "\n",
    "Hopefully by now you've noticed that the process of swapping out one `Estimator` for another is quite straightforward, especially given `scikit-learn`'s standardized API. The same is true for transformers/feature extractors. In fact, it's fairly easy to code a \"pipeline\" that iteratively loads and applies different vectorizers, scalers and classifiers to the same data and also loops through ranges of hyperparameter values (a \"grid search\"), allowing you to evaluate and compare the results of many different models and settings. There are also higher-level functions to automate running multiple cross-validation splits and training/test iterations with a given data set and classifier.\n",
    "\n",
    "**Caveat:** The above are certainly helpful features, although without sufficient understanding of how the models and their various parameters work, relying too much on the \"try each key until one works\" strategy can result in many wasted hours and computing cycles.\n",
    "\n",
    "But let's suppose that we're following the flowchart and are not satisfied with the performance of the LinearSVC model (or we're just tired of waiting for it to finish training). The next suggested models to try, acording to the flowchart, are KNeighbors, other SVC classifiers, and various types of ensemble models (Bagging, RandomForest, AdaBoost, gradient boosting). We can make use of the higher-level functions of `scikit-learn` described above to train and test several of these models in quick succession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#from sklearn.svm import SVC # Not used\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "def evaluate_model(cls, X_features, y_labels, cls_name, n_folds=3):\n",
    "    # Build the pipeline with scaling and classification steps\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('classifier', cls)])\n",
    "    cv_results = cross_validate(pipe, X_features, y_labels, cv=n_folds)\n",
    "    avg_score = sum(cv_results['test_score']) / n_folds\n",
    "    print(cls_name, avg_score)\n",
    "\n",
    "# Combine train and test images into a single set (to be split again during\n",
    "# cross-validation, which re-applies the scaler to the training set)\n",
    "X_features = list(X_features_train) + list(X_features_test)\n",
    "y_labels = list(y_labels_train) + list(y_labels_test)\n",
    "\n",
    "# Regular SVC also takes a really long time to train, so we'll skip it\n",
    "#evaluate_model(SVC(gamma='auto'), X_features, y_labels, \"SVC\")\n",
    "evaluate_model(KNeighborsClassifier(), X_features, y_labels, \"KNeighbors\")\n",
    "evaluate_model(DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0), X_features, y_labels, \"DecisionTree\")\n",
    "evaluate_model(RandomForestClassifier(n_estimators=10), X_features, y_labels, \"RandomForest\")\n",
    "evaluate_model(ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0), X_features, y_labels, \"ExtraTrees\")\n",
    "evaluate_model(BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5), X_features, y_labels, \"Bagging\")\n",
    "evaluate_model(HistGradientBoostingClassifier(), X_features, y_labels, \"HistGradientBoosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can squeeze any more accuracy out of an XGBoost (\"Extreme\" Gradient Boosting) classifier, which has been one of the more popular algorithms in recent years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "\n",
    "evaluate_model(xgb_cl, X_features, y_labels, \"ExtremeGradientBoosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecJf51KwCKSO"
   },
   "source": [
    "## Deep learning image classification with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cS2VBqO8CxW1"
   },
   "source": [
    "Initialize and load a previously generated Fashion-MNIST CNN classifier model from the workshop Github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTMNNfe_Ulq3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# This defines the neural network model: 3 sets of convolutional layers,\n",
    "# applying an increasing number of filters to learn more visual features while\n",
    "# downsampling the input, then a final set of layers to correlate the\n",
    "# observed features with the training label classes\n",
    "# The names of the Conv2D layers are used for the \"model introspection\" below\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=X_color_train.shape[1:]))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu', name='big_features'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=X_color_train.shape[1:]))\n",
    "    model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='elu', name='medium_features'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=X_color_train.shape[1:]))\n",
    "    model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu', name='small_features'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.Activation('elu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "    model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndK-XyQnXCpQ"
   },
   "outputs": [],
   "source": [
    "# Download and load the pretrained model weights (see below for training code)\n",
    "!wget -O fashion_classifier.h5 --no-check-certificate \"https://github.com/sul-cidr/Workshops/raw/master/Intro_to_ML_with_Python/fashion_classifier.h5\"\n",
    "\n",
    "# Add an empty color dimension to the images, to make the model happy\n",
    "X_color_train = np.expand_dims(X_images_train, -1)\n",
    "X_color_test = np.expand_dims(X_images_test, -1)\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights('./fashion_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7taDuOJqaD7"
   },
   "source": [
    "Predict the labels of the test images -- note that we're using the original images from the Fashion-MNIST data set, rather than the versions we transformed for the `scikit-learn` estimators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWZ7jey-DYQe"
   },
   "outputs": [],
   "source": [
    "y_labels_pred = model.predict(X_color_test)\n",
    "\n",
    "# Visualize the first 32 predictions\n",
    "viz_image_labels(np.squeeze(X_color_test[:32]), \n",
    "                 y_labels_pred[:32],\n",
    "                 y_labels_test[:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWzc-V6XqnRk"
   },
   "source": [
    "Evaluate the performance of the CNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjZBIsJVtkCp"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\" + str(accuracy_score(y_labels_test, y_labels_pred.argmax(axis=1))))\n",
    "\n",
    "per_label_precision = precision_score(y_labels_test, y_labels_pred.argmax(axis=1), average=None)\n",
    "per_label_recall = recall_score(y_labels_test, y_labels_pred.argmax(axis=1), average=None)\n",
    "\n",
    "for i, label in enumerate(LABEL_NAMES):\n",
    "    print(label + \" precision: \" + str(per_label_precision[i]))\n",
    "    print(label + \" recall: \" + str(per_label_recall[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NtgdoevOzyy7"
   },
   "source": [
    "**Interpretability:** Insight into the workings of deep-learning models is infamously difficult to obtain, hence the concerns about \"black-box\" models. This isn't necessarily because the developers or lazy or because they want the models to remain mysterious, but rather because it's really hard to explain or visualize how a model with 1.6 million trainable, interrelated parameters (as is the case with this very simple model) decides whether something is a shoe or not. The visualizations below are from a set of tools that attempt to address this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1KUk0gzn7-c"
   },
   "outputs": [],
   "source": [
    "from tf_explain.core.grad_cam import GradCAM\n",
    "from tf_explain.core.occlusion_sensitivity import OcclusionSensitivity\n",
    "from tf_explain.core.activations import ExtractActivations\n",
    "\n",
    "# Grad-CAM: highlights regions of the image that are important to prediction\n",
    "gradient_explainer = GradCAM()\n",
    "# Occlusion: highlights regions that increase confidence in a given label\n",
    "occlusion_explainer = OcclusionSensitivity()\n",
    "# Activations: visualizes the different convolutional layers at a given resolution\n",
    "activation_explainer = ExtractActivations()\n",
    "\n",
    "def explain_features(n, target_label=None):\n",
    "    columns = 4\n",
    "    rows = n\n",
    "    fig = plt.figure(figsize=(columns*2, rows*2))\n",
    "    ax = []\n",
    "    pos = 1\n",
    "\n",
    "    for i, label_index in enumerate(y_labels_test[:rows]):\n",
    "        if target_label is not None:\n",
    "            label = LABEL_NAMES[label_index]\n",
    "            label_index = LABEL_NAMES.index(target_label)\n",
    "        else:\n",
    "            label = LABEL_NAMES[label_index].upper()\n",
    "\n",
    "        ax.append(fig.add_subplot(rows, columns, pos))\n",
    "        plt.imshow(X_images_test[i])\n",
    "        ax[-1].axis('off')\n",
    "        ax[-1].set_title(label)\n",
    "        pos += 1\n",
    "\n",
    "        data = ([X_images_test[i]], None)\n",
    "        grid = gradient_explainer.explain(data, model, label_index, 'small_features')\n",
    "        ax.append(fig.add_subplot(rows, columns, pos))\n",
    "        plt.imshow(grid)\n",
    "        ax[-1].axis('off')\n",
    "        ax[-1].set_title('Grad-CAM')\n",
    "        pos += 1\n",
    "\n",
    "        data = ([X_color_test[i]], None)\n",
    "        grid = occlusion_explainer.explain(data, model, label_index, 20)\n",
    "        ax.append(fig.add_subplot(rows, columns, pos))\n",
    "        plt.imshow(grid)\n",
    "        ax[-1].axis('off')\n",
    "        ax[-1].set_title('Occlusion')\n",
    "        pos += 1\n",
    "\n",
    "        data = (np.array([X_images_test[i]]), None)\n",
    "        grid = activation_explainer.explain(data, model, ['big_features'])\n",
    "        ax.append(fig.add_subplot(rows, columns, pos))\n",
    "        plt.imshow(grid)\n",
    "        ax[-1].axis('off')\n",
    "        ax[-1].set_title('Activations')\n",
    "        pos += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "explain_features(3)\n",
    "print(\"For trousers\")\n",
    "explain_features(3, \"trouser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X05DwQWCDx6G"
   },
   "source": [
    "### Training your own deep-learning model\n",
    "\n",
    "The code below was used to train the CNN classifier model that we downloaded and then used on the Fashion-MNIST images at the end of the workshop.\n",
    "\n",
    "The training runs pretty quickly on the Colab TPU infrastructure, but loading a pre-cooked model is always preferable to wasting TPU cycles. Feel free to try modifying the model definition and then generating your own model, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUsTh3P-Egpi"
   },
   "outputs": [],
   "source": [
    "# Train the model on Google Colab's cloud TPU (Tensor Processing Unit) infrastructure\n",
    "# if available (i.e., when running this notebook on Colab), otherwise try to use the\n",
    "# local machine's GPU or CPU\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "X_cnn_train = X_color_train[:50000]\n",
    "y_cnn_train = y_labels_train[:50000]\n",
    "X_cnn_validate = X_color_train[50000:]\n",
    "y_cnn_validate = y_labels_train[50000:]\n",
    "\n",
    "import os\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except Exception as e:\n",
    "    print(\"TPU not available:\", e)\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:  # Default\n",
    "        print(\"Training on CPU -- this might take a while\")\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model=create_model()\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    X_cnn_train.astype(np.float32), y_cnn_train.astype(np.float32),\n",
    "    epochs=25,\n",
    "    steps_per_epoch=50,\n",
    "    validation_data=(X_cnn_validate.astype(np.float32), y_cnn_validate.astype(np.float32)),\n",
    "    validation_freq=25\n",
    ")\n",
    "\n",
    "model.save_weights('./fashion_classifier.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXUWN_Y2EeS4"
   },
   "source": [
    "# Evaluation survey\n",
    "\n",
    "Please take a few minutes to fill out the short [evaluation survey](https://evaluations.cidr.link/ML_with_Python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GJWwi7mGFGv"
   },
   "source": [
    "# Sources\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "\n",
    "https://cloudxlab.com/blog/fashion-mnist-using-machine-learning/\n",
    "\n",
    "https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Intro to ML with Python.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
