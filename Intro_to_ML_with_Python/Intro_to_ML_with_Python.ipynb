{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to ML with Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sul-cidr/Workshops/blob/master/Intro_to_ML_with_Python/Intro_to_ML_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbIcZ4u5Bwo0",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Machine Learning with Python\n",
        "\n",
        "## Personnel\n",
        "- Peter Broadwell ([CIDR](https://library.stanford.edu/research/cidr)), *broadwell@stanford.edu*\n",
        "- Simon Wiles ([CIDR](https://library.stanford.edu/research/cidr)), *simon.wiles@stanford.edu*\n",
        "\n",
        "## Roadmap\n",
        "1. Concepts: ML basics, key challenges, why Python\n",
        "2. Overview: Classification workflow (train, test, evaluate)\n",
        "3. Hands-on: text classification with `scikit-learn`\n",
        "4. Hands-on: image classification with `scikit-learn` and TensorFlow\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDiMeoPfBwo4",
        "colab_type": "text"
      },
      "source": [
        "## Goals\n",
        "\n",
        "By the end of the workshop, we hope you'll have a basic understanding of the main steps involved in typical workflows of machine learning using `scikit-learn` and TensorFlow with Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Ffn-tABwo7",
        "colab_type": "text"
      },
      "source": [
        "## What is AI? What is ML?\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://cdn-images-1.medium.com/max/1600/1*k5P2e-b_rhH2X4u9qMsrWg.jpeg\" width=\"50%\"></img>  <figcaption>The obligatory Venn diagram</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "- **Machine learning**: computational methods that learn (extract useful information) from data. This learned information may then be used to discern/predict qualities of other data or the same data.\n",
        "\n",
        "- ML combines aspects of **statistics**, which measures and tests numerical aspects of data, including making **inferences** about relationships among variables, and **probability theory** to make numerical **predictions** (i.e., educated guesses) about data.\n",
        "\n",
        "- ML benefits from the availability of large data sets. **Data mining** is a related concept, but in practice it tends to be more exploratory rather than predictive, with its fundamental goal the exploration of large data sets (i.e., **big data**).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLi2ct03McL7",
        "colab_type": "text"
      },
      "source": [
        "## Why Python?\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://docs.google.com/uc?export=download&id=11oYdzjeqlJXfPDUYwjXbUKS_rkwafXH8\"></img> <figcaption><div align=\"left\" style=\"padding-top: 4px;\">The AI/ML staircase</div></figcaption>\n",
        "</figure>\n",
        "\n",
        "Python is popular across the range of \"data science\" fields; the features of the language and ecosystem help one to climb the \"AI/ML staircase\":\n",
        "- clean syntax\n",
        "- open-source components\n",
        "- straightforward data structures\n",
        "- support for numerical computation and text processing\n",
        "- easily integrated into web apps (usually)\n",
        "- fairly intuitive mix of functional, imperative, and object-oriented paradigms\n",
        "- notebook integration\n",
        "\n",
        "It's also one of the main interface languages for popular machine-learning libraries like `scikit-learn` and TensorFlow, even if their implementations are sometimes done in more performance-oriented languages. Python is also a mainstay interface language of cloud-based systems like Google [AutoML](https://cloud.google.com/automl/), Amazon [SageMaker](https://aws.amazon.com/sagemaker/), and Microsoft [Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/) and IBM [Watson Machine Learning](https://www.ibm.com/cloud/machine-learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GanqFO8UBwo_",
        "colab_type": "text"
      },
      "source": [
        "## Types of machine learning\n",
        "\n",
        "Considering the type of *input*: \n",
        "- **Supervised learning**: building a mapping between **labeled inputs** and **labeled outputs** that can then be applied to new unlabeled inputs.\n",
        "- **Unsupervised learning**: discovering patterns and structure in **unlabeled inputs**.\n",
        "\n",
        "Considering the fundamental *techniques* used:\n",
        "- **Reinforcement learning**: using *feedback* to refine behavior in a dynamic environment in order to maximize some kind of *reward*.\n",
        "- **Deep learning**: applies optimization techniques based on theories of neural perception to perform enhanced model **feature selection** given labeled inputs; the resulting models typically are used for classification and clustering.\n",
        "\n",
        "Considering the *purpose* of the trained model (enter the \"estimator zoo\"): \n",
        "- Supervised learning applications:\n",
        "  - **Classification**: inputs are labeled with one or more classes, and the learner, called the **classifier**, must produce a model that assigns unseen inputs to one or more of these classes. *Example*: Spam filtering -- the inputs are email messages and the output classes are \"spam\" and \"not spam\".\n",
        "    \n",
        "    - *Algorithms*: Support Vector Machines, Decision Trees, AdaBoost, Gradient Boosting, Random Forest (ensemble), Logistic Regression, Maximum Entropy Classifier, k-Nearest Neighbor, Naïve Bayesian, Discriminant Analysis\n",
        "\n",
        "  - **Regression**: inputs have one or more *continuous* attributes (e.g., they lie along a range of real numbers, rather than being in a few *discrete* classes), and the learner, aka the **regressor**, generates a model that estimates the values of one or more *response* variables based on the input variables. *Example*: If you know someone's income, predict how much money they will spend on a car.\n",
        "    \n",
        "    - *Algorithms*: Support Vector Regression, Gaussian Process, Regression Trees, Gradient Boosting, Random Forest, RBF Networks, OLS, LASSO, Ridge Regression\n",
        "\n",
        "- Unsupervised learning applications:\n",
        "  - **Clustering**: inputs' group memberships are not known, and the learner must divide the inputs into groups, or **clusters**, based on some notion of the similarity between items.\n",
        "    \n",
        "    - *Algorithms*: DBScan, K-Means, Hierarchical Clustering, Self-Organizing Maps, Spectral Clustering, Minimum Entropy Clustering\n",
        "\n",
        "  - **Dimensionality reduction**: inputs' properties are reduced to a smaller number by keeping the most informative (**feature selection**), or by transforming (projecting) them into a space of fewer dimensions (**feature extraction**). \n",
        "    \n",
        "    - *Algorithms*: Principal Components Analysis, Kernel PCA, Linear Discriminant Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qeqt6JKW9qO",
        "colab_type": "text"
      },
      "source": [
        "## `scikit-learn`\n",
        "\n",
        "In Python, one solid choice for machine learning is the library [`scikit-learn`](http://scikit-learn.org/stable/):\n",
        "- Simple and efficient tools for data mining and data analysis\n",
        "- Open source, commercially usable (BSD license), and reusable in various contexts\n",
        "- Built on other popular Python libraries:\n",
        "  - NumPy (core numerical processing tools and data structures)\n",
        "  - SciPy (scientific computing functions, including clustering algorithms)\n",
        "  - Matplotlib (plotting and data visualization, intended to resemble MATLAB's viz features, but free)\n",
        "\n",
        "<figure>\n",
        "  <img src=\"http://scikit-learn.org/stable/_static/ml_map.png\" width=\"75%\"></img> <figcaption><div align=\"left\" style=\"padding-top: 4px;\">Source: <a href=\"http://scikit-learn.org/stable/tutorial/machine_learning_map/\">`scikit-learn`: Choosing the right estimator</a></div></figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxctXHq1_9T",
        "colab_type": "text"
      },
      "source": [
        "### scikit-learn's concise and unified API\n",
        "`scikit-learn` provides classes for most of the central machine learning tasks and methods, including those in the classification workflow above. These classes share many of the same interface points, with the goal of making it easier to swap or chain algorithms.\n",
        "\n",
        "For example, all `Estimators` (classes containing the actual learning algorithms)  provide the following interfaces:\n",
        "- `fit()`: load (labeled) training data and compute/\"learn\" various qualities (parameters) of the data\n",
        "- `predict()`: make predictions about other data (e.g., test data) after training\n",
        "\n",
        "Some `Estimators` and all feature extractors/`Vectorizers` also provide a `transform()` interface, which modifies and outputs data based on the parameters learned by running `fit()` on the data so that it can be used in subsequent calculations. The interface `fit_transform()` runs both of these steps on the same input data. \n",
        "\n",
        "Note also that `transform()` sometimes must be run on test (unlabeled) data prior to running `predict()` on it, but `fit()` is not run on test data because such unlabeled data is not used to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBBiLCywBwpA",
        "colab_type": "text"
      },
      "source": [
        "## A typical ML classification workflow\n",
        "\n",
        "The general workflow for any classification task is usually as follows: \n",
        "1. **Collect** or create **labeled data**\n",
        "2. **Transform** that data into a numeric representation\n",
        "  - Each numeric value representing a characteristic of the data is called a **feature**\n",
        "  - The set of all features representing a single pair of input data and labels is called the **feature vector**\n",
        "  - The whole labeled data set is split into two parts (at least) to train, evaluate and refine the model: a **training set** and a **test set**\n",
        "3. **Train** (learn/fit) a model on a part of the transformed labeled data (the training set)\n",
        "4. **Test** the model predictions on the test set to evaluate its performance\n",
        "5. **Assess** your model and revisit each of the previous steps, if necessary (hence this workflow often becomes a \"cycle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L6B3yoZBwpC",
        "colab_type": "text"
      },
      "source": [
        "# Text classification\n",
        "\n",
        "\n",
        "Given the specific task of assigning a category to a new text based on a set of labeled input texts:\n",
        "1. **Collect and label.** In the case of Twitter data, for example, you'd download a bunch of tweets using the Twitter API, extract their texts from the JSON format of the API response, and then assign one or more labels to each tweet (the easiest way is just to use the tweet's hashtags as labels). **NOTE:** Data collection and labeling can be quite time consuming. `scikit-learn` won't always help you very much with this stage, but other Python libraries can prove useful.\n",
        "2. **Transform.** There are many strategies for turning your textual data into numbers, and `scikit-learn` has built-in libraries for most of them. Usually you'll begin by making a list of the words that apear in a document, but probably you'll also want to count the *frequencies* of the words in each document (a \"bag of words\" with counts). In `scikit-learn`, this is done by a type of transformer called a `CountVectorizer`. We also must  choose whether to exclude uncommon words (i.e., words that only appear in a few documents) or very common words (\"stopwords\"). These high-level settings as a whole are called **hyperparameters** (different from **parameters**, which are the values learned by the model from the training data that enable it to make predictions).\n",
        "3. **Train.** You'll need to choose which learning model/algorithm to use, either by reading the documentation or talking to your friendly neighborhood data scientist. After choosing a model, we train it on part of the labeled input data (the training set).\n",
        "4. **Test.** We then use the trained model to predict the labels of the remainder of the labeled input data (the test/validation set).\n",
        "5. **Assess.** Apply one or more metrics (scores) to evaluate how well the predicted labels match the actual labels of the test/validation set. If the performance is unsatisfactory, we'll need to backtrack, possibly all the way to step #1, getting more labeled data and applying different transformations/hyper-parameters as needed, and/or trying a different model.\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://docs.google.com/uc?export=download&id=1MKvc93jGGdzfXX0T25fugP4GCWVxZFNy\"></img> <figcaption><div align=\"left\" style=\"padding-top: 4px;\">The importance of choosing appropriate hyperparameters for your model. Hat tip: <a href=\"https://mimno.infosci.cornell.edu/\">David Mimno</a>.</div></figcaption>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84SvTn1T2V9r",
        "colab_type": "text"
      },
      "source": [
        "## Loading and transforming text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6Czz0YKBwpD",
        "colab_type": "text"
      },
      "source": [
        "Let's begin by installing an extra code library (needed to visualize our classification results) and then import `scikit-learn`'s `CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KBru9F9XRMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install lime\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6130y_8BwpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = [\n",
        "     'This is the first document.',\n",
        "     'This document is the second document.',\n",
        "     'And this is the third one.',\n",
        "     'Is this the first document?',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJOk2kqVBwpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "count_vectorizer.fit(documents)\n",
        "print(\"Vocabulary size:\", len(count_vectorizer.vocabulary_))\n",
        "count_vectorizer.vocabulary_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEUpCgJ3BwpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counts = count_vectorizer.transform(documents)\n",
        "print(\"  doc word_id count\\n   |  |         |\")\n",
        "print(counts)\n",
        "print(counts.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQxwQ4iABwpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will go through the entire vocabulary, but only show counts from the first doc\n",
        "doc = 0\n",
        "for word, word_id in count_vectorizer.vocabulary_.items():\n",
        "    print(word, \":\", counts[doc, word_id])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9s5LZhqBwpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counts = count_vectorizer.fit_transform(documents)\n",
        "print(counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjP9lHpGQNPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Which word is missing from these counts, and why?\n",
        "new_counts = count_vectorizer.transform(['this is the fourth document'])\n",
        "print(new_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTa0EjUwBwpp",
        "colab_type": "text"
      },
      "source": [
        "`CountVectorizer` also has some options to disregard stopwords, count ngrams (multiple adjacent words) instead of single words, cap the maximum number of words in each bag, normalize spelling, or count terms within a frequency range. It is worth exploring the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRU2Q5BaBwpq",
        "colab_type": "text"
      },
      "source": [
        "### Activity\n",
        "\n",
        "Fill in the text in the code cell below so after transforming it using a `CountVectorizer`, the counts are as shown (order of words is not important).\n",
        "\n",
        "```\n",
        "flowers : 1\n",
        "garden : 1\n",
        "up : 1\n",
        "some : 1\n",
        "the : 1\n",
        "morning : 1\n",
        "place : 1\n",
        "every : 1\n",
        "pick : 1\n",
        "from : 1\n",
        "my : 1\n",
        "by : 1\n",
        "```\n",
        "\n",
        "**Hint**: No special parameters are needed to get this output.\n",
        "\n",
        "**Stretch goal**: What word is missing from the token counts? How would you figure out why it's missing, and how to get it back (if desired)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfQn7KbxBwpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = [\n",
        "    \"Every morning, I pick up some flowers from the garden by my place\",\n",
        "]\n",
        "# Code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYrOp4po4Dnd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "vectorizer = CountVectorizer()\n",
        "# token_pattern=(r'(?u)\\b\\w+\\b')\n",
        "counts = vectorizer.fit_transform(documents)\n",
        "for word, word_id in vectorizer.vocabulary_.items():\n",
        "    print(word, \":\", counts[0, word_id])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh_MmbTOBwpv",
        "colab_type": "text"
      },
      "source": [
        "## Training and testing\n",
        "\n",
        "`scikit-learn` provides  functions to split a labeled dataset into training and testing sets.\n",
        "\n",
        "**Note**: Many machine learning approaches call for splitting the labeled data into three sets:\n",
        "- **training** data (usually the largest set) for the initial model training\n",
        "- **validation** data, which is then used to evaluate the initial performance of the model and subsequently fine-tune the model settings and **hyperparameters** in the hopes of getting better results\n",
        "- **testing** data is \"held out\" until all model tuning is completed and then is used to give a final evaluation score or *benchmark* of the model's performance.\n",
        "\n",
        "![train test validation split](https://cdn-images-1.medium.com/max/800/1*Nv2NNALuokZEcV6hYEHdGA.png)\n",
        "\n",
        "*Train, test, and validation splits*  \n",
        "*Source: Tarang Shah, [About Train, Validation and Test Sets in Machine Learning](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)*\n",
        "\n",
        "To keep things simple for this tutorial, we'll mostly just use training and test sets.\n",
        "\n",
        "Additionally, in real-world applications, it is highly recommended to split the data set randomly in several different ways (*folds*) and then to compare the performance of the model on the validation/test data across all of these. This approach is called **cross-validation.** The result from a single split can be a fluke or outlier, leading to an unrealistic evaluation of the model. Cross-validation gives us a much clearer picture of the likely performance of the model given arbitrary data, and also can be a way to \"stretch\" the training data when the available training set is small.\n",
        "\n",
        "![4-fold cross validation](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
        "\n",
        "4-fold cross validation<br>Source: Wikimedia Commons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIIvxWYw2oX9",
        "colab_type": "text"
      },
      "source": [
        "## The text corpus\n",
        "\n",
        "For our text classification example, we will be using the [Brown corpus](https://www.nltk.org/book/ch02.html) included in the Natural Language Toolkit, which contains more than a million words of English from 500 texts, where each text is categorized into one of 15 genres. We will consider two of these genre categories: `news` (e.g., the Chicago *Tribune*'s society reportage), and `adventure` (e.g., Peter Field, *Rattlesnake Ridge* (1961)). The goal will be to create a classifier able to assign a text to `news` or `adventure` solely based on its textual contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GJ6Yk1QBwpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Npm59leeprR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for category in brown.categories():\n",
        "    print(category,len(brown.fileids(category)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9_mpxGvBwp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "dataset = []\n",
        "categories = ['adventure', 'news']\n",
        "examples = dict(zip(categories, []))\n",
        "for category in categories:\n",
        "    for i, fileid in enumerate(brown.fileids(category)):\n",
        "        text = \" \".join(brown.words(fileids=fileid))\n",
        "        dataset.append((text, category))\n",
        "        if i == 0:\n",
        "            examples[category] = \"...\" +  text[35:166] + \"...\"\n",
        "\n",
        "random.shuffle(dataset)\n",
        "print(len(dataset), \"documents:\", \", \".join(\" \".join((str(len(brown.fileids(c))), c)) for c in categories))\n",
        "\n",
        "examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn4AL1MfBwp5",
        "colab_type": "text"
      },
      "source": [
        "From the dataset, we can now separate the labels and the texts into two different variables. Usually, the variable containing the labels is named `y`, and the one containing the input features (in our case, the texts) is named `X`, as in you can obtain the output `y` as a function of the inputs `X`, which is the core abstraction in `scikit-learn`. But using arbitrary letters is confusing when you're trying to learn a new concept, so we'll add some explanatory info to the variable names after `X_` and `y_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDPS0bNWBwp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  # scikit-learn works internally with NumPy arrays\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "for text, label in dataset:\n",
        "    texts.append(text)\n",
        "    labels.append(label)\n",
        "    \n",
        "X_texts = np.array(texts)\n",
        "y_labels = np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9HxEv3zBwp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(X_texts_train, X_texts_test,\n",
        " y_labels_train, y_labels_test) = train_test_split(X_texts, y_labels, test_size=0.25, random_state=42)\n",
        "\n",
        "print(\"{} training documents\".format(*X_texts_train.shape))\n",
        "print(\"{} testing documents\".format(*X_texts_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpKxktETBwqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't forget to transform the text!\n",
        "vectorizer = CountVectorizer()\n",
        "X_features_train = vectorizer.fit_transform(X_texts_train)\n",
        "X_features_test = vectorizer.transform(X_texts_test)\n",
        "\n",
        "print(\"{} training documents with {} features per document\".format(*X_features_train.shape))\n",
        "print(\"{} testing documents with {} features per document\".format(*X_features_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geo1wLVGBwqH",
        "colab_type": "text"
      },
      "source": [
        "## Classification (prediction)\n",
        "\n",
        "Let's start with one of the Naïve Bayes classifiers.\n",
        "\n",
        "**Naïve Bayes** is a family of classifiers based on Bayes' Theorem of probability, which describes the probability of an event based on prior knowledge of possibly relevant conditions. Although its formulation can get confusing, all the math boils down to counting, multiplication and division, making Naïve Bayes (NB) classifiers very fast. On the other hand, NB makes the assumption that all of the features in the data set are equally important and independent, which is obviously not true for words. Despite this, Naïve Bayes classifiers are generally very accurate as text classifiers.\n",
        "\n",
        "<div align=\"left\"><b>\"All models are wrong but some are useful\" - George Box (1978)</b></div>\n",
        "\n",
        "There are three Naïve Bayes algorimths in `scikit-learn`: \n",
        "- Gaussian: assumes that features follow a normal distribution.\n",
        "- Multinomial: good for discrete counts, like in text classification problems using counts of words.\n",
        "- Bernoulli: useful for feature vectors that are binary (i.e. zeros and ones), like word presence or absence.\n",
        "\n",
        "Given that our feature vectors are counts of the words in each document with some additional vocabulary constraints, we will use `MultinomialNB`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFdpCUWvBwqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "classifier.fit(X_features_train, y_labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NufZ9Yf0Jeay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.shape(classifier.feature_log_prob_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlvgASU1BwqQ",
        "colab_type": "text"
      },
      "source": [
        "Now we can predict the categories of previously unseen texts and assess how good our classifier is at classifying them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FntA7BqEBwqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = [\n",
        "    \"This issue raises new and troubling questions.\",\n",
        "    \"Suddenly the cave entrance collapsed, trapping them inside.\"\n",
        "]\n",
        "transformed_samples = vectorizer.transform(samples)\n",
        "classifier.predict(transformed_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_jpMRATBwqV",
        "colab_type": "text"
      },
      "source": [
        "### Model interpretability\n",
        "\n",
        "It's important to be able to explain how the model makes its decisions, especially given anxieties about \"black box\" AI models. Understanding which words are most associated with a text being `news` or `adventure` also gives us more insights about the text corpus. Most `scikit-learn` models provide ways to identify the most influential features (vocabulary words, in this case) as calculated by the model at training time.\n",
        "\n",
        "External libraries such as the `LimeTextExplainer` also can be used to explain which features were most influential in producing a particular classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8gDmzaPBwqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = np.array(vectorizer.get_feature_names())\n",
        "\n",
        "def get_feature_counts(dtm, labels, categories, term, vocab):\n",
        "  category_counts = {}\n",
        "  for category in categories:\n",
        "    category_counts[category] = 0\n",
        "    for i, label in enumerate(labels):\n",
        "      if label == category:\n",
        "        vocab_position = np.where(vocab == term)[0][0]\n",
        "        category_counts[category] += dtm[i, vocab_position]\n",
        "  return category_counts\n",
        "\n",
        "def most_informative_features(classifier, vectorizer=None, n=20):\n",
        "    class_labels = classifier.classes_\n",
        "    if vectorizer is None:\n",
        "        feature_names = classifier.steps[0].get_feature_names()\n",
        "    else:\n",
        "        feature_names = vectorizer.get_feature_names()\n",
        "    topn_class1 = sorted(zip(classifier.feature_log_prob_[0], feature_names))[-n:]\n",
        "    topn_class2 = sorted(zip(classifier.feature_log_prob_[1], feature_names))[-n:]\n",
        "    for prob, feat in reversed(topn_class2):\n",
        "        print(class_labels[1], prob, feat)\n",
        "        print(str(get_feature_counts(X_features_train, y_labels_train, categories, feat, vocab)))\n",
        "    print()\n",
        "    for prob, feat in reversed(topn_class1):\n",
        "        print(class_labels[0], prob, feat)\n",
        "        print(str(get_feature_counts(X_features_train, y_labels_train, categories, feat, vocab)))\n",
        "\n",
        "most_informative_features(classifier, vectorizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76zrwGx7Bwqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture --no-display\n",
        "# Don't worry about this particular chunk of code\n",
        "\n",
        "%matplotlib inline\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def explain(entry, clf, vectorizer=None, n=10):\n",
        "    if vectorizer is None:\n",
        "        class_names = clf.steps[1].classes_.tolist()\n",
        "        pipeline = clf\n",
        "    else:\n",
        "        class_names = clf.classes_.tolist()\n",
        "        pipeline = make_pipeline(vectorizer, clf)\n",
        "    explainer = LimeTextExplainer(class_names=class_names)\n",
        "    exp = explainer.explain_instance(entry, pipeline.predict_proba, num_features=n)\n",
        "    exp.show_in_notebook()\n",
        "\n",
        "explain(\"Reports indicated that the cave entrance collapsed, trapping them inside.\", classifier, vectorizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KHsx-bbiTRXY"
      },
      "source": [
        "The results for the test sentence look quite promising, but to see how well the classifier really works, we should run it on all 19 test texts. We'll skip that for today, because as it turns out, the classifier does classify all of them correctly. Many machine learning classification tasks are not so straightforward, however.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ2eqZ3fWkpR",
        "colab_type": "text"
      },
      "source": [
        "### [Bonus] Feature inspection via chi-squared test\n",
        "\n",
        "This is more of a data/text mining activity, but it's also possible to use statistical methods like the chi-squared test to inspect which words are most significantly related to (\"dependent on\") the labels. This gives us a sense of which words might reward attention in further analyses (\"feature selection\"). Note that the results are somewhat different from the feature likelihood rankings from the Naive Bayes classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWIFxwsnFRXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "n_features = 20\n",
        "\n",
        "keyness, p_value = chi2(X_features_train, y_labels_train)\n",
        "ranking = np.argsort(keyness)[::-1]\n",
        "for n in range(n_features):\n",
        "  term = vocab[ranking][n]\n",
        "  print(term, keyness[ranking][n], p_value[ranking][n])\n",
        "  category_counts = get_feature_counts(X_features_train, y_labels_train, categories, term, vocab)\n",
        "  print(str(category_counts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpCAKnxAHxjR",
        "colab_type": "text"
      },
      "source": [
        "### Activity\n",
        "\n",
        "The lists of \"most informative features\" from the code above seem to include a lot of really common words, aka \"stopwords.\" We might get more meaningful results if we ignore them. Which code block above would we modify to exclude stopwords from the feature set? (Hint: it's pretty far back). Consulting the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) might also be helpful. \n",
        "\n",
        "Make this modification and then see how it changes the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8zK11JrHzDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nothing to see here, just a placeholder for the activity."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Gk6F0UiO8J",
        "colab_type": "text"
      },
      "source": [
        "# Image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdoykO6MjEEK",
        "colab_type": "text"
      },
      "source": [
        "For this next section, we'll use a simplified image classification task to consider how to evaluate the performance of a machine learning model.\n",
        "\n",
        "Machine learning has been applied quite successfully to image analysis, especially in recent years with the application of deep-learning techniques. Because computers see images as large lists (or arrays) of numbers, though, the computational \"features\" of images may not be as intuitively understandable as the \"features\" of texts -- i.e., words.\n",
        "\n",
        "On the other hand, it is usually quite easy to tell, literally at a glance, whether a machine-learning based classification of any single image has been successful or not.\n",
        "\n",
        "Rigorous evaluation of either text or image classifier performance, however, involves the same steps: counting classification \"hits\" and \"misses\" for the test set and then describing these results with various statistics. More on this below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyHnKkhAJKyp",
        "colab_type": "text"
      },
      "source": [
        "## The image corpus\n",
        "\n",
        "We'll be using [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist), Zalando Research's open corpus of small images of clothing items. It is intended as a drop-in replacement for the original and somewhat overused [MNIST](http://yann.lecun.com/exdb/mnist/) set of handwritten digits, which Yann LeCun et al. used in the late 90s to illustrate the power of machine learning for computer vision applications.\n",
        "\n",
        "Fashion-MNIST for Python is availabe from the [Keras](https://keras.io/) deep-learning library that runs on top of [TensorFlow](https://github.com/tensorflow/tensorflow), the open-source machine learning framework from Google (and others).\n",
        "\n",
        "Like its handwriting-oriented predecessor, Fashion-MNIST  contains 60,000 training and 10,000 test images, with 10 classes of clothing items in place of the 10 digits from MNIST, and is available pre-split into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1A418DlThr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "(X_images_train, y_labels_train), (X_images_test, y_labels_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "print(X_images_train.shape,X_images_test.shape)\n",
        "print(set(y_labels_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eut0PNZcKpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_NAMES = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
        "LABEL_ID = dict(zip(LABEL_NAMES, range(10)))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# This is just a helper function to plot the images and their labels; no need\n",
        "# to consider it too closely\n",
        "def viz_image_labels(images, labels, true_labels=[]):\n",
        "  n = images.shape[0]\n",
        "  columns = int(np.ceil(n / 4))\n",
        "  rows = n // columns\n",
        "  \n",
        "  fig = plt.figure(figsize=(columns, rows*5))\n",
        "  \n",
        "  ax = []\n",
        "  \n",
        "  for i in range(rows*columns):\n",
        "    ax.append(fig.add_subplot(columns, rows, i+1))\n",
        "    plt.imshow(images[i])\n",
        "    ax[-1].axis('off')\n",
        "    if type(labels[i]) == np.uint8:\n",
        "      label = LABEL_NAMES[labels[i]]\n",
        "    else:\n",
        "      label = LABEL_NAMES[np.argmax(labels[i])]\n",
        "      label += \"\\n%.3f\" % np.max(labels[i])\n",
        "    if len(true_labels) > 0:\n",
        "      label += \"\\n\" + LABEL_NAMES[true_labels[i]].upper()\n",
        "     \n",
        "    ax[-1].set_title(label)\n",
        "  plt.show()\n",
        "\n",
        "viz_image_labels(np.squeeze(X_images_train[:32]), y_labels_train[:32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "165W2jcN-ppt",
        "colab_type": "text"
      },
      "source": [
        "## Transform\n",
        "\n",
        "We'll first run the Fashion-MNIST images through some of `scikit-learn`'s preprocessing functions. Just as with our text corpus, the images need to be transformed to make the computation run more smoothly (even though they're already stored as sets of numbers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl0sVbjFviLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Convert each image from a 28x28 array into a 784-element vector\n",
        "X_features_train = X_images_train.reshape((X_images_train.shape[0], -1))\n",
        "X_features_test = X_images_test.reshape((X_images_test.shape[0], -1))\n",
        "\n",
        "# Compress the values of each image vector from 0-255 to a smaller range\n",
        "scaler = StandardScaler()\n",
        "X_scaled_train = scaler.fit_transform(X_features_train.astype(np.float64))\n",
        "X_scaled_test = scaler.fit_transform(X_features_test.astype(np.float64))\n",
        "\n",
        "# Convert an image into a simple 1,0 bitmap for demonstration purposes\n",
        "def binarize(x):\n",
        "  if (x != 0):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0 \n",
        "\n",
        "def bitmapper(xvec):\n",
        "  return np.vectorize(binarize)(xvec)\n",
        "\n",
        "bitmapped = np.apply_along_axis(bitmapper, 1, X_images_train[0])  \n",
        "\n",
        "# Can you see the boot?\n",
        "print(bitmapped)\n",
        "print(X_images_train[0].shape)\n",
        "print(X_images_train[0])\n",
        "print(X_features_train[0].shape)\n",
        "print(X_features_train[0])\n",
        "print(X_scaled_train[0].shape)\n",
        "print(X_scaled_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQv8fUSJ3iJ-",
        "colab_type": "text"
      },
      "source": [
        "## Train (fit) the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuOu5kzwAz4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time, datetime\n",
        "\n",
        "# The flowchart tells us to try LinearSVC first, but it takes *way* too long\n",
        "#from sklearn.svm import LinearSVC\n",
        "#classifier = LinearSVC(C=1,loss=\"squared_hinge\", multi_class=\"crammer_singer\", penalty=\"l1\")\n",
        "\n",
        "# So we'll use a logistic regression classifier instead, using the multinomial\n",
        "# LBFGS \"solver\" because we have 10 possible categories, not just two\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\n",
        "\n",
        "print(\"fitting the classifier, this might take a while\")\n",
        "\n",
        "# We can ignore the \"ConvergenceWarning\" that this produces\n",
        "start = time.time()\n",
        "classifier.fit(X_scaled_train, y_labels_train)\n",
        "end = time.time()\n",
        "\n",
        "print(datetime.timedelta(seconds=end-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hMIK6ZZ3uSP",
        "colab_type": "text"
      },
      "source": [
        "## Predict labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyTa_uX63v_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"predicting labels of test images\")\n",
        "\n",
        "y_labels_pred = classifier.predict(X_scaled_test)\n",
        "\n",
        "# Visualize the first 32 image label predictions\n",
        "viz_image_labels(np.squeeze(X_images_test[:32]), \n",
        "                 y_labels_pred[:32],\n",
        "                 y_labels_test[:32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24tpjoI-aD_T",
        "colab_type": "text"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "Although individual predictions may be correct, the only real way to assess a model's overall performance is by comparing all of the predicted labels for the test set to their true labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-BZTe7LKrGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Label\".ljust(11), \"Predicted\".ljust(11), \"Result\")\n",
        "print(\"-----\".ljust(11), \"---------\".ljust(11), \"------\")\n",
        "for i, true_label in enumerate(y_labels_test[:30]):\n",
        "    predicted_label = y_labels_pred[i]\n",
        "    if true_label == predicted_label:\n",
        "        result =  \"hit\"\n",
        "    else:\n",
        "        result = \"miss\"\n",
        "    print(LABEL_NAMES[true_label].ljust(11), LABEL_NAMES[predicted_label].ljust(11), result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNpJdcuuI_eC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Accuracy** is one of the most important metrics for evaluating classifiers. It's defined as the ratio of correct predictions (\"hits\") to the total number of predictions. We could code it up in a line of Python, but why not just use `scikit-learn`'s [built-in function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9HB-bL0b5Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y_labels_test, y_labels_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiSn7Az2cR7R",
        "colab_type": "text"
      },
      "source": [
        "While accuracy gives an idea of how well the classifier works, it doesn't tell us much about the nature of the failures. For example, were sneakers being misclassified as sandals, or the other way around?\n",
        "\n",
        "The logistic regression classifier does pretty well, so the misclassified clothes items might not seem very important. If instead of classifying clothes you were trying to screen for a serious disease, though, you might want to err on the side of caution and use a classifier with lower overall accuracy but a very low false negative rate (= very low probability of not detecting the disease at all, even if it's sometimes overly pessimistic). \n",
        "\n",
        "Fortunately, `scikit-learn` has a function to calculate a table, called a **confusion matrix**, that reveals the counts for all of the possible types of hits and misses among all of the classification categories. A confusion matrix can be a bit perplexing to read at first (the name is appropriate) but they are quite informative.\n",
        "\n",
        "Which Fashion-MNIST categories seem to confuse the classifier the most?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDJ6C734NEZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "cm = confusion_matrix(y_labels_test, y_labels_pred)\n",
        "\n",
        "# This prints the matrix as text -- see below for a graphical version\n",
        "#print(\"\".ljust(12) + \"\".join([label.ljust(9) for label in LABEL_NAMES]))\n",
        "#for i, row in enumerate(cm):\n",
        "#  print(LABEL_NAMES[i].ljust(12) + \"\".join([str(cell).ljust(9) for cell in row]))\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(cm, annot=True, cbar=False, xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, cmap='Blues', fmt='d')\n",
        "# The labels along the y axis, from top to bottom, are the same as the x axis from left to right\n",
        "# Read: \"[number at x,y] were predicted as [x_label] when the true label is [y_label]\"\n",
        "#\n",
        "# So, for a given row, all of the values off the main diagonal are the false negatives for the [y_label]\n",
        "# And for a given column, all of the values off the diagonal are the false positives for the [x_label]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRnSlsxCljA-",
        "colab_type": "text"
      },
      "source": [
        "Two other evaluation metrics that often prove useful are *precision* and *recall*. Like accuracy, these can be calculated (averaged) for the entire model, or considered separately for each category. We will use the latter approach here. In this context, the metrics can be defined as follows:\n",
        "\n",
        "- **Precision**: out of the test images the model classified as sneakers, what fraction of them were actually sneakers?\n",
        "- **Recall**: out of the total number of sneakers in the test image set, what fraction of them did the model find (i.e., correctly classify as sneakers)?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQM_NU1amNad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "per_label_precision = precision_score(y_labels_test, y_labels_pred, average=None)\n",
        "per_label_recall = recall_score(y_labels_test, y_labels_pred, average=None)\n",
        "\n",
        "for i, label in enumerate(LABEL_NAMES):\n",
        "  print(label + \" precision: \" + str(per_label_precision[i]))\n",
        "  print(label + \" recall: \" + str(per_label_recall[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmF6SBXJarFP",
        "colab_type": "text"
      },
      "source": [
        "### Model interpretability\n",
        "\n",
        "In this example, each of the 28x28 = 784 pixels in the Fashion-MNIST images becomes a feature of the model. Training the model involves computing **coefficient** values for each pixel that are combined with the input pixel's color value, adding to (if positive) or subtracting from (if negative) the likelihood of the image being assigned a particular classification label. `scikit-learn` makes it fairly easy to visualize these coefficients.\n",
        "\n",
        "For this model, it is also possible to inspect the **intercept** value of each possible label, which indicates the degree of overall bias learned in favor of or against the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxlUAmGX9PrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def viz_coefficients(classifier):\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 15))\n",
        "  ax = []\n",
        "\n",
        "  for i, label in enumerate(LABEL_NAMES):\n",
        "    coefficients = np.array(np.split(classifier.coef_[i], 28))\n",
        "\n",
        "    ax.append(fig.add_subplot(4, 3, i+1))\n",
        "    plt.imshow(coefficients)\n",
        "    plt.colorbar()\n",
        "    ax[-1].axis('off')\n",
        "    ax[-1].set_title(label)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "viz_coefficients(classifier)\n",
        "\n",
        "for i, label in enumerate(LABEL_NAMES):\n",
        "  print(label, str(classifier.intercept_[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Epp0Wxn9tA",
        "colab_type": "text"
      },
      "source": [
        "### Activity\n",
        "\n",
        "Follow the `scikit-learn` [machine learning flowchart](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) for the Fashion-MNIST data set and classification challenge, but **assume that we have >100K samples** instead of just 60K samples. Fit and run the classifier the flowchart would suggest in that case, and evaluate the performance of the model.\n",
        "\n",
        "Tip: if there's an \"early stopping\" parameter, you may wish to enable it, just so we don't have to wait too long for the training to finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZfro2UloeiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu83zyF3FeRP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import time, datetime\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "classifier = SGDClassifier(early_stopping=True)\n",
        "\n",
        "print(\"Training!\")\n",
        "\n",
        "start = time.time()\n",
        "classifier.fit(X_scaled_train, y_labels_train)\n",
        "end = time.time()\n",
        "\n",
        "print(datetime.timedelta(seconds=end-start))\n",
        "\n",
        "y_labels_pred = classifier.predict(X_scaled_test)\n",
        "\n",
        "# Visualize the first 32 image label predictions\n",
        "viz_image_labels(np.squeeze(X_images_test[:32]), \n",
        "                 y_labels_pred[:32],\n",
        "                 y_labels_test[:32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ein-dGpi1u3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Accuracy:\",accuracy_score(y_labels_test, y_labels_pred))\n",
        "per_label_precision = precision_score(y_labels_test, y_labels_pred, average=None)\n",
        "per_label_recall = recall_score(y_labels_test, y_labels_pred, average=None)\n",
        "\n",
        "for i, label in enumerate(LABEL_NAMES):\n",
        "  print(label + \" precision: \" + str(per_label_precision[i]))\n",
        "  print(label + \" recall: \" + str(per_label_recall[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0jmYQIaUjjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "viz_coefficients(classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr1OKMQ3bVoj",
        "colab_type": "text"
      },
      "source": [
        "### Towards a model evaluation \"pipeline\"\n",
        "\n",
        "Hopefully by now you've noticed that the process of swapping out one `Estimator` for another is quite straightforward, especially given `scikit-learn`'s standardized API. The same is true for transformers/feature extractors.\n",
        "\n",
        "In fact, it's fairly easy to code a \"pipeline\" that iteratively loads and applies different vectorizers and classifiers to the same data and also loops through different hyperparameters, allowing you to evaluate and compare the results of many different models and settings -- so that hopefully you will find the best possible combination for the problem you're trying to solve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecJf51KwCKSO",
        "colab_type": "text"
      },
      "source": [
        "## Bonus: Deep learning image classification with Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS2VBqO8CxW1",
        "colab_type": "text"
      },
      "source": [
        "Initialize and load a previously generated Fashion-MNIST CNN classifier model from the workshop Github repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTMNNfe_Ulq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# This defines the neural network model: 3 sets of convolutional layers,\n",
        "# applying an increasing number of filters to learn more visual features while\n",
        "# downsampling the input, then a final set of layers to correlate the\n",
        "# observed features with the training label classes\n",
        "def create_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape=X_color_train.shape[1:]))\n",
        "    model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape=X_color_train.shape[1:]))\n",
        "    model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='elu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape=X_color_train.shape[1:]))\n",
        "    model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(256))\n",
        "    model.add(tf.keras.layers.Activation('elu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(10))\n",
        "    model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndK-XyQnXCpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and load the pretrained model weights (see below for training code)\n",
        "!wget -O fashion_classifier.h5 --no-check-certificate \"https://github.com/sul-cidr/Workshops/raw/master/Intro_to_ML_with_Python/fashion_classifier.h5\"\n",
        "\n",
        "# Add an empty color dimension to the images, to make the model happy\n",
        "X_color_train = np.expand_dims(X_images_train, -1)\n",
        "X_color_test = np.expand_dims(X_images_test, -1)\n",
        "\n",
        "model = create_model()\n",
        "model.load_weights('./fashion_classifier.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7taDuOJqaD7",
        "colab_type": "text"
      },
      "source": [
        "Predict the labels of the test images -- note that we're using the original images from the Fashion-MNIST data set, rather than the versions we transformed for the `scikit-learn` estimators.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWZ7jey-DYQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_labels_pred = model.predict(X_color_test)\n",
        "\n",
        "# Visualize the first 32 predictions\n",
        "viz_image_labels(np.squeeze(X_color_test[:32]), \n",
        "                 y_labels_pred[:32],\n",
        "                 y_labels_test[:32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWzc-V6XqnRk",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the performance of the CNN classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjZBIsJVtkCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Accuracy:\" + str(accuracy_score(y_labels_test, y_labels_pred.argmax(axis=1))))\n",
        "\n",
        "per_label_precision = precision_score(y_labels_test, y_labels_pred.argmax(axis=1), average=None)\n",
        "per_label_recall = recall_score(y_labels_test, y_labels_pred.argmax(axis=1), average=None)\n",
        "\n",
        "for i, label in enumerate(LABEL_NAMES):\n",
        "  print(label + \" precision: \" + str(per_label_precision[i]))\n",
        "  print(label + \" recall: \" + str(per_label_recall[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtgdoevOzyy7",
        "colab_type": "text"
      },
      "source": [
        "**Interpretability:** Insight into the workings of deep-learning models is infamously difficult to obtain, hence the concerns about \"black-box\" models. This isn't necessarily because the developers or lazy or because they want the models to remain mysterious, but rather because it's really hard to explain or visualize how a model with 1.6 million trainable, interrelated parameters (as is the case with this very simple model) decides whether something is a shoe or not. The visualizations below are from a set of tools that attempt to address this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXIg7yRWqzol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/raghakot/keras-vis.git\n",
        "\n",
        "# Currently, these visualizations only work on a version of the model trained\n",
        "# under the old Colab TPU environment\n",
        "!wget -O fashion_classifier_old.h5 --no-check-certificate \"https://github.com/sul-cidr/Workshops/raw/master/Intro_to_ML_with_Python/fashion_classifier_old.h5\"\n",
        "tpu_model = tf.keras.models.load_model('fashion_classifier_old.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1KUk0gzn7-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vis.visualization import visualize_saliency, visualize_activation, visualize_cam\n",
        "from vis.utils import utils\n",
        "import keras\n",
        "\n",
        "def visualize_cnn_features(images, titles):\n",
        "  fig = plt.figure(figsize=(10, 3))\n",
        "  ax = []\n",
        "\n",
        "  for i, image in enumerate(images):\n",
        "    ax.append(fig.add_subplot(1, 3, i+1))\n",
        "    plt.imshow(image)\n",
        "    plt.colorbar()\n",
        "    ax[-1].axis('off')\n",
        "    ax[-1].set_title(titles[i])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "for l, layer_id in enumerate(tpu_model.layers):\n",
        "  print(l, layer_id)\n",
        "\n",
        "session = tf.compat.v1.Session()\n",
        "\n",
        "# This function shows \"the model input that maximizes the output of the\n",
        "# filter_index (ankle boots) in the given layer of the network.\"\n",
        "bmp = visualize_activation(tpu_model, layer_idx=1, filter_indices=[int(y_labels_test[0])])\n",
        "activation = bmp[:, :, 0]\n",
        "\n",
        "# The next two functions generate a heatmap visualization of the \"input regions\n",
        "# whose change would most contribute towards the model classifying the input\n",
        "# image (ankle boots) as the filter_index (ankle boots)\"\n",
        "\n",
        "saliency = visualize_saliency(tpu_model, layer_idx=1,\n",
        "                              filter_indices=[LABEL_ID['ankle_boots']],\n",
        "                              seed_input=X_color_test[0])\n",
        "\n",
        "cam = visualize_cam(tpu_model, layer_idx=2, filter_indices=[LABEL_ID['ankle_boots']],\n",
        "                    seed_input=X_color_test[0], penultimate_layer_idx=1)\n",
        "\n",
        "visualize_cnn_features([activation, saliency, cam],\n",
        "                       [\"activation\", \"saliency\", \"class activation map\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X05DwQWCDx6G",
        "colab_type": "text"
      },
      "source": [
        "### Training your own deep-learning model\n",
        "\n",
        "The code below was used to train the CNN classifier model that we downloaded and then used on the Fashion-MNIST images at the end of the workshop.\n",
        "\n",
        "The training runs pretty quickly on the Colab TPU infrastructure, but loading a pre-cooked model is always preferable to wasting TPU cycles. Feel free to try modifying the model definition and then generating your own model, though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUsTh3P-Egpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model on Google Colab's cloud TPU (Tensor Processing Unit) infrastructure\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "X_cnn_train = X_color_train[:50000]\n",
        "y_cnn_train = y_labels_train[:50000]\n",
        "X_cnn_validate = X_color_train[50000:]\n",
        "y_cnn_validate = y_labels_train[50000:]\n",
        "\n",
        "import os\n",
        "\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "with strategy.scope():\n",
        "  model=create_model()\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    X_cnn_train.astype(np.float32), y_cnn_train.astype(np.float32),\n",
        "    epochs=25,\n",
        "    steps_per_epoch=50,\n",
        "    validation_data=(X_cnn_validate.astype(np.float32), y_cnn_validate.astype(np.float32)),\n",
        "    validation_freq=25\n",
        ")\n",
        "\n",
        "model.save_weights('./fashion_classifier.h5', overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXUWN_Y2EeS4",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation survey\n",
        "\n",
        "Please take a few minutes to fill out the short evaluation survey."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GJWwi7mGFGv",
        "colab_type": "text"
      },
      "source": [
        "# Sources\n",
        "\n",
        "https://cloudxlab.com/blog/fashion-mnist-using-machine-learning/\n",
        "\n",
        "https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus\n",
        "\n"
      ]
    }
  ]
}